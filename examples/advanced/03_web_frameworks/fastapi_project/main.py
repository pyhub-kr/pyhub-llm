#!/usr/bin/env python3
"""
ì˜ˆì œ: FastAPIì™€ pyhub-llm í†µí•©
ë‚œì´ë„: ê³ ê¸‰
ì„¤ëª…: FastAPIë¥¼ ì‚¬ìš©í•œ AI ì±—ë´‡ API ì„œë²„
ìš”êµ¬ì‚¬í•­: 
  - pyhub-llm (pip install pyhub-llm)
  - fastapi (pip install fastapi)
  - uvicorn (pip install uvicorn)
  - python-multipart (pip install python-multipart)
  - OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜

ì‹¤í–‰ ë°©ë²•:
  uvicorn main:app --reload
"""

import os
import asyncio
from typing import List, Optional, Dict, Any
from datetime import datetime
import uuid

from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from pyhub.llm import LLM
from pyhub.llm.types import Message


# Pydantic ëª¨ë¸ë“¤
class ChatRequest(BaseModel):
    message: str
    conversation_id: Optional[str] = None
    stream: bool = False
    model: str = "gpt-4o-mini"
    temperature: float = Field(default=0.7, ge=0, le=2)


class ChatResponse(BaseModel):
    response: str
    conversation_id: str
    tokens_used: int
    timestamp: datetime


class AnalysisRequest(BaseModel):
    text: str
    analysis_type: str = "sentiment"  # sentiment, summary, keywords, all


class SentimentAnalysis(BaseModel):
    sentiment: str
    confidence: float
    emotions: List[str]


class TextAnalysis(BaseModel):
    sentiment: Optional[SentimentAnalysis]
    summary: Optional[str]
    keywords: Optional[List[str]]
    entities: Optional[List[str]]


class ImageAnalysisRequest(BaseModel):
    question: str = "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."


# ì „ì—­ ë³€ìˆ˜
app = FastAPI(title="pyhub-llm API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ëŒ€í™” ë‚´ì—­ ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” DB ì‚¬ìš©)
conversations: Dict[str, List[Message]] = {}

# LLM ì¸ìŠ¤í„´ìŠ¤ ìºì‹œ
llm_cache: Dict[str, LLM] = {}


def get_llm(model: str = "gpt-4o-mini", **kwargs) -> LLM:
    """LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ìºì‹±)"""
    cache_key = f"{model}_{hash(frozenset(kwargs.items()))}"
    if cache_key not in llm_cache:
        llm_cache[cache_key] = LLM.create(model, **kwargs)
    return llm_cache[cache_key]


@app.get("/")
async def root():
    """API ì •ë³´"""
    return {
        "message": "pyhub-llm FastAPI Server",
        "endpoints": {
            "chat": "/chat",
            "chat_stream": "/chat/stream",
            "analyze": "/analyze",
            "analyze_image": "/analyze/image",
            "embeddings": "/embeddings",
            "conversations": "/conversations/{conversation_id}"
        }
    }


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì¼ë°˜ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ëŒ€í™” ID ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        conversation_id = request.conversation_id or str(uuid.uuid4())
        
        # ëŒ€í™” ë‚´ì—­ ê°€ì ¸ì˜¤ê¸°
        if conversation_id not in conversations:
            conversations[conversation_id] = []
        
        messages = conversations[conversation_id]
        messages.append(Message(role="user", content=request.message))
        
        # LLM í˜¸ì¶œ
        llm = get_llm(request.model, temperature=request.temperature)
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n".join([f"{msg.role}: {msg.content}" for msg in messages[-10:]])
        reply = await llm.ask_async(context)
        
        # ì‘ë‹µ ì €ì¥
        messages.append(Message(role="assistant", content=reply.text))
        
        return ChatResponse(
            response=reply.text,
            conversation_id=conversation_id,
            tokens_used=reply.usage.total if reply.usage else 0,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    async def generate():
        try:
            conversation_id = request.conversation_id or str(uuid.uuid4())
            
            # ëŒ€í™” ë‚´ì—­
            if conversation_id not in conversations:
                conversations[conversation_id] = []
            
            messages = conversations[conversation_id]
            messages.append(Message(role="user", content=request.message))
            
            # LLM ìŠ¤íŠ¸ë¦¬ë°
            llm = get_llm(request.model, temperature=request.temperature)
            context = "\n".join([f"{msg.role}: {msg.content}" for msg in messages[-10:]])
            
            full_response = ""
            async for chunk in llm.ask_async(context, stream=True):
                full_response += chunk.text
                yield f"data: {chunk.text}\n\n"
            
            # ì „ì²´ ì‘ë‹µ ì €ì¥
            messages.append(Message(role="assistant", content=full_response))
            
            # ì™„ë£Œ ì‹ í˜¸
            yield f"data: [DONE]\n\n"
            
        except Exception as e:
            yield f"data: ERROR: {str(e)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


@app.post("/analyze", response_model=TextAnalysis)
async def analyze_text(request: AnalysisRequest):
    """í…ìŠ¤íŠ¸ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        llm = get_llm("gpt-4o-mini")
        result = TextAnalysis()
        
        if request.analysis_type in ["sentiment", "all"]:
            sentiment_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
{{
    "sentiment": "ê¸ì •/ë¶€ì •/ì¤‘ë¦½",
    "confidence": 0.0-1.0,
    "emotions": ["ê°ì •1", "ê°ì •2"]
}}

í…ìŠ¤íŠ¸: {request.text}
"""
            sentiment_reply = await llm.ask_async(sentiment_prompt)
            try:
                import json
                sentiment_data = json.loads(sentiment_reply.text)
                result.sentiment = SentimentAnalysis(**sentiment_data)
            except:
                pass
        
        if request.analysis_type in ["summary", "all"]:
            summary_reply = await llm.ask_async(
                f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”: {request.text}"
            )
            result.summary = summary_reply.text
        
        if request.analysis_type in ["keywords", "all"]:
            keywords_reply = await llm.ask_async(
                f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”: {request.text}"
            )
            # ê°„ë‹¨í•œ íŒŒì‹±
            result.keywords = [k.strip() for k in keywords_reply.text.split(",")][:5]
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/analyze/image")
async def analyze_image(
    file: UploadFile = File(...),
    request: ImageAnalysisRequest = ImageAnalysisRequest()
):
    """ì´ë¯¸ì§€ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # íŒŒì¼ í™•ì¸
        if not file.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False, suffix=file.filename) as tmp:
            contents = await file.read()
            tmp.write(contents)
            tmp_path = tmp.name
        
        try:
            # ì´ë¯¸ì§€ ë¶„ì„
            llm = get_llm("gpt-4o-mini")  # vision ì§€ì› ëª¨ë¸
            reply = await llm.ask_async(request.question, files=[tmp_path])
            
            return {
                "filename": file.filename,
                "question": request.question,
                "analysis": reply.text,
                "timestamp": datetime.now()
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            import os
            os.unlink(tmp_path)
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/embeddings")
async def create_embeddings(texts: List[str]):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    try:
        if not texts:
            raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
        embed_llm = get_llm("text-embedding-3-small")
        embeddings = embed_llm.embed(texts)
        
        return {
            "model": "text-embedding-3-small",
            "embeddings": embeddings.embeddings,
            "dimensions": len(embeddings.embeddings[0]) if embeddings.embeddings else 0,
            "count": len(embeddings.embeddings)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/conversations/{conversation_id}")
async def get_conversation(conversation_id: str):
    """ëŒ€í™” ë‚´ì—­ ì¡°íšŒ"""
    if conversation_id not in conversations:
        raise HTTPException(status_code=404, detail="ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    messages = conversations[conversation_id]
    return {
        "conversation_id": conversation_id,
        "messages": [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ],
        "message_count": len(messages)
    }


@app.delete("/conversations/{conversation_id}")
async def delete_conversation(conversation_id: str):
    """ëŒ€í™” ì‚­ì œ"""
    if conversation_id not in conversations:
        raise HTTPException(status_code=404, detail="ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    del conversations[conversation_id]
    return {"message": "ëŒ€í™”ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.", "conversation_id": conversation_id}


# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì˜ˆì œ
async def process_long_task(task_id: str, prompt: str):
    """ê¸´ ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬"""
    llm = get_llm("gpt-4o-mini")
    reply = await llm.ask_async(prompt)
    # ì‹¤ì œë¡œëŠ” ê²°ê³¼ë¥¼ DBë‚˜ ìºì‹œì— ì €ì¥
    print(f"Task {task_id} completed: {reply.text[:100]}...")


@app.post("/tasks/create")
async def create_task(
    prompt: str,
    background_tasks: BackgroundTasks
):
    """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒì„±"""
    task_id = str(uuid.uuid4())
    background_tasks.add_task(process_long_task, task_id, prompt)
    
    return {
        "task_id": task_id,
        "status": "processing",
        "message": "ì‘ì—…ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ë˜ê³  ìˆìŠµë‹ˆë‹¤."
    }


# í—¬ìŠ¤ì²´í¬
@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        # API í‚¤ í™•ì¸
        api_key_set = bool(os.getenv("OPENAI_API_KEY"))
        
        # ê°„ë‹¨í•œ LLM í…ŒìŠ¤íŠ¸
        llm_working = False
        try:
            test_llm = get_llm("gpt-4o-mini")
            # ì‹¤ì œ í˜¸ì¶œì€ í•˜ì§€ ì•Šê³  ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë§Œ í™•ì¸
            llm_working = True
        except:
            pass
        
        return {
            "status": "healthy" if api_key_set and llm_working else "unhealthy",
            "api_key_configured": api_key_set,
            "llm_available": llm_working,
            "active_conversations": len(conversations),
            "cached_llms": len(llm_cache),
            "timestamp": datetime.now()
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "error", "detail": str(e)}
        )


if __name__ == "__main__":
    import uvicorn
    
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        exit(1)
    
    print("ğŸš€ FastAPI ì„œë²„ ì‹œì‘...")
    print("ğŸ“ http://localhost:8000")
    print("ğŸ“š API ë¬¸ì„œ: http://localhost:8000/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)