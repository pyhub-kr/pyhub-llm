#!/usr/bin/env python3
"""
ì˜ˆì œ: ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œ (Q&A System)
ë‚œì´ë„: ê³ ê¸‰
ì„¤ëª…: ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ë° ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ
ìš”êµ¬ì‚¬í•­:
  - pyhub-llm (pip install "pyhub-llm[all]")
  - numpy (pip install numpy)
  - OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜

ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ me@pyhub.krë¡œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
"""

import hashlib
import json
import os
import re
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from pyhub.llm import LLM
from pyhub.llm.cache import FileCache


@dataclass
class Document:
    """ë¬¸ì„œ"""

    id: str
    title: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    chunks: List["DocumentChunk"] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class DocumentChunk:
    """ë¬¸ì„œ ì²­í¬"""

    id: str
    document_id: str
    content: str
    embedding: Optional[List[float]] = None
    position: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Answer:
    """ë‹µë³€"""

    question: str
    answer: str
    confidence: float
    sources: List[DocumentChunk]
    metadata: Dict[str, Any] = field(default_factory=dict)


class KnowledgeBase:
    """ì§€ì‹ ë² ì´ìŠ¤"""

    def __init__(self, storage_path: str = "knowledge_base"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)
        self.documents: Dict[str, Document] = {}
        self.chunks: List[DocumentChunk] = []
        self.embeddings_cache = FileCache(str(self.storage_path / "embeddings"))
        self._load_knowledge_base()

    def _load_knowledge_base(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        kb_file = self.storage_path / "knowledge_base.json"
        if kb_file.exists():
            with open(kb_file, "r", encoding="utf-8") as f:
                data = json.load(f)

                # ë¬¸ì„œ ë³µì›
                for doc_data in data.get("documents", []):
                    doc = Document(
                        id=doc_data["id"],
                        title=doc_data["title"],
                        content=doc_data["content"],
                        metadata=doc_data.get("metadata", {}),
                        created_at=datetime.fromisoformat(doc_data["created_at"]),
                    )
                    self.documents[doc.id] = doc

                # ì²­í¬ ë³µì›
                for chunk_data in data.get("chunks", []):
                    chunk = DocumentChunk(
                        id=chunk_data["id"],
                        document_id=chunk_data["document_id"],
                        content=chunk_data["content"],
                        embedding=chunk_data.get("embedding"),
                        position=chunk_data.get("position", 0),
                        metadata=chunk_data.get("metadata", {}),
                    )
                    self.chunks.append(chunk)

    def save(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥"""
        data = {
            "documents": [
                {
                    "id": doc.id,
                    "title": doc.title,
                    "content": doc.content,
                    "metadata": doc.metadata,
                    "created_at": doc.created_at.isoformat(),
                }
                for doc in self.documents.values()
            ],
            "chunks": [
                {
                    "id": chunk.id,
                    "document_id": chunk.document_id,
                    "content": chunk.content,
                    "embedding": chunk.embedding,
                    "position": chunk.position,
                    "metadata": chunk.metadata,
                }
                for chunk in self.chunks
            ],
        }

        kb_file = self.storage_path / "knowledge_base.json"
        with open(kb_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def add_document(self, title: str, content: str, metadata: Optional[Dict] = None) -> Document:
        """ë¬¸ì„œ ì¶”ê°€"""
        # ë¬¸ì„œ ID ìƒì„±
        doc_id = hashlib.md5(f"{title}{content}".encode()).hexdigest()[:8]

        # ë¬¸ì„œ ìƒì„±
        doc = Document(id=doc_id, title=title, content=content, metadata=metadata or {})

        # ì²­í¬ ìƒì„±
        chunks = self._create_chunks(doc)
        doc.chunks = chunks
        self.chunks.extend(chunks)

        # ì €ì¥
        self.documents[doc_id] = doc
        self.save()

        return doc

    def _create_chunks(self, document: Document, chunk_size: int = 500) -> List[DocumentChunk]:
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []

        # ë‹¨ë½ ê¸°ë°˜ ë¶„í• 
        paragraphs = document.content.split("\n\n")
        current_chunk = []
        current_size = 0
        position = 0

        for para in paragraphs:
            para_size = len(para)

            if current_size + para_size > chunk_size and current_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunk_content = "\n\n".join(current_chunk)
                chunk_id = f"{document.id}_{position}"

                chunk = DocumentChunk(id=chunk_id, document_id=document.id, content=chunk_content, position=position)
                chunks.append(chunk)

                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = [para]
                current_size = para_size
                position += 1
            else:
                current_chunk.append(para)
                current_size += para_size

        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            chunk_content = "\n\n".join(current_chunk)
            chunk_id = f"{document.id}_{position}"

            chunk = DocumentChunk(id=chunk_id, document_id=document.id, content=chunk_content, position=position)
            chunks.append(chunk)

        return chunks

    def search_chunks(self, query: str, top_k: int = 5) -> List[Tuple[DocumentChunk, float]]:
        """ì²­í¬ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self._get_embedding(query)

        # ì²­í¬ë³„ ìœ ì‚¬ë„ ê³„ì‚°
        results = []
        for chunk in self.chunks:
            if chunk.embedding is None:
                chunk.embedding = self._get_embedding(chunk.content)

            similarity = self._cosine_similarity(query_embedding, chunk.embedding)
            results.append((chunk, similarity))

        # ìƒìœ„ kê°œ ë°˜í™˜
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

    def _get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_key = f"embed_{hashlib.md5(text.encode()).hexdigest()}"
        cached = self.embeddings_cache.get(cache_key)
        if cached:
            return cached

        # ì„ë² ë”© ìƒì„±
        llm = LLM.create("text-embedding-3-small")
        embeddings = llm.embed([text])
        embedding = embeddings[0]

        # ìºì‹œ ì €ì¥
        self.embeddings_cache.set(cache_key, embedding)

        return embedding

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            import numpy as np

            vec1 = np.array(vec1)
            vec2 = np.array(vec2)

            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            return dot_product / (norm1 * norm2)
        except ImportError:
            # NumPy ì—†ì´ ê³„ì‚°
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            norm1 = sum(a * a for a in vec1) ** 0.5
            norm2 = sum(b * b for b in vec2) ** 0.5

            if norm1 == 0 or norm2 == 0:
                return 0.0

            return dot_product / (norm1 * norm2)


class QASystem:
    """ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œ"""

    def __init__(self, model: str = "gpt-4o", knowledge_base: Optional[KnowledgeBase] = None):
        self.llm = LLM.create(model)
        self.knowledge_base = knowledge_base or KnowledgeBase()
        self.cache = FileCache("qa_cache")
        self.conversation_history: List[Tuple[str, str]] = []

    def add_knowledge(self, title: str, content: str, metadata: Optional[Dict] = None):
        """ì§€ì‹ ì¶”ê°€"""
        doc = self.knowledge_base.add_document(title, content, metadata)
        print(f"âœ… ë¬¸ì„œ ì¶”ê°€ë¨: {doc.title} (ID: {doc.id})")
        return doc

    def answer_question(
        self,
        question: str,
        use_knowledge_base: bool = True,
        include_sources: bool = True,
        max_context_length: int = 2000,
    ) -> Answer:
        """ì§ˆë¬¸ì— ë‹µë³€"""
        print(f"ğŸ¤” ì§ˆë¬¸: {question}")

        # ìºì‹œ í™•ì¸
        cache_key = f"qa_{hashlib.md5(question.encode()).hexdigest()}"
        cached = self.cache.get(cache_key)
        if cached and not include_sources:  # ì†ŒìŠ¤ê°€ í•„ìš”í•œ ê²½ìš° ìºì‹œ ì‚¬ìš© ì•ˆí•¨
            return Answer(**cached)

        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_chunks = []
        context = ""

        if use_knowledge_base and self.knowledge_base.chunks:
            print("ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
            search_results = self.knowledge_base.search_chunks(question, top_k=5)

            for chunk, similarity in search_results:
                if similarity > 0.7:  # ìœ ì‚¬ë„ ì„ê³„ê°’
                    relevant_chunks.append(chunk)
                    context += f"\n---\n{chunk.content}\n"

                    if len(context) > max_context_length:
                        break

            print(f"  ê´€ë ¨ ë¬¸ì„œ {len(relevant_chunks)}ê°œ ë°œê²¬")

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_prompt(question, context)

        # LLMì— ì§ˆë¬¸
        reply = self.llm.ask(prompt)
        answer_text = reply.text

        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(question, answer_text, relevant_chunks)

        # ë‹µë³€ ìƒì„±
        answer = Answer(
            question=question,
            answer=answer_text,
            confidence=confidence,
            sources=relevant_chunks if include_sources else [],
            metadata={"model": self.llm.model, "timestamp": datetime.now().isoformat(), "context_used": bool(context)},
        )

        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversation_history.append((question, answer_text))

        # ìºì‹œ ì €ì¥ (ì†ŒìŠ¤ ì œì™¸)
        if not include_sources:
            cache_data = {
                "question": answer.question,
                "answer": answer.answer,
                "confidence": answer.confidence,
                "sources": [],
                "metadata": answer.metadata,
            }
            self.cache.set(cache_key, cache_data)

        return answer

    def _create_prompt(self, question: str, context: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if context:
            prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ë‹µì´ ì—†ë‹¤ë©´ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        else:
            # ëŒ€í™” ê¸°ë¡ í¬í•¨
            history = ""
            if self.conversation_history:
                recent = self.conversation_history[-3:]  # ìµœê·¼ 3ê°œ
                history = "\n".join([f"Q: {q}\nA: {a}" for q, a in recent])

                prompt = f"""ì´ì „ ëŒ€í™”:
{history}

í˜„ì¬ ì§ˆë¬¸: {question}

ë‹µë³€:"""
            else:
                prompt = f"ì§ˆë¬¸: {question}\n\në‹µë³€:"

        return prompt

    def _calculate_confidence(self, question: str, answer: str, sources: List[DocumentChunk]) -> float:
        """ë‹µë³€ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.5  # ê¸°ë³¸ ì‹ ë¢°ë„

        # ì†ŒìŠ¤ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
        if sources:
            confidence += 0.3 * min(len(sources) / 3, 1.0)

        # ë‹µë³€ ê¸¸ì´ì— ë”°ë¥¸ ì¡°ì •
        if len(answer) > 100:
            confidence += 0.1

        # ë¶ˆí™•ì‹¤í•œ í‘œí˜„ í™•ì¸
        uncertain_phrases = ["ì•„ë§ˆë„", "ì¶”ì¸¡", "í™•ì‹¤í•˜ì§€ ì•Š", "might", "maybe", "possibly"]
        if any(phrase in answer.lower() for phrase in uncertain_phrases):
            confidence -= 0.2

        return max(0.1, min(1.0, confidence))

    def ask_followup(self, question: str) -> Answer:
        """í›„ì† ì§ˆë¬¸"""
        # ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í™œìš©
        return self.answer_question(question, use_knowledge_base=True)

    def explain_answer(self, answer: Answer) -> str:
        """ë‹µë³€ ì„¤ëª…"""
        prompt = f"""ë‹¤ìŒ ë‹µë³€ì´ ì–´ë–»ê²Œ ë„ì¶œë˜ì—ˆëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”:

ì§ˆë¬¸: {answer.question}
ë‹µë³€: {answer.answer}

ì‚¬ìš©ëœ ì†ŒìŠ¤ ìˆ˜: {len(answer.sources)}
ì‹ ë¢°ë„: {answer.confidence:.1%}

ì„¤ëª…:"""

        reply = self.llm.ask(prompt)
        return reply.text

    def generate_related_questions(self, question: str, num_questions: int = 3) -> List[str]:
        """ê´€ë ¨ ì§ˆë¬¸ ìƒì„±"""
        prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í›„ì† ì§ˆë¬¸ {num_questions}ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”:

ì›ë˜ ì§ˆë¬¸: {question}

ê´€ë ¨ ì§ˆë¬¸:
1."""

        reply = self.llm.ask(prompt)

        # ì§ˆë¬¸ íŒŒì‹±
        questions = []
        for line in reply.text.split("\n"):
            line = line.strip()
            if re.match(r"^\d+\.", line):
                question = re.sub(r"^\d+\.\s*", "", line)
                questions.append(question)

        return questions[:num_questions]

    def export_qa_history(self, format: str = "json") -> str:
        """Q&A ê¸°ë¡ ë‚´ë³´ë‚´ê¸°"""
        history = []

        for q, a in self.conversation_history:
            history.append({"question": q, "answer": a, "timestamp": datetime.now().isoformat()})

        if format == "json":
            return json.dumps(history, ensure_ascii=False, indent=2)
        elif format == "markdown":
            lines = ["# Q&A History\n"]
            for i, (q, a) in enumerate(self.conversation_history, 1):
                lines.append(f"## {i}. {q}")
                lines.append(f"\n{a}\n")
            return "\n".join(lines)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")


def example_basic_qa():
    """ê¸°ë³¸ Q&A ì˜ˆì œ"""
    print("\nğŸ’¬ ê¸°ë³¸ Q&A")
    print("-" * 50)

    qa_system = QASystem()

    # ì§€ì‹ ì—†ì´ ì§ˆë¬¸
    questions = [
        "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ê¸°ê³„í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "REST APIë€ ë¬´ì—‡ì¸ê°€ìš”?",
    ]

    for question in questions:
        answer = qa_system.answer_question(question, use_knowledge_base=False)
        print(f"\nâ“ Q: {question}")
        print(f"ğŸ’¡ A: {answer.answer[:200]}...")
        print(f"ğŸ“Š ì‹ ë¢°ë„: {answer.confidence:.1%}")


def example_knowledge_based_qa():
    """ì§€ì‹ ê¸°ë°˜ Q&A ì˜ˆì œ"""
    print("\nğŸ“š ì§€ì‹ ê¸°ë°˜ Q&A")
    print("-" * 50)

    qa_system = QASystem()

    # ì§€ì‹ ì¶”ê°€
    qa_system.add_knowledge(
        "Python ê°€ì´ë“œ",
        """Pythonì€ ê³ ìˆ˜ì¤€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.
        
ì£¼ìš” íŠ¹ì§•:
- ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•
- ë™ì  íƒ€ì´í•‘
- ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬
- í’ë¶€í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬

Pythonì˜ ì² í•™:
- Beautiful is better than ugly
- Explicit is better than implicit
- Simple is better than complex""",
    )

    qa_system.add_knowledge(
        "ì›¹ ê°œë°œ ê¸°ì´ˆ",
        """ì›¹ ê°œë°œì€ í¬ê²Œ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

í”„ë¡ íŠ¸ì—”ë“œ:
- HTML: êµ¬ì¡°
- CSS: ìŠ¤íƒ€ì¼
- JavaScript: ë™ì‘

ë°±ì—”ë“œ:
- ì„œë²„ ì¸¡ ë¡œì§
- ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
- API ê°œë°œ""",
    )

    # ì§€ì‹ ê¸°ë°˜ ì§ˆë¬¸
    questions = [
        "Pythonì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì›¹ ê°œë°œì—ì„œ í”„ë¡ íŠ¸ì—”ë“œë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "Pythonì˜ ì² í•™ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.",
    ]

    for question in questions:
        answer = qa_system.answer_question(question)
        print(f"\nâ“ Q: {question}")
        print(f"ğŸ’¡ A: {answer.answer}")
        print(f"ğŸ“Š ì‹ ë¢°ë„: {answer.confidence:.1%}")
        print(f"ğŸ“š ì†ŒìŠ¤: {len(answer.sources)}ê°œ ë¬¸ì„œ ì°¸ì¡°")


def example_conversational_qa():
    """ëŒ€í™”í˜• Q&A ì˜ˆì œ"""
    print("\nğŸ—£ï¸ ëŒ€í™”í˜• Q&A")
    print("-" * 50)

    qa_system = QASystem()

    # ì´ˆê¸° ì§ˆë¬¸
    answer1 = qa_system.answer_question("ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")
    print("Q1: ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")
    print(f"A1: {answer1.answer[:150]}...")

    # í›„ì† ì§ˆë¬¸
    answer2 = qa_system.ask_followup("ê·¸ëŸ¼ ê¸°ê³„í•™ìŠµì€ ë­ì£ ?")
    print("\nQ2: ê·¸ëŸ¼ ê¸°ê³„í•™ìŠµì€ ë­ì£ ?")
    print(f"A2: {answer2.answer[:150]}...")

    # ê´€ë ¨ ì§ˆë¬¸ ìƒì„±
    related = qa_system.generate_related_questions("ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")
    print("\nğŸ” ê´€ë ¨ ì§ˆë¬¸ ì œì•ˆ:")
    for i, q in enumerate(related, 1):
        print(f"  {i}. {q}")


def example_answer_explanation():
    """ë‹µë³€ ì„¤ëª… ì˜ˆì œ"""
    print("\nğŸ” ë‹µë³€ ì„¤ëª…")
    print("-" * 50)

    qa_system = QASystem()

    # ì§€ì‹ ì¶”ê°€
    qa_system.add_knowledge(
        "AI ìœ¤ë¦¬",
        """AI ìœ¤ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ê°œë°œê³¼ ì‚¬ìš©ì— ìˆì–´ ì¤‘ìš”í•œ ì›ì¹™ë“¤ì…ë‹ˆë‹¤.

ì£¼ìš” ì›ì¹™:
1. íˆ¬ëª…ì„±: AIì˜ ê²°ì • ê³¼ì •ì´ ì´í•´ ê°€ëŠ¥í•´ì•¼ í•¨
2. ê³µì •ì„±: í¸ê²¬ ì—†ëŠ” AI
3. ì±…ì„ì„±: AIì˜ ê²°ì •ì— ëŒ€í•œ ì±…ì„ ì†Œì¬ ëª…í™•í™”
4. í”„ë¼ì´ë²„ì‹œ: ê°œì¸ì •ë³´ ë³´í˜¸""",
    )

    # ì§ˆë¬¸ ë° ë‹µë³€
    answer = qa_system.answer_question("AI ìœ¤ë¦¬ì˜ ì£¼ìš” ì›ì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?")
    print(f"â“ ì§ˆë¬¸: {answer.question}")
    print(f"ğŸ’¡ ë‹µë³€: {answer.answer}")

    # ë‹µë³€ ì„¤ëª…
    explanation = qa_system.explain_answer(answer)
    print("\nğŸ“ ë‹µë³€ ì„¤ëª…:")
    print(explanation)


def example_export_history():
    """ê¸°ë¡ ë‚´ë³´ë‚´ê¸° ì˜ˆì œ"""
    print("\nğŸ’¾ Q&A ê¸°ë¡ ë‚´ë³´ë‚´ê¸°")
    print("-" * 50)

    qa_system = QASystem()

    # ì—¬ëŸ¬ ì§ˆë¬¸ ìˆ˜í–‰
    questions = ["í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì´ë€?", "ë„ì»¤(Docker)ì˜ ì¥ì ì€?", "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ë€?"]

    for q in questions:
        qa_system.answer_question(q, use_knowledge_base=False)

    # JSON ë‚´ë³´ë‚´ê¸°
    json_export = qa_system.export_qa_history("json")
    print("JSON í˜•ì‹:")
    print(json_export[:300] + "...")

    # ë§ˆí¬ë‹¤ìš´ ë‚´ë³´ë‚´ê¸°
    md_export = qa_system.export_qa_history("markdown")
    print("\në§ˆí¬ë‹¤ìš´ í˜•ì‹:")
    print(md_export[:300] + "...")


def example_advanced_search():
    """ê³ ê¸‰ ê²€ìƒ‰ ì˜ˆì œ"""
    print("\nğŸ” ê³ ê¸‰ ì§€ì‹ ê²€ìƒ‰")
    print("-" * 50)

    # ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±
    kb = KnowledgeBase()

    # ë‹¤ì–‘í•œ ë¬¸ì„œ ì¶”ê°€
    documents = [
        ("Python ê¸°ì´ˆ", "Pythonì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤..."),
        ("Python ê³ ê¸‰", "Pythonì˜ ê³ ê¸‰ ê¸°ëŠ¥ì—ëŠ” ë°ì½”ë ˆì´í„°, ì œë„ˆë ˆì´í„°..."),
        ("JavaScript ê¸°ì´ˆ", "JavaScriptëŠ” ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í–‰ë˜ëŠ”..."),
        ("ë°ì´í„°ë² ì´ìŠ¤", "ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ëŠ” í…Œì´ë¸” í˜•íƒœë¡œ..."),
    ]

    for title, content in documents:
        kb.add_document(title, content * 10)  # ë‚´ìš© í™•ì¥

    # ê²€ìƒ‰
    query = "Python í”„ë¡œê·¸ë˜ë°"
    results = kb.search_chunks(query, top_k=3)

    print(f"ğŸ” ê²€ìƒ‰ì–´: {query}")
    print("\nê²€ìƒ‰ ê²°ê³¼:")
    for chunk, similarity in results:
        doc = kb.documents.get(chunk.document_id)
        print(f"\nğŸ“„ ë¬¸ì„œ: {doc.title if doc else 'Unknown'}")
        print(f"   ìœ ì‚¬ë„: {similarity:.2%}")
        print(f"   ë‚´ìš©: {chunk.content[:100]}...")


def main():
    """Q&A ì‹œìŠ¤í…œ ì˜ˆì œ ë©”ì¸ í•¨ìˆ˜"""

    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    print("â“ ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œ ì˜ˆì œ")
    print("=" * 50)

    try:
        # 1. ê¸°ë³¸ Q&A
        example_basic_qa()

        # 2. ì§€ì‹ ê¸°ë°˜ Q&A
        example_knowledge_based_qa()

        # 3. ëŒ€í™”í˜• Q&A
        example_conversational_qa()

        # 4. ë‹µë³€ ì„¤ëª…
        example_answer_explanation()

        # 5. ê¸°ë¡ ë‚´ë³´ë‚´ê¸°
        example_export_history()

        # 6. ê³ ê¸‰ ê²€ìƒ‰
        example_advanced_search()

        print("\nâœ… ëª¨ë“  Q&A ì‹œìŠ¤í…œ ì˜ˆì œ ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
