#!/usr/bin/env python3
"""
ì˜ˆì œ: ë¬¸ì„œ ìš”ì•½ê¸°
ë‚œì´ë„: ê³ ê¸‰
ì„¤ëª…: ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ëŠ” ê³ ê¸‰ ì‹œìŠ¤í…œ
ìš”êµ¬ì‚¬í•­: 
  - pyhub-llm (pip install pyhub-llm)
  - PyPDF2 (pip install PyPDF2)
  - python-docx (pip install python-docx)
  - beautifulsoup4 (pip install beautifulsoup4)
  - OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜
"""

import os
import re
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass
from enum import Enum
import json

from pyhub.llm import LLM
from pyhub.llm.cache import FileCache
from pyhub.llm.templates.engine import TemplateEngine


class DocumentType(Enum):
    """ë¬¸ì„œ íƒ€ì…"""
    TEXT = "text"
    PDF = "pdf"
    DOCX = "docx"
    HTML = "html"
    MARKDOWN = "markdown"


@dataclass
class DocumentSection:
    """ë¬¸ì„œ ì„¹ì…˜"""
    title: str
    content: str
    level: int
    word_count: int
    position: int


@dataclass
class SummaryResult:
    """ìš”ì•½ ê²°ê³¼"""
    executive_summary: str
    key_points: List[str]
    section_summaries: Dict[str, str]
    statistics: Dict[str, Any]
    recommendations: Optional[List[str]] = None


class DocumentSummarizer:
    """ë¬¸ì„œ ìš”ì•½ê¸°"""
    
    def __init__(self, model: str = "gpt-4o-mini"):
        self.llm = LLM.create(model)
        self.cache = FileCache("summaries")
        self.template_engine = TemplateEngine()
        self._setup_templates()
    
    def _setup_templates(self):
        """í…œí”Œë¦¿ ì„¤ì •"""
        # ìš”ì•½ í…œí”Œë¦¿
        self.template_engine.add_template(
            "summarize",
            """ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì„¸ìš”:

ì œëª©: {{ title }}
ê¸¸ì´: {{ word_count }}ë‹¨ì–´

ë‚´ìš©:
{{ content }}

ìš”êµ¬ì‚¬í•­:
- {{ style }} ìŠ¤íƒ€ì¼ë¡œ ìš”ì•½
- í•µì‹¬ í¬ì¸íŠ¸ {{ num_points }}ê°œ ì¶”ì¶œ
- ìµœëŒ€ {{ max_words }}ë‹¨ì–´ë¡œ ìš”ì•½"""
        )
        
        # ì„¹ì…˜ ë¶„ì„ í…œí”Œë¦¿
        self.template_engine.add_template(
            "analyze_section",
            """ë‹¤ìŒ ì„¹ì…˜ì„ ë¶„ì„í•˜ì„¸ìš”:

{{ section_content }}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
1. ì£¼ì œ: (í•œ ë¬¸ì¥)
2. í•µì‹¬ ë‚´ìš©: (2-3ë¬¸ì¥)
3. ì¤‘ìš”ë„: (ìƒ/ì¤‘/í•˜)
4. í‚¤ì›Œë“œ: (ì‰¼í‘œë¡œ êµ¬ë¶„)"""
        )
    
    def load_document(self, file_path: str) -> Tuple[str, DocumentType]:
        """ë¬¸ì„œ ë¡œë“œ"""
        path = Path(file_path)
        
        if not path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        suffix = path.suffix.lower()
        
        if suffix == '.txt':
            return self._load_text(file_path), DocumentType.TEXT
        elif suffix == '.pdf':
            return self._load_pdf(file_path), DocumentType.PDF
        elif suffix == '.docx':
            return self._load_docx(file_path), DocumentType.DOCX
        elif suffix in ['.html', '.htm']:
            return self._load_html(file_path), DocumentType.HTML
        elif suffix == '.md':
            return self._load_markdown(file_path), DocumentType.MARKDOWN
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
            return self._load_text(file_path), DocumentType.TEXT
    
    def _load_text(self, file_path: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _load_pdf(self, file_path: str) -> str:
        """PDF íŒŒì¼ ë¡œë“œ"""
        try:
            import PyPDF2
            
            text = []
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text.append(page.extract_text())
            
            return '\n'.join(text)
        except ImportError:
            raise ImportError("PyPDF2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install PyPDF2")
    
    def _load_docx(self, file_path: str) -> str:
        """DOCX íŒŒì¼ ë¡œë“œ"""
        try:
            from docx import Document
            
            doc = Document(file_path)
            text = []
            
            for paragraph in doc.paragraphs:
                text.append(paragraph.text)
            
            return '\n'.join(text)
        except ImportError:
            raise ImportError("python-docxê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install python-docx")
    
    def _load_html(self, file_path: str) -> str:
        """HTML íŒŒì¼ ë¡œë“œ"""
        try:
            from bs4 import BeautifulSoup
            
            with open(file_path, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f, 'html.parser')
                
                # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ ì œê±°
                for script in soup(["script", "style"]):
                    script.decompose()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = soup.get_text()
                
                # ê³µë°± ì •ë¦¬
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = '\n'.join(chunk for chunk in chunks if chunk)
                
                return text
        except ImportError:
            raise ImportError("beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install beautifulsoup4")
    
    def _load_markdown(self, file_path: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def extract_sections(self, content: str, doc_type: DocumentType) -> List[DocumentSection]:
        """ë¬¸ì„œì—ì„œ ì„¹ì…˜ ì¶”ì¶œ"""
        sections = []
        
        if doc_type == DocumentType.MARKDOWN:
            # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ ì„¹ì…˜ ë¶„ë¦¬
            pattern = r'^(#{1,6})\s+(.+)$'
            current_section = None
            current_content = []
            position = 0
            
            for line in content.split('\n'):
                match = re.match(pattern, line)
                if match:
                    # ì´ì „ ì„¹ì…˜ ì €ì¥
                    if current_section:
                        content_text = '\n'.join(current_content).strip()
                        if content_text:
                            current_section.content = content_text
                            current_section.word_count = len(content_text.split())
                            sections.append(current_section)
                    
                    # ìƒˆ ì„¹ì…˜ ì‹œì‘
                    level = len(match.group(1))
                    title = match.group(2)
                    current_section = DocumentSection(
                        title=title,
                        content="",
                        level=level,
                        word_count=0,
                        position=position
                    )
                    current_content = []
                    position += 1
                else:
                    current_content.append(line)
            
            # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
            if current_section and current_content:
                content_text = '\n'.join(current_content).strip()
                if content_text:
                    current_section.content = content_text
                    current_section.word_count = len(content_text.split())
                    sections.append(current_section)
        
        else:
            # ê¸°ë³¸: ë‹¨ë½ ê¸°ë°˜ ë¶„ë¦¬
            paragraphs = re.split(r'\n\n+', content)
            for i, para in enumerate(paragraphs):
                if para.strip():
                    sections.append(DocumentSection(
                        title=f"ì„¹ì…˜ {i+1}",
                        content=para.strip(),
                        level=1,
                        word_count=len(para.split()),
                        position=i
                    ))
        
        return sections
    
    def summarize_section(self, section: DocumentSection, style: str = "concise") -> str:
        """ì„¹ì…˜ ìš”ì•½"""
        # ìºì‹œ í™•ì¸
        cache_key = f"section_{section.title}_{style}_{section.word_count}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        # ì§§ì€ ì„¹ì…˜ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if section.word_count < 50:
            return section.content
        
        # ìš”ì•½ ìƒì„±
        prompt = self.template_engine.render_template(
            "summarize",
            title=section.title,
            content=section.content,
            word_count=section.word_count,
            style=style,
            num_points=3,
            max_words=100
        )
        
        reply = self.llm.ask(prompt)
        summary = reply.text
        
        # ìºì‹œ ì €ì¥
        self.cache.set(cache_key, summary)
        
        return summary
    
    def generate_executive_summary(
        self,
        sections: List[DocumentSection],
        max_words: int = 300
    ) -> str:
        """ì „ì²´ ìš”ì•½ ìƒì„±"""
        # ì„¹ì…˜ ìš”ì•½ë“¤ì„ í•©ì¹¨
        section_summaries = []
        for section in sections[:10]:  # ìµœëŒ€ 10ê°œ ì„¹ì…˜
            summary = self.summarize_section(section)
            section_summaries.append(f"{section.title}: {summary}")
        
        combined = "\n".join(section_summaries)
        
        # ì „ì²´ ìš”ì•½ ìƒì„±
        prompt = f"""ë‹¤ìŒ ì„¹ì…˜ ìš”ì•½ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë¬¸ì„œì˜ í•µì‹¬ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
ìµœëŒ€ {max_words}ë‹¨ì–´ë¡œ ì‘ì„±í•˜ê³ , ê°€ì¥ ì¤‘ìš”í•œ ë‚´ìš©ì„ í¬í•¨í•˜ì„¸ìš”.

ì„¹ì…˜ ìš”ì•½:
{combined}

ì „ì²´ ìš”ì•½:"""
        
        reply = self.llm.ask(prompt)
        return reply.text
    
    def extract_key_points(self, content: str, num_points: int = 5) -> List[str]:
        """í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ {num_points}ê°œì˜ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ê° í¬ì¸íŠ¸ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{content[:3000]}  # í† í° ì œí•œ

í˜•ì‹:
1. ì²« ë²ˆì§¸ í•µì‹¬ í¬ì¸íŠ¸
2. ë‘ ë²ˆì§¸ í•µì‹¬ í¬ì¸íŠ¸
..."""
        
        reply = self.llm.ask(prompt)
        
        # ì‘ë‹µ íŒŒì‹±
        points = []
        for line in reply.text.split('\n'):
            line = line.strip()
            if re.match(r'^\d+\.', line):
                point = re.sub(r'^\d+\.\s*', '', line)
                points.append(point)
        
        return points[:num_points]
    
    def analyze_document_structure(self, sections: List[DocumentSection]) -> Dict[str, Any]:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„"""
        total_words = sum(s.word_count for s in sections)
        
        # ì„¹ì…˜ë³„ í†µê³„
        section_stats = {}
        for section in sections:
            if section.level not in section_stats:
                section_stats[section.level] = {
                    "count": 0,
                    "total_words": 0,
                    "avg_words": 0
                }
            
            stats = section_stats[section.level]
            stats["count"] += 1
            stats["total_words"] += section.word_count
        
        # í‰ê·  ê³„ì‚°
        for level_stats in section_stats.values():
            if level_stats["count"] > 0:
                level_stats["avg_words"] = level_stats["total_words"] / level_stats["count"]
        
        return {
            "total_sections": len(sections),
            "total_words": total_words,
            "avg_words_per_section": total_words / len(sections) if sections else 0,
            "section_hierarchy": section_stats,
            "depth": max(s.level for s in sections) if sections else 0
        }
    
    def generate_recommendations(self, summary_result: SummaryResult) -> List[str]:
        """ë¬¸ì„œ ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        stats = summary_result.statistics
        
        # ë¬¸ì„œ ê¸¸ì´ ê´€ë ¨
        if stats["total_words"] > 5000:
            recommendations.append("ë¬¸ì„œê°€ ë§¤ìš° ê¹ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ë‹´ì€ ìš”ì•½ë³¸ì„ ë³„ë„ë¡œ ì œê³µí•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        elif stats["total_words"] < 500:
            recommendations.append("ë¬¸ì„œê°€ ì§§ìŠµë‹ˆë‹¤. ë” ìì„¸í•œ ì„¤ëª…ì´ë‚˜ ì˜ˆì‹œë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # êµ¬ì¡° ê´€ë ¨
        if stats["depth"] > 4:
            recommendations.append("ë¬¸ì„œ êµ¬ì¡°ê°€ ë³µì¡í•©ë‹ˆë‹¤. ì„¹ì…˜ ê³„ì¸µì„ ë‹¨ìˆœí™”í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if stats["avg_words_per_section"] > 500:
            recommendations.append("ì„¹ì…˜ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. í•˜ìœ„ ì„¹ì…˜ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # AI ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì„œ ê°œì„ ì„ ìœ„í•œ ì¶”ì²œì‚¬í•­ 2-3ê°œë¥¼ ì œì‹œí•˜ì„¸ìš”:

ìš”ì•½: {summary_result.executive_summary}

ì£¼ìš” í¬ì¸íŠ¸:
{chr(10).join(f"- {point}" for point in summary_result.key_points[:3])}

ì¶”ì²œì‚¬í•­ (ê°ê° í•œ ë¬¸ì¥):**"""
        
        reply = self.llm.ask(prompt)
        
        # ì‘ë‹µ íŒŒì‹±
        for line in reply.text.split('\n'):
            line = line.strip()
            if line and not line.startswith('ì¶”ì²œì‚¬í•­'):
                recommendations.append(line.lstrip('- ').lstrip('â€¢ '))
        
        return recommendations[:5]  # ìµœëŒ€ 5ê°œ
    
    def summarize(
        self,
        file_path: str,
        style: str = "detailed",
        include_recommendations: bool = True
    ) -> SummaryResult:
        """ë¬¸ì„œ ìš”ì•½ ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ ì¤‘: {file_path}")
        
        # ë¬¸ì„œ ë¡œë“œ
        content, doc_type = self.load_document(file_path)
        print(f"  ë¬¸ì„œ íƒ€ì…: {doc_type.value}")
        
        # ì„¹ì…˜ ì¶”ì¶œ
        sections = self.extract_sections(content, doc_type)
        print(f"  ì„¹ì…˜ ìˆ˜: {len(sections)}")
        
        # êµ¬ì¡° ë¶„ì„
        statistics = self.analyze_document_structure(sections)
        print(f"  ì´ ë‹¨ì–´ ìˆ˜: {statistics['total_words']:,}")
        
        # ì„¹ì…˜ë³„ ìš”ì•½
        print("  ì„¹ì…˜ ìš”ì•½ ì¤‘...")
        section_summaries = {}
        for section in sections[:20]:  # ìµœëŒ€ 20ê°œ ì„¹ì…˜
            summary = self.summarize_section(section, style)
            section_summaries[section.title] = summary
        
        # ì „ì²´ ìš”ì•½
        print("  ì „ì²´ ìš”ì•½ ìƒì„± ì¤‘...")
        executive_summary = self.generate_executive_summary(sections)
        
        # í•µì‹¬ í¬ì¸íŠ¸
        print("  í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ ì¤‘...")
        key_points = self.extract_key_points(content)
        
        # ê²°ê³¼ ìƒì„±
        result = SummaryResult(
            executive_summary=executive_summary,
            key_points=key_points,
            section_summaries=section_summaries,
            statistics=statistics
        )
        
        # ì¶”ì²œì‚¬í•­
        if include_recommendations:
            print("  ì¶”ì²œì‚¬í•­ ìƒì„± ì¤‘...")
            result.recommendations = self.generate_recommendations(result)
        
        print("âœ… ìš”ì•½ ì™„ë£Œ!")
        return result


def example_text_summary():
    """í…ìŠ¤íŠ¸ íŒŒì¼ ìš”ì•½ ì˜ˆì œ"""
    print("\nğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ ìš”ì•½")
    print("-" * 50)
    
    # ì˜ˆì œ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
    sample_text = """# ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜

## ì„œë¡ 
ì¸ê³µì§€ëŠ¥(AI)ì€ 21ì„¸ê¸°ì˜ ê°€ì¥ ì¤‘ìš”í•œ ê¸°ìˆ  í˜ì‹  ì¤‘ í•˜ë‚˜ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤. 
ê¸°ê³„í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ì˜ ë°œì „ìœ¼ë¡œ AIëŠ” ì´ì œ ìš°ë¦¬ ì¼ìƒìƒí™œì˜ ë§ì€ ë¶€ë¶„ì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤.

## í˜„ì¬ ìƒí™©
### ê¸°ìˆ  ë°œì „
ìµœê·¼ ëª‡ ë…„ê°„ AI ê¸°ìˆ ì€ ë†€ë¼ìš´ ì†ë„ë¡œ ë°œì „í–ˆìŠµë‹ˆë‹¤. 
íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ì¸ê°„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

### ì‚°ì—… ì‘ìš©
AIëŠ” ì˜ë£Œ, ê¸ˆìœµ, ì œì¡°, êµìœ¡ ë“± ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.
ê° ë¶„ì•¼ì—ì„œ íš¨ìœ¨ì„± ì¦ëŒ€ì™€ ìƒˆë¡œìš´ ê°€ì¹˜ ì°½ì¶œì„ ì´ëŒì–´ë‚´ê³  ìˆìŠµë‹ˆë‹¤.

## ë¯¸ë˜ ì „ë§
### ê¸°ìˆ ì  ì§„ë³´
ì•ìœ¼ë¡œ AIëŠ” ë”ìš± ì •êµí•˜ê³  ë²”ìš©ì ì¸ í˜•íƒœë¡œ ë°œì „í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
AGI(Artificial General Intelligence)ì˜ ì‹¤í˜„ ê°€ëŠ¥ì„±ë„ ë…¼ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤.

### ì‚¬íšŒì  ì˜í–¥
AIì˜ ë°œì „ì€ ì¼ìë¦¬ êµ¬ì¡°, êµìœ¡ ì‹œìŠ¤í…œ, ì‚¬íšŒ ì œë„ ì „ë°˜ì— í° ë³€í™”ë¥¼ ê°€ì ¸ì˜¬ ê²ƒì…ë‹ˆë‹¤.
ì´ì— ëŒ€í•œ ì¤€ë¹„ì™€ ëŒ€ì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.

## ê²°ë¡ 
AIëŠ” ì¸ë¥˜ì—ê²Œ í° ê¸°íšŒì´ì ë„ì „ì…ë‹ˆë‹¤. 
ê¸°ìˆ ì˜ ë°œì „ê³¼ í•¨ê»˜ ìœ¤ë¦¬ì , ì‚¬íšŒì  ê³ ë ¤ì‚¬í•­ì„ ê· í˜•ìˆê²Œ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤."""
    
    # ì„ì‹œ íŒŒì¼ ìƒì„±
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(sample_text)
        temp_path = f.name
    
    try:
        # ìš”ì•½ê¸° ìƒì„±
        summarizer = DocumentSummarizer()
        
        # ìš”ì•½ ì‹¤í–‰
        result = summarizer.summarize(temp_path, style="concise")
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“‹ ì „ì²´ ìš”ì•½:")
        print(result.executive_summary)
        
        print(f"\nğŸ¯ í•µì‹¬ í¬ì¸íŠ¸:")
        for i, point in enumerate(result.key_points, 1):
            print(f"  {i}. {point}")
        
        print(f"\nğŸ“Š ë¬¸ì„œ í†µê³„:")
        print(f"  - ì´ ë‹¨ì–´ ìˆ˜: {result.statistics['total_words']:,}")
        print(f"  - ì„¹ì…˜ ìˆ˜: {result.statistics['total_sections']}")
        print(f"  - ë¬¸ì„œ ê¹Šì´: {result.statistics['depth']}")
        
        if result.recommendations:
            print(f"\nğŸ’¡ ê°œì„  ì¶”ì²œì‚¬í•­:")
            for i, rec in enumerate(result.recommendations, 1):
                print(f"  {i}. {rec}")
    
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_path)


def example_batch_processing():
    """ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ"""
    print("\nğŸ“š ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬")
    print("-" * 50)
    
    # ì—¬ëŸ¬ ë¬¸ì„œ ìƒì„±
    documents = {
        "report1.txt": "This is a technical report about machine learning...",
        "report2.txt": "Financial analysis for Q4 2024...",
        "report3.txt": "Marketing strategy proposal..."
    }
    
    summarizer = DocumentSummarizer()
    results = {}
    
    # ì„ì‹œ íŒŒì¼ë“¤ ìƒì„± ë° ì²˜ë¦¬
    import tempfile
    temp_files = []
    
    try:
        for filename, content in documents.items():
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                f.write(content * 50)  # ë‚´ìš© í™•ì¥
                temp_files.append(f.name)
                
                print(f"\nì²˜ë¦¬ ì¤‘: {filename}")
                result = summarizer.summarize(f.name, style="brief", include_recommendations=False)
                results[filename] = result
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        print("\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼:")
        for filename, result in results.items():
            print(f"\n{filename}:")
            print(f"  ìš”ì•½: {result.executive_summary[:100]}...")
            print(f"  ë‹¨ì–´ ìˆ˜: {result.statistics['total_words']:,}")
    
    finally:
        # ì„ì‹œ íŒŒì¼ë“¤ ì‚­ì œ
        for temp_file in temp_files:
            os.unlink(temp_file)


def example_export_results():
    """ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì˜ˆì œ"""
    print("\nğŸ’¾ ìš”ì•½ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
    print("-" * 50)
    
    # ìƒ˜í”Œ ë¬¸ì„œ
    sample_text = "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤." * 100
    
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
        f.write(sample_text)
        temp_path = f.name
    
    try:
        summarizer = DocumentSummarizer()
        result = summarizer.summarize(temp_path)
        
        # JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        export_data = {
            "file": os.path.basename(temp_path),
            "summary": result.executive_summary,
            "key_points": result.key_points,
            "statistics": result.statistics,
            "recommendations": result.recommendations,
            "timestamp": str(datetime.now())
        }
        
        json_output = json.dumps(export_data, ensure_ascii=False, indent=2)
        print("JSON í˜•ì‹:")
        print(json_output[:300] + "...")
        
        # ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        markdown_output = f"""# ë¬¸ì„œ ìš”ì•½ ë³´ê³ ì„œ

## íŒŒì¼ ì •ë³´
- íŒŒì¼ëª…: {os.path.basename(temp_path)}
- ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ì „ì²´ ìš”ì•½
{result.executive_summary}

## í•µì‹¬ í¬ì¸íŠ¸
{chr(10).join(f'- {point}' for point in result.key_points)}

## í†µê³„
- ì´ ë‹¨ì–´ ìˆ˜: {result.statistics['total_words']:,}
- ì„¹ì…˜ ìˆ˜: {result.statistics['total_sections']}

## ì¶”ì²œì‚¬í•­
{chr(10).join(f'{i+1}. {rec}' for i, rec in enumerate(result.recommendations or []))}
"""
        
        print("\në§ˆí¬ë‹¤ìš´ í˜•ì‹:")
        print(markdown_output[:300] + "...")
    
    finally:
        os.unlink(temp_path)


def main():
    """ë¬¸ì„œ ìš”ì•½ê¸° ì˜ˆì œ ë©”ì¸ í•¨ìˆ˜"""
    
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    print("ğŸ“„ ë¬¸ì„œ ìš”ì•½ê¸° ì˜ˆì œ")
    print("=" * 50)
    
    try:
        # 1. í…ìŠ¤íŠ¸ íŒŒì¼ ìš”ì•½
        example_text_summary()
        
        # 2. ë°°ì¹˜ ì²˜ë¦¬
        example_batch_processing()
        
        # 3. ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        example_export_results()
        
        print("\nâœ… ëª¨ë“  ë¬¸ì„œ ìš”ì•½ ì˜ˆì œ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()