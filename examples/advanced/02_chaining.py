#!/usr/bin/env python3
"""
ì˜ˆì œ: ì²´ì´ë‹
ë‚œì´ë„: ê³ ê¸‰
ì„¤ëª…: ì—¬ëŸ¬ LLMì„ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰
ìš”êµ¬ì‚¬í•­:
  - pyhub-llm (pip install "pyhub-llm[all]")
  - OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜

ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ me@pyhub.krë¡œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
from dataclasses import dataclass
from typing import Any, Dict, List

from pyhub.llm import LLM


@dataclass
class ChainResult:
    """ì²´ì¸ ì‹¤í–‰ ê²°ê³¼"""

    final_output: str
    intermediate_results: List[Dict[str, Any]]
    total_tokens: int = 0
    execution_time: float = 0.0


class SimpleChain:
    """ê°„ë‹¨í•œ ì²´ì¸ êµ¬í˜„"""

    def __init__(self, llms: List[LLM]):
        self.llms = llms
        self.results = []

    def run(self, initial_input: str) -> ChainResult:
        """ì²´ì¸ ì‹¤í–‰"""
        import time

        start_time = time.time()

        current_input = initial_input
        intermediate_results = []
        total_tokens = 0

        for i, llm in enumerate(self.llms):
            print(f"\nğŸ”— ì²´ì¸ ë‹¨ê³„ {i+1}/{len(self.llms)}")

            # LLM í˜¸ì¶œ
            reply = llm.ask(current_input)

            # ê²°ê³¼ ì €ì¥
            result = {
                "step": i + 1,
                "input": current_input[:100] + "..." if len(current_input) > 100 else current_input,
                "output": reply.text,
                "tokens": reply.usage.total if reply.usage else 0,
            }
            intermediate_results.append(result)

            # ë‹¤ìŒ ë‹¨ê³„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            current_input = reply.text
            total_tokens += result["tokens"]

            print(f"   ì…ë ¥: {result['input']}")
            print(f"   ì¶œë ¥: {result['output'][:100]}...")

        execution_time = time.time() - start_time

        return ChainResult(
            final_output=current_input,
            intermediate_results=intermediate_results,
            total_tokens=total_tokens,
            execution_time=execution_time,
        )


def example_basic_chaining():
    """ê¸°ë³¸ ì²´ì´ë‹ ì˜ˆì œ"""
    print("\nğŸ”— ê¸°ë³¸ ì²´ì´ë‹")
    print("-" * 50)

    # ì—¬ëŸ¬ LLMì„ ì—°ê²°
    chain = SimpleChain(
        [
            LLM.create("gpt-4o-mini", system_prompt="í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."),
            LLM.create("gpt-4o-mini", system_prompt="ì˜ì–´ë¥¼ ì¼ë³¸ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."),
            LLM.create("gpt-4o-mini", system_prompt="ì¼ë³¸ì–´ë¥¼ ë‹¤ì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."),
        ]
    )

    # ì²´ì¸ ì‹¤í–‰
    original_text = "ì¸ê³µì§€ëŠ¥ì€ ì¸ë¥˜ì˜ ë¯¸ë˜ë¥¼ ë°”ê¿€ í˜ì‹ ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤."
    print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {original_text}")

    result = chain.run(original_text)

    print(f"\nìµœì¢… ê²°ê³¼: {result.final_output}")
    print(f"ì´ í† í° ì‚¬ìš©: {result.total_tokens}")
    print(f"ì‹¤í–‰ ì‹œê°„: {result.execution_time:.2f}ì´ˆ")

    # ê° ë‹¨ê³„ ê²°ê³¼
    print("\nğŸ“Š ê° ë‹¨ê³„ë³„ ê²°ê³¼:")
    for step in result.intermediate_results:
        print(f"  ë‹¨ê³„ {step['step']}: {step['output']}")


def example_conditional_chaining():
    """ì¡°ê±´ë¶€ ì²´ì´ë‹ ì˜ˆì œ"""
    print("\nğŸ”€ ì¡°ê±´ë¶€ ì²´ì´ë‹")
    print("-" * 50)

    class ConditionalChain:
        """ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì²´ì¸ì„ ì‹¤í–‰"""

        def __init__(self):
            # ë¶„ë¥˜ê¸°
            self.classifier = LLM.create(
                "gpt-4o-mini", system_prompt="í…ìŠ¤íŠ¸ì˜ ì£¼ì œë¥¼ 'ê¸°ìˆ ', 'ë¹„ì¦ˆë‹ˆìŠ¤', 'ì¼ë°˜' ì¤‘ í•˜ë‚˜ë¡œ]ë§Œ ë‹µí•˜ì„¸ìš”."
            )

            # ì£¼ì œë³„ ì „ë¬¸ LLM
            self.tech_expert = LLM.create(
                "gpt-4o-mini", system_prompt="ë‹¹ì‹ ì€ ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ìˆ ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
            )

            self.business_expert = LLM.create(
                "gpt-4o-mini", system_prompt="ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ë‹µë³€í•˜ì„¸ìš”."
            )

            self.general_expert = LLM.create(
                "gpt-4o-mini", system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‰½ê³  ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”."
            )

        def process(self, text: str) -> Dict[str, Any]:
            # 1. ì£¼ì œ ë¶„ë¥˜
            classification = self.classifier.ask(text, choices=["ê¸°ìˆ ", "ë¹„ì¦ˆë‹ˆìŠ¤", "ì¼ë°˜"])
            topic = classification.choice

            print(f"ğŸ“ ì£¼ì œ ë¶„ë¥˜: {topic}")

            # 2. ì£¼ì œì— ë”°ë¥¸ ì²˜ë¦¬
            if topic == "ê¸°ìˆ ":
                expert = self.tech_expert
            elif topic == "ë¹„ì¦ˆë‹ˆìŠ¤":
                expert = self.business_expert
            else:
                expert = self.general_expert

            # 3. ì „ë¬¸ê°€ ë‹µë³€
            response = expert.ask(text)

            return {"topic": topic, "question": text, "answer": response.text, "expert_used": topic + "_expert"}

    # ì¡°ê±´ë¶€ ì²´ì¸ ì‚¬ìš©
    conditional_chain = ConditionalChain()

    questions = [
        "ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ìŠ¤íƒ€íŠ¸ì—…ì˜ ìê¸ˆ ì¡°ë‹¬ ë°©ë²•ì—ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?",
        "ì˜¤ëŠ˜ ì €ë… ë©”ë‰´ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
    ]

    for question in questions:
        print(f"\nğŸ’¬ ì§ˆë¬¸: {question}")
        result = conditional_chain.process(question)
        print(f"ğŸ·ï¸ ë¶„ë¥˜: {result['topic']}")
        print(f"ğŸ‘¨â€ğŸ’¼ ì „ë¬¸ê°€: {result['expert_used']}")
        print(f"ğŸ’¡ ë‹µë³€: {result['answer'][:200]}...")


def example_parallel_chaining():
    """ë³‘ë ¬ ì²˜ë¦¬ ì²´ì¸ ì˜ˆì œ"""
    print("\nâš¡ ë³‘ë ¬ ì²˜ë¦¬ ì²´ì¸")
    print("-" * 50)

    async def parallel_analysis(text: str) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ë¶„ì„ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰"""

        # ì„œë¡œ ë‹¤ë¥¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” LLMë“¤
        sentiment_llm = LLM.create("gpt-4o-mini", system_prompt="í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”.")
        keyword_llm = LLM.create("gpt-4o-mini", system_prompt="í•µì‹¬ í‚¤ì›Œë“œ 3ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.")
        summary_llm = LLM.create("gpt-4o-mini", system_prompt="í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.")
        category_llm = LLM.create("gpt-4o-mini", system_prompt="ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì œì•ˆí•˜ì„¸ìš”.")

        # ë³‘ë ¬ ì‹¤í–‰
        tasks = [
            sentiment_llm.ask_async(text),
            keyword_llm.ask_async(text),
            summary_llm.ask_async(text),
            category_llm.ask_async(text),
        ]

        import time

        start_time = time.time()

        results = await asyncio.gather(*tasks)

        execution_time = time.time() - start_time

        return {
            "original_text": text,
            "sentiment": results[0].text,
            "keywords": results[1].text,
            "summary": results[2].text,
            "category": results[3].text,
            "execution_time": execution_time,
        }

    # ë¶„ì„í•  í…ìŠ¤íŠ¸
    text = """
    ìµœê·¼ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ë§ì€ ì‚°ì—…ì´ í˜ì‹ ì ìœ¼ë¡œ ë³€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ í° ë°œì „ì´ ìˆì—ˆê³ , ì´ëŠ” ê³ ê° ì„œë¹„ìŠ¤, ì½˜í…ì¸  ìƒì„±,
    ë²ˆì—­ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë™ì‹œì— ì¼ìë¦¬ ëŒ€ì²´, 
    ê°œì¸ì •ë³´ ë³´í˜¸, ìœ¤ë¦¬ì  ë¬¸ì œ ë“± í•´ê²°í•´ì•¼ í•  ê³¼ì œë“¤ë„ ìˆìŠµë‹ˆë‹¤.
    """

    print(f"ë¶„ì„í•  í…ìŠ¤íŠ¸: {text[:100]}...")

    # ë™ê¸° ì‹¤í–‰ì„ ìœ„í•œ ë˜í¼
    result = asyncio.run(parallel_analysis(text))

    print("\nğŸ“Š ë³‘ë ¬ ë¶„ì„ ê²°ê³¼:")
    print(f"  ğŸ˜Š ê°ì •: {result['sentiment']}")
    print(f"  ğŸ”‘ í‚¤ì›Œë“œ: {result['keywords']}")
    print(f"  ğŸ“ ìš”ì•½: {result['summary']}")
    print(f"  ğŸ“ ì¹´í…Œê³ ë¦¬: {result['category']}")
    print(f"  â±ï¸  ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")


def example_complex_pipeline():
    """ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ ì˜ˆì œ"""
    print("\nğŸ­ ë³µì¡í•œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
    print("-" * 50)

    class DataProcessingPipeline:
        """ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

        def __init__(self):
            # ê° ë‹¨ê³„ë³„ LLM
            self.extractor = LLM.create(
                "gpt-4o-mini", system_prompt="í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”."
            )

            self.validator = LLM.create("gpt-4o-mini", system_prompt="ë°ì´í„°ì˜ ì •í™•ì„±ì„ ê²€ì¦í•˜ê³  ë¬¸ì œì ì„ ì§€ì í•˜ì„¸ìš”.")

            self.enricher = LLM.create(
                "gpt-4o-mini", system_prompt="ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë³´ê°•í•˜ê³  ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”."
            )

            self.formatter = LLM.create(
                "gpt-4o-mini", system_prompt="ì •ë³´ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”."
            )

        def process(self, raw_text: str) -> Dict[str, Any]:
            """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
            results = {"stages": []}

            # 1ë‹¨ê³„: ì •ë³´ ì¶”ì¶œ
            print("1ï¸âƒ£ ì •ë³´ ì¶”ì¶œ ì¤‘...")
            extract_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
{raw_text}

JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
{{
    "main_topic": "ì£¼ì œ",
    "key_points": ["í¬ì¸íŠ¸1", "í¬ì¸íŠ¸2"],
    "entities": ["ê°œì²´1", "ê°œì²´2"],
    "sentiment": "ê¸ì •/ì¤‘ë¦½/ë¶€ì •"
}}
"""
            extract_result = self.extractor.ask(extract_prompt)
            results["stages"].append({"stage": "extraction", "output": extract_result.text})

            # 2ë‹¨ê³„: ê²€ì¦
            print("2ï¸âƒ£ ë°ì´í„° ê²€ì¦ ì¤‘...")
            validate_prompt = f"""
ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ê²€ì¦í•˜ì„¸ìš”:
{extract_result.text}

ì›ë³¸ í…ìŠ¤íŠ¸:
{raw_text}

ì •í™•ì„±ê³¼ ì™„ì „ì„±ì„ í‰ê°€í•˜ê³  ê°œì„ ì ì„ ì œì‹œí•˜ì„¸ìš”.
"""
            validate_result = self.validator.ask(validate_prompt)
            results["stages"].append({"stage": "validation", "output": validate_result.text})

            # 3ë‹¨ê³„: ë³´ê°•
            print("3ï¸âƒ£ ì •ë³´ ë³´ê°• ì¤‘...")
            enrich_prompt = f"""
ë‹¤ìŒ ì •ë³´ë¥¼ ë³´ê°•í•˜ê³  ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”:
{extract_result.text}

ê²€ì¦ ê²°ê³¼:
{validate_result.text}
"""
            enrich_result = self.enricher.ask(enrich_prompt)
            results["stages"].append({"stage": "enrichment", "output": enrich_result.text})

            # 4ë‹¨ê³„: ìµœì¢… í¬ë§·íŒ…
            print("4ï¸âƒ£ ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ì¤‘...")
            format_prompt = f"""
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì½ê¸° ì‰¬ìš´ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:

ì›ë³¸ ë°ì´í„°: {extract_result.text}
ê²€ì¦ ê²°ê³¼: {validate_result.text}
ë³´ê°•ëœ ì •ë³´: {enrich_result.text}

ë³´ê³ ì„œëŠ” ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:
1. ìš”ì•½
2. ì£¼ìš” ë°œê²¬ì‚¬í•­
3. ìƒì„¸ ë¶„ì„
4. ê¶Œì¥ì‚¬í•­
"""
            format_result = self.formatter.ask(format_prompt)
            results["stages"].append({"stage": "formatting", "output": format_result.text})

            results["final_report"] = format_result.text
            return results

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = DataProcessingPipeline()

    sample_text = """
    ABC íšŒì‚¬ëŠ” 2024ë…„ 1ë¶„ê¸°ì— ë§¤ì¶œ 150ì–µì›ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. 
    ì´ëŠ” ì „ë…„ ë™ê¸° ëŒ€ë¹„ 23% ì¦ê°€í•œ ìˆ˜ì¹˜ì…ë‹ˆë‹¤. 
    ì£¼ìš” ì„±ì¥ ë™ë ¥ì€ ì‹ ì œí’ˆ ì¶œì‹œì™€ í•´ì™¸ ì‹œì¥ í™•ëŒ€ì˜€ìŠµë‹ˆë‹¤. 
    íŠ¹íˆ ë™ë‚¨ì•„ì‹œì•„ ì§€ì—­ì—ì„œ 45%ì˜ ì„±ì¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
    """

    print(f"ì…ë ¥ í…ìŠ¤íŠ¸: {sample_text}\n")

    result = pipeline.process(sample_text)

    print("\nğŸ“‹ ìµœì¢… ë³´ê³ ì„œ:")
    print("-" * 50)
    print(result["final_report"])


def example_feedback_loop_chain():
    """í”¼ë“œë°± ë£¨í”„ ì²´ì¸ ì˜ˆì œ"""
    print("\nğŸ”„ í”¼ë“œë°± ë£¨í”„ ì²´ì¸")
    print("-" * 50)

    class FeedbackChain:
        """ìê¸° ê°œì„  í”¼ë“œë°± ë£¨í”„"""

        def __init__(self, max_iterations: int = 3):
            self.max_iterations = max_iterations
            self.generator = LLM.create("gpt-4o-mini", system_prompt="ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•œ ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”.")
            self.critic = LLM.create("gpt-4o-mini", system_prompt="í…ìŠ¤íŠ¸ë¥¼ í‰ê°€í•˜ê³  êµ¬ì²´ì ì¸ ê°œì„ ì ì„ ì œì‹œí•˜ì„¸ìš”.")
            self.improver = LLM.create("gpt-4o-mini", system_prompt="í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê°œì„ í•˜ì„¸ìš”.")

        def generate_with_feedback(self, topic: str) -> Dict[str, Any]:
            """í”¼ë“œë°±ì„ í†µí•œ ë°˜ë³µ ê°œì„ """
            results = {"topic": topic, "iterations": []}

            # ì´ˆê¸° ìƒì„±
            print(f"ğŸ¯ ì£¼ì œ: {topic}")
            current_text = self.generator.ask(f"ì£¼ì œ: {topic}").text

            for i in range(self.max_iterations):
                print(f"\nğŸ”„ ë°˜ë³µ {i+1}/{self.max_iterations}")

                iteration_result = {"iteration": i + 1, "text": current_text}

                # ë¹„í‰
                critique_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í‰ê°€í•˜ê³  ê°œì„ ì ì„ ì œì‹œí•˜ì„¸ìš”:

í…ìŠ¤íŠ¸:
{current_text}

í‰ê°€ ê¸°ì¤€:
1. ì •í™•ì„±
2. ëª…í™•ì„±
3. ì™„ì „ì„±
4. ê°€ë…ì„±
"""
                critique = self.critic.ask(critique_prompt).text
                iteration_result["critique"] = critique

                print(f"   ğŸ“ í˜„ì¬ í…ìŠ¤íŠ¸: {current_text[:100]}...")
                print(f"   ğŸ” ë¹„í‰: {critique[:100]}...")

                # ê°œì„ 
                improve_prompt = f"""
ë‹¤ìŒ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê°œì„ í•˜ì„¸ìš”:

ì›ë³¸ í…ìŠ¤íŠ¸:
{current_text}

í”¼ë“œë°±:
{critique}
"""
                improved_text = self.improver.ask(improve_prompt).text
                iteration_result["improved_text"] = improved_text

                results["iterations"].append(iteration_result)
                current_text = improved_text

            results["final_text"] = current_text
            return results

    # í”¼ë“œë°± ì²´ì¸ ì‹¤í–‰
    feedback_chain = FeedbackChain(max_iterations=2)
    result = feedback_chain.generate_with_feedback("ì–‘ì ì»´í“¨íŒ…ì˜ ì›ë¦¬ì™€ ì‘ìš©")

    print("\nğŸ“ˆ ê°œì„  ê³¼ì •:")
    for iteration in result["iterations"]:
        print(f"\në°˜ë³µ {iteration['iteration']}:")
        print(f"  í…ìŠ¤íŠ¸: {iteration['text'][:150]}...")
        print(f"  ë¹„í‰: {iteration['critique'][:150]}...")

    print("\nâœ… ìµœì¢… ê²°ê³¼:")
    print(result["final_text"])


def main():
    """ì²´ì´ë‹ ì˜ˆì œ ë©”ì¸ í•¨ìˆ˜"""

    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    print("ğŸ”— ì²´ì´ë‹ ì˜ˆì œ")
    print("=" * 50)

    try:
        # 1. ê¸°ë³¸ ì²´ì´ë‹
        example_basic_chaining()

        # 2. ì¡°ê±´ë¶€ ì²´ì´ë‹
        example_conditional_chaining()

        # 3. ë³‘ë ¬ ì²˜ë¦¬ ì²´ì¸
        example_parallel_chaining()

        # 4. ë³µì¡í•œ íŒŒì´í”„ë¼ì¸
        example_complex_pipeline()

        # 5. í”¼ë“œë°± ë£¨í”„ ì²´ì¸
        example_feedback_loop_chain()

        print("\nâœ… ëª¨ë“  ì²´ì´ë‹ ì˜ˆì œ ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
