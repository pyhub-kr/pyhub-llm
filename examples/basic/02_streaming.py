#!/usr/bin/env python3
"""
ì˜ˆì œ: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
ë‚œì´ë„: ì´ˆê¸‰
ì„¤ëª…: ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ ë°›ì•„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•
ìš”êµ¬ì‚¬í•­: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜

ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ me@pyhub.krë¡œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
"""

import os
import sys

from pyhub.llm import LLM


def main():
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜ˆì œ"""

    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    print("ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜ˆì œ\n")

    # LLM ìƒì„±
    llm = LLM.create("gpt-4o-mini")

    # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì´ì•¼ê¸° ìƒì„±
    prompt = "íŒŒì´ì¬ì˜ ì¥ì ì„ 3ê°€ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    print(f"í”„ë¡¬í”„íŠ¸: {prompt}\n")
    print("ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°):\n" + "-" * 50)

    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
    full_response = ""
    for chunk in llm.ask(prompt, stream=True):
        # ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥
        print(chunk.text, end="", flush=True)
        full_response += chunk.text

    print("\n" + "-" * 50)
    print(f"\nâœ… ì „ì²´ ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì")

    # ìŠ¤íŠ¸ë¦¬ë° with ì§„í–‰ í‘œì‹œ
    print("\n\nğŸ¯ ì§„í–‰ í‘œì‹œì™€ í•¨ê»˜ ìŠ¤íŠ¸ë¦¬ë°:")
    prompt2 = "ì§§ì€ ì‹œë¥¼ í•˜ë‚˜ ì‘ì„±í•´ì£¼ì„¸ìš”."
    print(f"í”„ë¡¬í”„íŠ¸: {prompt2}\n")

    char_count = 0
    for chunk in llm.ask(prompt2, stream=True):
        print(chunk.text, end="", flush=True)
        char_count += len(chunk.text)

        # ì§„í–‰ ìƒíƒœë¥¼ í„°ë¯¸ë„ ì œëª©ì— í‘œì‹œ (ì„ íƒì‚¬í•­)
        sys.stdout.write(f"\033]0;ìƒì„± ì¤‘... ({char_count}ì)\007")

    print(f"\n\nâœ… ì™„ë£Œ! ì´ {char_count}ì ìƒì„±")


if __name__ == "__main__":
    main()
