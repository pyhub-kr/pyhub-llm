#!/usr/bin/env python3
"""
ì˜ˆì œ: ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬
ë‚œì´ë„: ì´ˆê¸‰
ì„¤ëª…: LLM ì‚¬ìš© ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•
ìš”êµ¬ì‚¬í•­: 
  - pyhub-llm (pip install pyhub-llm)
  - OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜
"""

import os
import time
from pyhub.llm import LLM
from pyhub.llm.exceptions import (
    LLMError,
    RateLimitError,
    AuthenticationError,
    InvalidRequestError
)


def example_basic_error_handling():
    """ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ"""
    print("\nğŸ›¡ï¸ ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬")
    print("-" * 50)
    
    # ì˜ëª»ëœ API í‚¤ë¡œ ì‹œë„
    print("1. ì¸ì¦ ì—ëŸ¬ ì²˜ë¦¬:")
    try:
        # ì„ì‹œë¡œ ì˜ëª»ëœ API í‚¤ ì„¤ì •
        old_key = os.environ.get("OPENAI_API_KEY", "")
        os.environ["OPENAI_API_KEY"] = "invalid-key"
        
        llm = LLM.create("gpt-4o-mini")
        reply = llm.ask("ì•ˆë…•í•˜ì„¸ìš”")
        
    except AuthenticationError as e:
        print(f"âŒ ì¸ì¦ ì‹¤íŒ¨: API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {str(e)[:100]}...")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {type(e).__name__}")
    finally:
        # API í‚¤ ë³µì›
        os.environ["OPENAI_API_KEY"] = old_key
    
    # ì •ìƒ API í‚¤ë¡œ ë³µì› í›„ ê³„ì†
    llm = LLM.create("gpt-4o-mini")
    
    # 2. ì˜ëª»ëœ ìš”ì²­ ì²˜ë¦¬
    print("\n2. ì˜ëª»ëœ ìš”ì²­ ì²˜ë¦¬:")
    try:
        # ë„ˆë¬´ ê¸´ ì…ë ¥ (í† í° ì œí•œ ì´ˆê³¼ ì‹œë®¬ë ˆì´ì…˜)
        very_long_text = "A" * 50000  # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸
        reply = llm.ask(very_long_text)
    except InvalidRequestError as e:
        print(f"âŒ ì˜ëª»ëœ ìš”ì²­: ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤.")
        print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {str(e)[:100]}...")
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {type(e).__name__}")
        print(f"   í† í° ì œí•œì„ ì´ˆê³¼í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def example_retry_logic():
    """ì¬ì‹œë„ ë¡œì§ ì˜ˆì œ"""
    print("\nğŸ”„ ì¬ì‹œë„ ë¡œì§")
    print("-" * 50)
    
    llm = LLM.create("gpt-4o-mini")
    
    def ask_with_retry(prompt: str, max_retries: int = 3) -> str:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ LLM í˜¸ì¶œ"""
        for attempt in range(max_retries):
            try:
                print(f"ì‹œë„ {attempt + 1}/{max_retries}...")
                reply = llm.ask(prompt)
                print("âœ… ì„±ê³µ!")
                return reply.text
            
            except RateLimitError as e:
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    print(f"â³ ì†ë„ ì œí•œ. {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(wait_time)
                else:
                    print("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    raise
            
            except Exception as e:
                print(f"âŒ ì—ëŸ¬ ë°œìƒ: {type(e).__name__}")
                if attempt < max_retries - 1:
                    print("ì¬ì‹œë„ ì¤‘...")
                else:
                    raise
        
        return "ì¬ì‹œë„ ì‹¤íŒ¨"
    
    # ì •ìƒ ìš”ì²­
    result = ask_with_retry("íŒŒì´ì¬ì˜ ì¥ì ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
    print(f"\nì‘ë‹µ: {result}")


def example_fallback_handling():
    """í´ë°± ì²˜ë¦¬ ì˜ˆì œ"""
    print("\nğŸ”€ í´ë°± ì²˜ë¦¬")
    print("-" * 50)
    
    def ask_with_fallback(prompt: str, models: list) -> str:
        """ì—¬ëŸ¬ ëª¨ë¸ì„ ì‹œë„í•˜ëŠ” í´ë°± ë¡œì§"""
        for i, model in enumerate(models):
            try:
                print(f"ëª¨ë¸ ì‹œë„: {model}")
                llm = LLM.create(model)
                reply = llm.ask(prompt)
                print(f"âœ… {model} ì„±ê³µ!")
                return reply.text
            
            except Exception as e:
                print(f"âŒ {model} ì‹¤íŒ¨: {type(e).__name__}")
                if i < len(models) - 1:
                    print(f"ë‹¤ìŒ ëª¨ë¸ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
                else:
                    print("ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    raise
        
        return "ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨"
    
    # ì—¬ëŸ¬ ëª¨ë¸ ì‹œë„ (ì¼ë¶€ëŠ” ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
    models = ["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"]
    prompt = "AIì˜ ë¯¸ë˜ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”."
    
    try:
        result = ask_with_fallback(prompt, models)
        print(f"\nìµœì¢… ì‘ë‹µ: {result}")
    except Exception as e:
        print(f"\nâŒ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨: {e}")


def example_timeout_handling():
    """íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ ì˜ˆì œ"""
    print("\nâ±ï¸ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬")
    print("-" * 50)
    
    import signal
    
    class TimeoutException(Exception):
        pass
    
    def timeout_handler(signum, frame):
        raise TimeoutException("ìš”ì²­ ì‹œê°„ ì´ˆê³¼")
    
    def ask_with_timeout(llm, prompt: str, timeout_seconds: int = 10):
        """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” LLM í˜¸ì¶œ"""
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (Unix ì‹œìŠ¤í…œì—ì„œë§Œ ì‘ë™)
        if hasattr(signal, 'SIGALRM'):
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout_seconds)
        
        try:
            print(f"ìš”ì²­ ì¤‘... (íƒ€ì„ì•„ì›ƒ: {timeout_seconds}ì´ˆ)")
            reply = llm.ask(prompt)
            if hasattr(signal, 'SIGALRM'):
                signal.alarm(0)  # íƒ€ì„ì•„ì›ƒ í•´ì œ
            return reply.text
        
        except TimeoutException:
            print(f"âŒ {timeout_seconds}ì´ˆ ë‚´ì— ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None
        except Exception as e:
            if hasattr(signal, 'SIGALRM'):
                signal.alarm(0)  # íƒ€ì„ì•„ì›ƒ í•´ì œ
            raise
    
    llm = LLM.create("gpt-4o-mini")
    
    # ì§§ì€ íƒ€ì„ì•„ì›ƒ í…ŒìŠ¤íŠ¸
    result = ask_with_timeout(
        llm,
        "ì§§ì€ ë‹µë³€: 1+1ì€?",
        timeout_seconds=10
    )
    
    if result:
        print(f"âœ… ì‘ë‹µ: {result}")
    else:
        print("ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


def example_graceful_degradation():
    """ìš°ì•„í•œ ì„±ëŠ¥ ì €í•˜ ì˜ˆì œ"""
    print("\nğŸ“‰ ìš°ì•„í•œ ì„±ëŠ¥ ì €í•˜")
    print("-" * 50)
    
    class SmartLLM:
        """ì—ëŸ¬ ì‹œ ìë™ìœ¼ë¡œ ì„±ëŠ¥ì„ ë‚®ì¶”ëŠ” LLM ë˜í¼"""
        
        def __init__(self):
            self.quality_levels = [
                {"model": "gpt-4o", "max_tokens": 2000},
                {"model": "gpt-4o-mini", "max_tokens": 1000},
                {"model": "gpt-3.5-turbo", "max_tokens": 500}
            ]
            self.current_level = 0
        
        def ask(self, prompt: str) -> str:
            while self.current_level < len(self.quality_levels):
                config = self.quality_levels[self.current_level]
                
                try:
                    print(f"ì‹œë„ ì¤‘: {config['model']} (ìµœëŒ€ í† í°: {config['max_tokens']})")
                    llm = LLM.create(config['model'])
                    reply = llm.ask(prompt)
                    
                    # ì„±ê³µí•˜ë©´ ë ˆë²¨ ìœ ì§€
                    print(f"âœ… ì„±ê³µ!")
                    return reply.text
                
                except Exception as e:
                    print(f"âŒ ì‹¤íŒ¨: {type(e).__name__}")
                    self.current_level += 1
                    
                    if self.current_level < len(self.quality_levels):
                        print("í’ˆì§ˆ ë ˆë²¨ì„ ë‚®ì¶°ì„œ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    else:
                        print("ëª¨ë“  í’ˆì§ˆ ë ˆë²¨ì—ì„œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return "ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€"
    
    # ì‚¬ìš© ì˜ˆ
    smart_llm = SmartLLM()
    response = smart_llm.ask("ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
    print(f"\nìµœì¢… ì‘ë‹µ: {response[:100]}...")


def main():
    """ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ ë©”ì¸ í•¨ìˆ˜"""
    
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    print("ğŸ›¡ï¸ ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ")
    print("=" * 50)
    
    try:
        # 1. ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬
        example_basic_error_handling()
        
        # 2. ì¬ì‹œë„ ë¡œì§
        example_retry_logic()
        
        # 3. í´ë°± ì²˜ë¦¬
        example_fallback_handling()
        
        # 4. íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
        example_timeout_handling()
        
        # 5. ìš°ì•„í•œ ì„±ëŠ¥ ì €í•˜
        example_graceful_degradation()
        
        print("\nâœ… ëª¨ë“  ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()