#!/usr/bin/env python3
"""
ì˜ˆì œ: ë¹„ë™ê¸° ì²˜ë¦¬
ë‚œì´ë„: ì¤‘ê¸‰
ì„¤ëª…: ë¹„ë™ê¸°ë¡œ ì—¬ëŸ¬ ì‘ì—…ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ë°©ë²•
ìš”êµ¬ì‚¬í•­: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜

ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ me@pyhub.krë¡œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
import time

from pyhub.llm import LLM


async def async_question(llm, question: str, index: int) -> tuple:
    """ë¹„ë™ê¸°ë¡œ ì§ˆë¬¸ ì²˜ë¦¬"""
    start_time = time.time()
    print(f"ğŸš€ ì‘ì—… {index} ì‹œì‘: {question[:30]}...")

    # ë¹„ë™ê¸° ask í˜¸ì¶œ
    reply = await llm.ask_async(question)

    end_time = time.time()
    elapsed = end_time - start_time

    print(f"âœ… ì‘ì—… {index} ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
    return index, question, reply.text, elapsed


async def parallel_processing():
    """ë³‘ë ¬ ì²˜ë¦¬ ì˜ˆì œ"""
    print("\nâš¡ ë³‘ë ¬ ì²˜ë¦¬ ì˜ˆì œ")
    print("-" * 50)

    # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = LLM.create("gpt-4o-mini")

    # ì—¬ëŸ¬ ì§ˆë¬¸ ì¤€ë¹„
    questions = [
        "íŒŒì´ì¬ì˜ ì¥ì ì„ 3ê°€ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "REST APIì˜ ì£¼ìš” íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "Gitê³¼ GitHubì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "Dockerì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    ]

    print(f"ì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")

    # ìˆœì°¨ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
    print("1ï¸âƒ£ ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    sequential_start = time.time()

    for i, question in enumerate(questions, 1):
        _ = llm.ask(question)
        print(f"  - ì§ˆë¬¸ {i} ì™„ë£Œ")

    sequential_time = time.time() - sequential_start
    print(f"ìˆœì°¨ ì²˜ë¦¬ ì´ ì‹œê°„: {sequential_time:.2f}ì´ˆ\n")

    # ë³‘ë ¬ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
    print("2ï¸âƒ£ ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    parallel_start = time.time()

    # ëª¨ë“  ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
    tasks = [async_question(llm, question, i) for i, question in enumerate(questions, 1)]

    results = await asyncio.gather(*tasks)

    parallel_time = time.time() - parallel_start
    print(f"\në³‘ë ¬ ì²˜ë¦¬ ì´ ì‹œê°„: {parallel_time:.2f}ì´ˆ")
    print(f"ğŸ¯ ì†ë„ í–¥ìƒ: {sequential_time/parallel_time:.2f}ë°° ë¹ ë¦„!")

    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
    for index, question, answer, elapsed in results:
        print(f"\nQ{index}: {question}")
        print(f"A: {answer[:100]}...")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")


async def streaming_async():
    """ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì œ"""
    print("\nğŸŒŠ ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì œ")
    print("-" * 50)

    llm = LLM.create("gpt-4o-mini")
    prompt = "ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì˜ ì¥ì ì„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."

    print(f"í”„ë¡¬í”„íŠ¸: {prompt}\n")
    print("ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ:")
    print("-" * 30)

    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
    full_response = ""
    async for chunk in llm.ask_async(prompt, stream=True):
        print(chunk.text, end="", flush=True)
        full_response += chunk.text

    print("\n" + "-" * 30)
    print(f"\nì´ {len(full_response)}ì ìƒì„±ë¨")


async def concurrent_conversations():
    """ë™ì‹œ ëŒ€í™” ì²˜ë¦¬ ì˜ˆì œ"""
    print("\nğŸ’¬ ë™ì‹œ ëŒ€í™” ì²˜ë¦¬ ì˜ˆì œ")
    print("-" * 50)

    # ì—¬ëŸ¬ LLM ì¸ìŠ¤í„´ìŠ¤ë¡œ ë‹¤ë¥¸ í˜ë¥´ì†Œë‚˜ ìƒì„±
    poet_llm = LLM.create("gpt-4o-mini", system_prompt="ë‹¹ì‹ ì€ ê°ì„±ì ì¸ ì‹œì¸ì…ë‹ˆë‹¤.")
    scientist_llm = LLM.create("gpt-4o-mini", system_prompt="ë‹¹ì‹ ì€ ë…¼ë¦¬ì ì¸ ê³¼í•™ìì…ë‹ˆë‹¤.")
    philosopher_llm = LLM.create("gpt-4o-mini", system_prompt="ë‹¹ì‹ ì€ ê¹Šì´ ìˆëŠ” ì² í•™ìì…ë‹ˆë‹¤.")

    topic = "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜"
    print(f"ì£¼ì œ: {topic}\n")

    # ë™ì‹œì— ì„¸ ê´€ì ì—ì„œ ë‹µë³€ ìƒì„±
    tasks = [
        poet_llm.ask_async(f"{topic}ì— ëŒ€í•´ ì‹œì ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”."),
        scientist_llm.ask_async(f"{topic}ì— ëŒ€í•´ ê³¼í•™ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."),
        philosopher_llm.ask_async(f"{topic}ì— ëŒ€í•´ ì² í•™ì ìœ¼ë¡œ ê³ ì°°í•´ì£¼ì„¸ìš”."),
    ]

    print("ì„¸ ê°€ì§€ ê´€ì ì—ì„œ ë™ì‹œì— ë‹µë³€ ìƒì„± ì¤‘...\n")

    poet_reply, scientist_reply, philosopher_reply = await asyncio.gather(*tasks)

    print("ğŸ­ ì‹œì¸ì˜ ê´€ì :")
    print(poet_reply.text)
    print("\nğŸ”¬ ê³¼í•™ìì˜ ê´€ì :")
    print(scientist_reply.text)
    print("\nğŸ¤” ì² í•™ìì˜ ê´€ì :")
    print(philosopher_reply.text)


async def error_handling_async():
    """ë¹„ë™ê¸° ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ"""
    print("\nâš ï¸  ë¹„ë™ê¸° ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ")
    print("-" * 50)

    llm = LLM.create("gpt-4o-mini")

    # ì—¬ëŸ¬ ì‘ì—… ì¤‘ ì¼ë¶€ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆëŠ” ìƒí™©
    tasks = [
        llm.ask_async("ì •ìƒì ì¸ ì§ˆë¬¸ì…ë‹ˆë‹¤."),
        llm.ask_async("ì´ê²ƒë„ ì •ìƒì ì¸ ì§ˆë¬¸ì…ë‹ˆë‹¤."),
        # ì˜ë„ì ìœ¼ë¡œ ë§¤ìš° ê¸´ í”„ë¡¬í”„íŠ¸ë¡œ ì—ëŸ¬ ìœ ë°œ ê°€ëŠ¥ì„±
        llm.ask_async("A" * 100000),  # í† í° ì œí•œ ì´ˆê³¼ ê°€ëŠ¥
    ]

    # gather with return_exceptions=True
    results = await asyncio.gather(*tasks, return_exceptions=True)

    for i, result in enumerate(results, 1):
        if isinstance(result, Exception):
            print(f"âŒ ì‘ì—… {i} ì‹¤íŒ¨: {type(result).__name__}: {str(result)[:100]}")
        else:
            print(f"âœ… ì‘ì—… {i} ì„±ê³µ: {result.text[:50]}...")


async def main():
    """ë¹„ë™ê¸° ì²˜ë¦¬ ì˜ˆì œ ë©”ì¸ í•¨ìˆ˜"""

    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    print("ğŸ”„ ë¹„ë™ê¸° ì²˜ë¦¬ ì˜ˆì œ")
    print("=" * 50)

    try:
        # 1. ë³‘ë ¬ ì²˜ë¦¬ ì˜ˆì œ
        await parallel_processing()

        # 2. ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì œ
        await streaming_async()

        # 3. ë™ì‹œ ëŒ€í™” ì²˜ë¦¬ ì˜ˆì œ
        await concurrent_conversations()

        # 4. ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ
        await error_handling_async()

        print("\nâœ… ëª¨ë“  ë¹„ë™ê¸° ì˜ˆì œ ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    # ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    asyncio.run(main())
