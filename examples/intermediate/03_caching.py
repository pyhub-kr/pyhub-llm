#!/usr/bin/env python3
"""
ì˜ˆì œ: ìºì‹±
ë‚œì´ë„: ì¤‘ê¸‰
ì„¤ëª…: ì‘ë‹µ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ë° ë¹„ìš© ì ˆê°
ìš”êµ¬ì‚¬í•­: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜

ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ me@pyhub.krë¡œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
"""

import json
import os
import time
from datetime import datetime
from pathlib import Path

from pyhub.llm import LLM
from pyhub.llm.cache import BaseCache, FileCache, MemoryCache


class CustomJSONCache(BaseCache):
    """ì»¤ìŠ¤í…€ JSON ìºì‹œ êµ¬í˜„ ì˜ˆì œ"""

    def __init__(self, cache_file="custom_cache.json"):
        self.cache_file = cache_file
        self.cache_data = {}
        self.load_cache()

    def load_cache(self):
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if Path(self.cache_file).exists():
            with open(self.cache_file, "r", encoding="utf-8") as f:
                self.cache_data = json.load(f)

    def save_cache(self):
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        with open(self.cache_file, "w", encoding="utf-8") as f:
            json.dump(self.cache_data, f, ensure_ascii=False, indent=2)

    def get(self, key: str):
        """ìºì‹œì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        if key in self.cache_data:
            entry = self.cache_data[key]
            # TTL ì²´í¬
            if "expires_at" in entry:
                if datetime.fromisoformat(entry["expires_at"]) > datetime.now():
                    print(f"ğŸ’¾ ìºì‹œ íˆíŠ¸: {key[:50]}...")
                    return entry["value"]
                else:
                    print(f"â° ìºì‹œ ë§Œë£Œ: {key[:50]}...")
                    del self.cache_data[key]
                    self.save_cache()
        return None

    def set(self, key: str, value, ttl: int = 3600):
        """ìºì‹œì— ê°’ ì €ì¥"""
        expires_at = datetime.now().timestamp() + ttl
        self.cache_data[key] = {
            "value": value,
            "expires_at": datetime.fromtimestamp(expires_at).isoformat(),
            "created_at": datetime.now().isoformat(),
        }
        self.save_cache()
        print(f"ğŸ’¾ ìºì‹œ ì €ì¥: {key[:50]}...")

    def clear(self):
        """ìºì‹œ í´ë¦¬ì–´"""
        self.cache_data = {}
        if Path(self.cache_file).exists():
            Path(self.cache_file).unlink()
        print("ğŸ—‘ï¸  ìºì‹œ í´ë¦¬ì–´ ì™„ë£Œ")


def example_memory_cache():
    """ë©”ëª¨ë¦¬ ìºì‹œ ì˜ˆì œ"""
    print("\nğŸ§  ë©”ëª¨ë¦¬ ìºì‹œ ì˜ˆì œ")
    print("-" * 50)

    # ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©
    cache = MemoryCache()
    llm = LLM.create("gpt-4o-mini", cache=cache)

    # ê°™ì€ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰
    question = "íŒŒì´ì¬ì˜ ë°ì½”ë ˆì´í„°ë€ ë¬´ì—‡ì¸ê°€ìš”?"

    print(f"ì§ˆë¬¸: {question}\n")

    # ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤)
    start = time.time()
    reply1 = llm.ask(question)
    time1 = time.time() - start
    print(f"1ì°¨ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤): {time1:.2f}ì´ˆ")
    print(f"ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {reply1.text[:100]}...\n")

    # ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸)
    start = time.time()
    reply2 = llm.ask(question)
    time2 = time.time() - start
    print(f"2ì°¨ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸): {time2:.2f}ì´ˆ")
    print(f"ì†ë„ í–¥ìƒ: {time1/time2:.0f}ë°° ë¹ ë¦„!")

    # ì‘ë‹µì´ ë™ì¼í•œì§€ í™•ì¸
    print(f"ì‘ë‹µ ë™ì¼: {reply1.text == reply2.text}")


def example_file_cache():
    """íŒŒì¼ ìºì‹œ ì˜ˆì œ"""
    print("\nğŸ“ íŒŒì¼ ìºì‹œ ì˜ˆì œ")
    print("-" * 50)

    # íŒŒì¼ ìºì‹œ ì‚¬ìš© (ìºì‹œ ë””ë ‰í† ë¦¬ ì§€ì •)
    cache_dir = "./llm_cache"
    cache = FileCache(cache_dir=cache_dir)
    llm = LLM.create("gpt-4o-mini", cache=cache)

    print(f"ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}\n")

    # ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬
    questions = [
        "ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë”¥ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",  # ì¤‘ë³µ ì§ˆë¬¸
    ]

    for i, question in enumerate(questions, 1):
        print(f"\nì§ˆë¬¸ {i}: {question}")
        start = time.time()
        reply = llm.ask(question)
        elapsed = time.time() - start
        print(f"ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print(f"ì‘ë‹µ ê¸¸ì´: {len(reply.text)}ì")

    # ìºì‹œ íŒŒì¼ í™•ì¸
    cache_files = list(Path(cache_dir).glob("*.cache"))
    print(f"\nğŸ“Š ìƒì„±ëœ ìºì‹œ íŒŒì¼: {len(cache_files)}ê°œ")


def example_custom_cache():
    """ì»¤ìŠ¤í…€ ìºì‹œ ì˜ˆì œ"""
    print("\nğŸ› ï¸  ì»¤ìŠ¤í…€ JSON ìºì‹œ ì˜ˆì œ")
    print("-" * 50)

    # ì»¤ìŠ¤í…€ ìºì‹œ ì‚¬ìš©
    cache = CustomJSONCache("my_llm_cache.json")
    llm = LLM.create("gpt-4o-mini", cache=cache)

    # TTL í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì§ˆë¬¸
    question = "ìºì‹±ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"

    print(f"ì§ˆë¬¸: {question}\n")

    # ì²« ë²ˆì§¸ í˜¸ì¶œ
    print("1ï¸âƒ£ ì²« ë²ˆì§¸ í˜¸ì¶œ")
    reply = llm.ask(question)
    print(f"ì‘ë‹µ: {reply.text[:100]}...\n")

    # ìºì‹œ íŒŒì¼ ë‚´ìš© í™•ì¸
    if Path("my_llm_cache.json").exists():
        with open("my_llm_cache.json", "r", encoding="utf-8") as f:
            cache_content = json.load(f)
        print("ğŸ“„ ìºì‹œ íŒŒì¼ ë‚´ìš©:")
        for key, value in list(cache_content.items())[:1]:  # ì²« ë²ˆì§¸ í•­ëª©ë§Œ
            print(f"  - í‚¤: {key[:50]}...")
            print(f"  - ìƒì„± ì‹œê°„: {value['created_at']}")
            print(f"  - ë§Œë£Œ ì‹œê°„: {value['expires_at']}")

    # ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸)
    print("\n2ï¸âƒ£ ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œì—ì„œ ë¡œë“œ)")
    _ = llm.ask(question)

    # ìºì‹œ í´ë¦¬ì–´
    print("\nğŸ—‘ï¸  ìºì‹œ í´ë¦¬ì–´")
    cache.clear()


def example_cache_strategy():
    """ìºì‹œ ì „ëµ ì˜ˆì œ"""
    print("\nğŸ“ˆ ìºì‹œ ì „ëµ ì˜ˆì œ")
    print("-" * 50)

    # ì§§ì€ TTLì˜ ë©”ëª¨ë¦¬ ìºì‹œ
    short_cache = MemoryCache(ttl=5)  # 5ì´ˆ TTL
    llm_short = LLM.create("gpt-4o-mini", cache=short_cache)

    question = "í˜„ì¬ ì‹œê°„ì€ ëª‡ ì‹œì¸ê°€ìš”?"

    print(f"ì§ˆë¬¸: {question}")
    print("(5ì´ˆ TTL ìºì‹œ ì‚¬ìš©)\n")

    # ì²« ë²ˆì§¸ í˜¸ì¶œ
    print("1ï¸âƒ£ ì²« ë²ˆì§¸ í˜¸ì¶œ")
    reply1 = llm_short.ask(question)
    print(f"ì‘ë‹µ: {reply1.text}\n")

    # 3ì´ˆ í›„ (ìºì‹œ ìœ íš¨)
    print("â±ï¸  3ì´ˆ ëŒ€ê¸° ì¤‘...")
    time.sleep(3)
    print("2ï¸âƒ£ 3ì´ˆ í›„ í˜¸ì¶œ (ìºì‹œ ìœ íš¨)")
    reply2 = llm_short.ask(question)
    print(f"ì‘ë‹µ: {reply2.text}")
    print(f"ìºì‹œ ì‚¬ìš©: {reply1.text == reply2.text}\n")

    # 3ì´ˆ ë” ëŒ€ê¸° (ì´ 6ì´ˆ, ìºì‹œ ë§Œë£Œ)
    print("â±ï¸  3ì´ˆ ë” ëŒ€ê¸° ì¤‘...")
    time.sleep(3)
    print("3ï¸âƒ£ 6ì´ˆ í›„ í˜¸ì¶œ (ìºì‹œ ë§Œë£Œ)")
    reply3 = llm_short.ask(question)
    print(f"ì‘ë‹µ: {reply3.text}")
    print(f"ìƒˆë¡œìš´ ì‘ë‹µ: {reply1.text != reply3.text}")


def example_cache_statistics():
    """ìºì‹œ í†µê³„ ì˜ˆì œ"""
    print("\nğŸ“Š ìºì‹œ í†µê³„ ì˜ˆì œ")
    print("-" * 50)

    # í†µê³„ ê¸°ëŠ¥ì´ ìˆëŠ” ìºì‹œ
    class StatisticsCache(MemoryCache):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.stats = {"hits": 0, "misses": 0}

        def get(self, key: str):
            result = super().get(key)
            if result is not None:
                self.stats["hits"] += 1
            else:
                self.stats["misses"] += 1
            return result

        def get_statistics(self):
            total = self.stats["hits"] + self.stats["misses"]
            hit_rate = (self.stats["hits"] / total * 100) if total > 0 else 0
            return {
                "total_requests": total,
                "cache_hits": self.stats["hits"],
                "cache_misses": self.stats["misses"],
                "hit_rate": f"{hit_rate:.1f}%",
            }

    # í†µê³„ ìºì‹œ ì‚¬ìš©
    stats_cache = StatisticsCache()
    llm = LLM.create("gpt-4o-mini", cache=stats_cache)

    # ë‹¤ì–‘í•œ ì§ˆë¬¸ë“¤
    questions = [
        "Pythonì´ë€?",
        "JavaScriptë€?",
        "Pythonì´ë€?",  # ì¤‘ë³µ
        "Javaë€?",
        "Pythonì´ë€?",  # ì¤‘ë³µ
        "JavaScriptë€?",  # ì¤‘ë³µ
    ]

    print("ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...\n")
    for q in questions:
        _ = llm.ask(q)
        print(f"âœ“ {q}")

    # í†µê³„ ì¶œë ¥
    stats = stats_cache.get_statistics()
    print("\nğŸ“ˆ ìºì‹œ í†µê³„:")
    print(f"  - ì „ì²´ ìš”ì²­: {stats['total_requests']}")
    print(f"  - ìºì‹œ íˆíŠ¸: {stats['cache_hits']}")
    print(f"  - ìºì‹œ ë¯¸ìŠ¤: {stats['cache_misses']}")
    print(f"  - íˆíŠ¸ìœ¨: {stats['hit_rate']}")


def main():
    """ìºì‹± ì˜ˆì œ ë©”ì¸ í•¨ìˆ˜"""

    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    print("ğŸ’¾ ìºì‹± ì˜ˆì œ")
    print("=" * 50)

    try:
        # 1. ë©”ëª¨ë¦¬ ìºì‹œ ì˜ˆì œ
        example_memory_cache()

        # 2. íŒŒì¼ ìºì‹œ ì˜ˆì œ
        example_file_cache()

        # 3. ì»¤ìŠ¤í…€ ìºì‹œ ì˜ˆì œ
        example_custom_cache()

        # 4. ìºì‹œ ì „ëµ ì˜ˆì œ
        example_cache_strategy()

        # 5. ìºì‹œ í†µê³„ ì˜ˆì œ
        example_cache_statistics()

        print("\nâœ… ëª¨ë“  ìºì‹± ì˜ˆì œ ì™„ë£Œ!")

        # ì •ë¦¬
        print("\nğŸ§¹ ì •ë¦¬ ì¤‘...")
        # ìƒì„±ëœ ìºì‹œ íŒŒì¼ë“¤ ì‚­ì œ (ì„ íƒì‚¬í•­)
        cache_files = ["my_llm_cache.json"]
        for file in cache_files:
            if Path(file).exists():
                Path(file).unlink()
                print(f"  - {file} ì‚­ì œë¨")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
