"""
Python í´ë¼ì´ì–¸íŠ¸ ì˜ˆì œ

pyhub-llm FastAPI ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” Python í´ë¼ì´ì–¸íŠ¸ ì˜ˆì œì…ë‹ˆë‹¤.
"""

import asyncio
import httpx
from typing import List, Optional, Dict, Any


class PyHubLLMClient:
    """pyhub-llm FastAPI ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(
        self, 
        base_url: str = "http://localhost:8000",
        api_key: Optional[str] = None,
        timeout: float = 30.0
    ):
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        self.headers = {"Content-Type": "application/json"}
        
        if api_key:
            self.headers["Authorization"] = f"Bearer {api_key}"
    
    async def chat(
        self, 
        message: str, 
        model: str = "gpt-4o-mini",
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        protected: bool = False
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ì±„íŒ… ìš”ì²­"""
        endpoint = "/api/chat" if protected else "/chat"
        
        payload = {
            "message": message,
            "model": model
        }
        
        if system_prompt:
            payload["system_prompt"] = system_prompt
        if temperature is not None:
            payload["temperature"] = temperature
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                f"{self.base_url}{endpoint}",
                headers=self.headers,
                json=payload
            )
            response.raise_for_status()
            return response.json()
    
    async def batch(
        self,
        messages: List[str],
        model: str = "gpt-4o-mini",
        max_parallel: int = 3,
        history_mode: str = "independent",
        protected: bool = False
    ) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ ìš”ì²­"""
        endpoint = "/api/batch" if protected else "/batch"
        
        payload = {
            "messages": messages,
            "model": model,
            "max_parallel": max_parallel,
            "history_mode": history_mode
        }
        
        async with httpx.AsyncClient(timeout=self.timeout * len(messages)) as client:
            response = await client.post(
                f"{self.base_url}{endpoint}",
                headers=self.headers,
                json=payload
            )
            response.raise_for_status()
            return response.json()
    
    async def translate(
        self,
        text: str,
        target_language: str,
        source_language: Optional[str] = None,
        model: str = "gpt-4o-mini"
    ) -> Dict[str, Any]:
        """ë²ˆì—­ ìš”ì²­"""
        payload = {
            "text": text,
            "target_language": target_language,
            "model": model
        }
        
        if source_language:
            payload["source_language"] = source_language
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                f"{self.base_url}/api/translate",
                headers=self.headers,
                json=payload
            )
            response.raise_for_status()
            return response.json()
    
    async def summarize(
        self,
        text: str,
        max_length: int = 200,
        language: str = "ko",
        model: str = "gpt-4o-mini"
    ) -> Dict[str, Any]:
        """ìš”ì•½ ìš”ì²­"""
        payload = {
            "text": text,
            "max_length": max_length,
            "language": language,
            "model": model
        }
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                f"{self.base_url}/api/summarize",
                headers=self.headers,
                json=payload
            )
            response.raise_for_status()
            return response.json()
    
    async def get_supported_languages(self) -> Dict[str, Any]:
        """ì§€ì›í•˜ëŠ” ì–¸ì–´ ëª©ë¡ ì¡°íšŒ"""
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.get(
                f"{self.base_url}/api/supported-languages",
                headers=self.headers
            )
            response.raise_for_status()
            return response.json()
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬"""
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{self.base_url}/health")
            response.raise_for_status()
            return response.json()


# =============================================================================
# ì‚¬ìš© ì˜ˆì œ
# =============================================================================

async def basic_examples():
    """ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ"""
    print("ğŸš€ ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ")
    
    # í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì¸ì¦ ì—†ì´)
    client = PyHubLLMClient()
    
    # í—¬ìŠ¤ì²´í¬
    health = await client.health_check()
    print(f"âœ… ì„œë¹„ìŠ¤ ìƒíƒœ: {health['status']}")
    
    # ë‹¨ì¼ ì±„íŒ…
    print("\nğŸ’¬ ë‹¨ì¼ ì±„íŒ… ì˜ˆì œ:")
    chat_result = await client.chat(
        message="FastAPIì™€ pyhub-llmì˜ ì¡°í•©ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        model="gpt-4o-mini"
    )
    print(f"ì‘ë‹µ: {chat_result['response'][:100]}...")
    
    # ë°°ì¹˜ ì²˜ë¦¬
    print("\nğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ:")
    questions = [
        "Pythonì˜ ì£¼ìš” íŠ¹ì§• 3ê°€ì§€ëŠ”?",
        "FastAPIë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?",
        "LLM API ì„œë¹„ìŠ¤ì˜ ì¥ì ì€?"
    ]
    
    batch_result = await client.batch(questions)
    print(f"âœ… {batch_result['success_count']}/{batch_result['total_count']} ì„±ê³µ")
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {batch_result['execution_time']:.2f}ì´ˆ")
    
    for i, response in enumerate(batch_result['responses']):
        print(f"Q{i+1}: {questions[i]}")
        print(f"A{i+1}: {response['response'][:80]}...")
        print()


async def authenticated_examples():
    """ì¸ì¦ì´ í•„ìš”í•œ ì˜ˆì œ"""
    print("ğŸ” ì¸ì¦ ì˜ˆì œ")
    
    # API í‚¤ê°€ í•„ìš”í•œ í´ë¼ì´ì–¸íŠ¸
    api_key = "demo-key-12345"  # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
    client = PyHubLLMClient(api_key=api_key)
    
    try:
        # ë³´í˜¸ëœ ì±„íŒ…
        print("\nğŸ›¡ï¸ ë³´í˜¸ëœ ì±„íŒ…:")
        protected_result = await client.chat(
            message="ì¸ì¦ì´ í•„ìš”í•œ ë³´ì•ˆ ì±„íŒ…ì…ë‹ˆë‹¤!",
            protected=True
        )
        print(f"ì‘ë‹µ: {protected_result['response'][:100]}...")
        
        # ë²ˆì—­ ì„œë¹„ìŠ¤
        print("\nğŸŒ ë²ˆì—­ ì„œë¹„ìŠ¤:")
        translation = await client.translate(
            text="Hello, how are you today?",
            target_language="ko"
        )
        print(f"ì›ë¬¸: {translation['original_text']}")
        print(f"ë²ˆì—­: {translation['translated_text']}")
        
        # ìš”ì•½ ì„œë¹„ìŠ¤
        print("\nğŸ“ ìš”ì•½ ì„œë¹„ìŠ¤:")
        long_text = """
        FastAPIëŠ” Pythonìœ¼ë¡œ APIë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í˜„ëŒ€ì ì´ê³  ë¹ ë¥¸ ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
        ì´ í”„ë ˆì„ì›Œí¬ëŠ” í‘œì¤€ Python íƒ€ì… íŒíŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ìë™ ë¬¸ì„œ ìƒì„±,
        ë°ì´í„° ê²€ì¦, ì§ë ¬í™” ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ ë†’ì€ ì„±ëŠ¥ì„ ìë‘í•˜ë©°
        NodeJS ë° Goì™€ ê²¬ì¤„ ë§Œí•œ ì†ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. pyhub-llmê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´
        ê°•ë ¥í•œ LLM ê¸°ë°˜ ì›¹ ì„œë¹„ìŠ¤ë¥¼ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        
        summary = await client.summarize(
            text=long_text.strip(),
            max_length=100
        )
        print(f"ì›ë³¸ ê¸¸ì´: {summary['original_length']}ì")
        print(f"ìš”ì•½ ê¸¸ì´: {summary['summary_length']}ì")
        print(f"ì••ì¶•ë¥ : {summary['compression_ratio']:.2%}")
        print(f"ìš”ì•½: {summary['summary']}")
        
        # ì§€ì› ì–¸ì–´ ëª©ë¡
        print("\nğŸŒ ì§€ì› ì–¸ì–´:")
        languages = await client.get_supported_languages()
        if languages.get('premium_user'):
            print(f"âœ¨ í”„ë¦¬ë¯¸ì—„ ì‚¬ìš©ì: {languages['total_count']}ê°œ ì–¸ì–´ ì§€ì›")
        else:
            print(f"ğŸ“š ê¸°ë³¸ ì‚¬ìš©ì: {languages['total_count']}ê°œ ì–¸ì–´ ì§€ì›")
        
    except httpx.HTTPStatusError as e:
        print(f"âŒ HTTP ì˜¤ë¥˜: {e.response.status_code}")
        print(f"ìƒì„¸: {e.response.text}")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {str(e)}")


async def advanced_batch_examples():
    """ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ"""
    print("âš¡ ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ")
    
    client = PyHubLLMClient()
    
    # ìˆœì°¨ì  ëŒ€í™” (ê° ì‘ë‹µì´ ë‹¤ìŒ ì§ˆë¬¸ì˜ ì»¨í…ìŠ¤íŠ¸ê°€ ë¨)
    print("\nğŸ”— ìˆœì°¨ì  ëŒ€í™”:")
    sequential_questions = [
        "í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì´ê²ƒì„ Pythonìœ¼ë¡œ êµ¬í˜„í•´ì£¼ì„¸ìš”.",
        "ì´ êµ¬í˜„ì˜ ì‹œê°„ ë³µì¡ë„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ë” íš¨ìœ¨ì ì¸ ë°©ë²•ì´ ìˆì„ê¹Œìš”?"
    ]
    
    sequential_result = await client.batch(
        messages=sequential_questions,
        history_mode="sequential"
    )
    
    print("ğŸ“ˆ ìˆœì°¨ì  ëŒ€í™” ê²°ê³¼:")
    for i, (question, response) in enumerate(zip(sequential_questions, sequential_result['responses'])):
        print(f"\në‹¨ê³„ {i+1}: {question}")
        print(f"ì‘ë‹µ: {response['response'][:120]}...")
    
    # ê³µìœ  ì»¨í…ìŠ¤íŠ¸ (ëª¨ë“  ì§ˆë¬¸ì´ ë™ì¼í•œ ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸ ê³µìœ )
    print("\nğŸ¤ ê³µìœ  ì»¨í…ìŠ¤íŠ¸ ë°°ì¹˜:")
    context_questions = [
        "FastAPIì˜ ì£¼ìš” íŠ¹ì§•ì€?",
        "pyhub-llmê³¼ì˜ ì—°ë™ ë°©ë²•ì€?",
        "ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œ ê³ ë ¤í•  ì ì€?"
    ]
    
    shared_result = await client.batch(
        messages=context_questions,
        history_mode="shared"
    )
    
    print("ğŸŒŸ ê³µìœ  ì»¨í…ìŠ¤íŠ¸ ê²°ê³¼:")
    for i, (question, response) in enumerate(zip(context_questions, shared_result['responses'])):
        print(f"\nQ{i+1}: {question}")
        print(f"A{i+1}: {response['response'][:100]}...")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒŸ pyhub-llm FastAPI í´ë¼ì´ì–¸íŠ¸ ì˜ˆì œ")
    print("=" * 50)
    
    try:
        await basic_examples()
        print("\n" + "=" * 50)
        
        await authenticated_examples()
        print("\n" + "=" * 50)
        
        await advanced_batch_examples()
        
    except httpx.ConnectError:
        print("âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë¨¼ì € FastAPI ì„œë²„ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        print("   python main.py  ë˜ëŠ”  python advanced.py")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
    
    print("\nâœ… ëª¨ë“  ì˜ˆì œ ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(main())