"""
FastAPI + pyhub-llm ê¸°ë³¸ í†µí•© ì˜ˆì œ

ì´ íŒŒì¼ì€ pyhub-llmì„ FastAPIì™€ ì—°ë™í•˜ëŠ” ê¸°ë³¸ì ì¸ ì˜ˆì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬, ë°°ì¹˜ ì²˜ë¦¬, ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë“±ì˜ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.
"""

import asyncio
import os
from typing import List, Optional, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import uvicorn

from pyhub.llm import LLM


# =============================================================================
# Pydantic ëª¨ë¸ ì •ì˜
# =============================================================================

class ChatRequest(BaseModel):
    """ë‹¨ì¼ ì±„íŒ… ìš”ì²­"""
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€", min_length=1)
    model: str = Field(default="gpt-4o-mini", description="ì‚¬ìš©í•  LLM ëª¨ë¸")
    system_prompt: Optional[str] = Field(None, description="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0, description="ìƒì„± ì˜¨ë„")
    max_tokens: Optional[int] = Field(None, gt=0, description="ìµœëŒ€ í† í° ìˆ˜")


class BatchRequest(BaseModel):
    """ë°°ì¹˜ ì²˜ë¦¬ ìš”ì²­"""
    messages: List[str] = Field(..., description="ì²˜ë¦¬í•  ë©”ì‹œì§€ ëª©ë¡", min_items=1, max_items=10)
    model: str = Field(default="gpt-4o-mini", description="ì‚¬ìš©í•  LLM ëª¨ë¸")
    system_prompt: Optional[str] = Field(None, description="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
    max_parallel: int = Field(default=3, ge=1, le=5, description="ìµœëŒ€ ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜")
    history_mode: str = Field(default="independent", description="íˆìŠ¤í† ë¦¬ ëª¨ë“œ")


class SessionChatRequest(BaseModel):
    """ì„¸ì…˜ ê¸°ë°˜ ì±„íŒ… ìš”ì²­"""
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€", min_length=1)
    session_id: str = Field(..., description="ì„¸ì…˜ ID", min_length=1)
    model: str = Field(default="gpt-4o-mini", description="ì‚¬ìš©í•  LLM ëª¨ë¸")


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ"""
    response: str = Field(..., description="LLM ì‘ë‹µ")
    model: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸")
    usage: Optional[Dict[str, Any]] = Field(None, description="í† í° ì‚¬ìš©ëŸ‰")


class BatchResponse(BaseModel):
    """ë°°ì¹˜ ì²˜ë¦¬ ì‘ë‹µ"""
    responses: List[ChatResponse] = Field(..., description="ê° ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ")
    total_count: int = Field(..., description="ì²˜ë¦¬ëœ ë©”ì‹œì§€ ìˆ˜")
    success_count: int = Field(..., description="ì„±ê³µí•œ ë©”ì‹œì§€ ìˆ˜")
    execution_time: float = Field(..., description="ì´ ì‹¤í–‰ ì‹œê°„(ì´ˆ)")


# =============================================================================
# ì „ì—­ ë³€ìˆ˜ ë° ì„¤ì •
# =============================================================================

# LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì €ì¥í•  ì „ì—­ ë”•ì…”ë„ˆë¦¬
llm_instances: Dict[str, Any] = {}

# ê°„ë‹¨í•œ ì¸ë©”ëª¨ë¦¬ ì„¸ì…˜ ì €ì¥ì†Œ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Redis ë“± ì‚¬ìš© ê¶Œì¥)
sessions: Dict[str, Any] = {}


# =============================================================================
# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
# =============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë  ë¡œì§"""
    # ì‹œì‘ ì‹œ: ìì£¼ ì‚¬ìš©ë˜ëŠ” LLM ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
    print("ğŸš€ FastAPI + pyhub-llm ì„œë¹„ìŠ¤ ì‹œì‘")
    print("ğŸ“¡ LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    
    try:
        # ê¸°ë³¸ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
        llm_instances["gpt-4o-mini"] = LLM.create("gpt-4o-mini")
        print("âœ… gpt-4o-mini ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    yield
    
    # ì¢…ë£Œ ì‹œ: ì •ë¦¬ ì‘ì—…
    print("ğŸ›‘ FastAPI + pyhub-llm ì„œë¹„ìŠ¤ ì¢…ë£Œ")
    llm_instances.clear()
    sessions.clear()


app = FastAPI(
    title="pyhub-llm FastAPI Integration",
    description="pyhub-llmì„ FastAPIì™€ ì—°ë™í•œ ì˜ˆì œ ì„œë¹„ìŠ¤",
    version="1.0.0",
    lifespan=lifespan
)


# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================

def get_llm_instance(model: str) -> Any:
    """LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±"""
    if model not in llm_instances:
        try:
            llm_instances[model] = LLM.create(model)
        except Exception as e:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model}. ì˜¤ë¥˜: {str(e)}"
            )
    return llm_instances[model]


def validate_api_key():
    """API í‚¤ ê²€ì¦ (ê°„ë‹¨í•œ ì˜ˆì œ)"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise HTTPException(
            status_code=500,
            detail="API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
        )


# =============================================================================
# API ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "service": "pyhub-llm FastAPI Integration",
        "status": "running",
        "endpoints": {
            "chat": "/chat - ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬",
            "batch": "/batch - ë°°ì¹˜ ì²˜ë¦¬", 
            "stream": "/stream - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ",
            "session": "/chat/session - ì„¸ì…˜ ê¸°ë°˜ ì±„íŒ…",
            "docs": "/docs - API ë¬¸ì„œ"
        }
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "models_loaded": list(llm_instances.keys()),
        "active_sessions": len(sessions)
    }


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
    
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ LLMì´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    validate_api_key()
    
    try:
        # LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        llm = get_llm_instance(request.model)
        
        # LLM í˜¸ì¶œ
        response = await llm.ask_async(
            input=request.message,
            system_prompt=request.system_prompt,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        return ChatResponse(
            response=response.text,
            model=request.model,
            usage=response.usage.__dict__ if response.usage else None
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"LLM ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.post("/batch", response_model=BatchResponse)
async def batch_process(request: BatchRequest):
    """
    ë°°ì¹˜ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
    
    ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    """
    validate_api_key()
    
    import time
    start_time = time.time()
    
    try:
        # LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        llm = get_llm_instance(request.model)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        responses = await llm.batch(
            prompts=request.messages,
            system_prompt=request.system_prompt,
            max_parallel=request.max_parallel,
            history_mode=request.history_mode
        )
        
        # ì‘ë‹µ ë³€í™˜
        chat_responses = []
        success_count = 0
        
        for i, response in enumerate(responses):
            if "Error processing prompt" not in response.text:
                success_count += 1
            
            chat_responses.append(ChatResponse(
                response=response.text,
                model=request.model,
                usage=response.usage.__dict__ if response.usage else None
            ))
        
        execution_time = time.time() - start_time
        
        return BatchResponse(
            responses=chat_responses,
            total_count=len(request.messages),
            success_count=success_count,
            execution_time=execution_time
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.post("/stream")
async def stream_chat(request: ChatRequest):
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—”ë“œí¬ì¸íŠ¸
    
    LLM ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    """
    validate_api_key()
    
    try:
        # LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        llm = get_llm_instance(request.model)
        
        async def generate_stream():
            """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±ê¸°"""
            try:
                async for chunk in llm.ask_stream_async(
                    input=request.message,
                    system_prompt=request.system_prompt,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens
                ):
                    if chunk.text:
                        yield f"data: {chunk.text}\n\n"
                
                yield f"data: [DONE]\n\n"
                
            except Exception as e:
                yield f"data: ERROR: {str(e)}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/plain",
            headers={"Cache-Control": "no-cache"}
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.post("/chat/session", response_model=ChatResponse)
async def session_chat(request: SessionChatRequest):
    """
    ì„¸ì…˜ ê¸°ë°˜ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    
    ì„¸ì…˜ì„ ìœ ì§€í•˜ë©° ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    validate_api_key()
    
    try:
        # LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°  
        llm = get_llm_instance(request.model)
        
        # ì„¸ì…˜ë³„ë¡œ ë³„ë„ì˜ LLM ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
        session_key = f"{request.session_id}_{request.model}"
        
        if session_key not in sessions:
            # ìƒˆ ì„¸ì…˜ ìƒì„±
            sessions[session_key] = LLM.create(request.model)
        
        session_llm = sessions[session_key]
        
        # íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ë©° ì‘ë‹µ ìƒì„±
        response = await session_llm.ask_async(
            input=request.message,
            use_history=True
        )
        
        return ChatResponse(
            response=response.text,
            model=request.model,
            usage=response.usage.__dict__ if response.usage else None
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì„¸ì…˜ ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.delete("/chat/session/{session_id}")
async def clear_session(session_id: str):
    """
    ì„¸ì…˜ ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸
    
    íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    deleted_sessions = []
    
    # í•´ë‹¹ ì„¸ì…˜ IDë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì„¸ì…˜ ì‚­ì œ
    for key in list(sessions.keys()):
        if key.startswith(f"{session_id}_"):
            del sessions[key]
            deleted_sessions.append(key)
    
    if not deleted_sessions:
        raise HTTPException(
            status_code=404,
            detail=f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}"
        )
    
    return {
        "message": f"ì„¸ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {session_id}",
        "deleted_sessions": deleted_sessions
    }


@app.get("/sessions")
async def list_sessions():
    """í™œì„± ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
    session_info = {}
    
    for session_key in sessions.keys():
        session_id, model = session_key.rsplit("_", 1)
        if session_id not in session_info:
            session_info[session_id] = []
        session_info[session_id].append(model)
    
    return {
        "total_sessions": len(session_info),
        "sessions": session_info
    }


# =============================================================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

if __name__ == "__main__":
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ ê²½ê³ : OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ì‚¬ìš©ë²•: OPENAI_API_KEY=your_key python main.py")
    
    print("ğŸŒŸ FastAPI + pyhub-llm ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“– API ë¬¸ì„œ: http://localhost:8000/docs")
    print("ğŸ” ëŒ€í™”í˜• API: http://localhost:8000/redoc")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0", 
        port=8000,
        reload=True,
        log_level="info"
    )