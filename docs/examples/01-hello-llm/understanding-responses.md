# ğŸ” AI ì‘ë‹µ ì´í•´í•˜ê¸°

AIê°€ ì£¼ëŠ” ë‹µë³€ì—ëŠ” í…ìŠ¤íŠ¸ ì™¸ì—ë„ ë§ì€ ì •ë³´ê°€ ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. ìì„¸íˆ ì•Œì•„ë´…ì‹œë‹¤!

## ğŸ“¦ ì‘ë‹µ ê°ì²´ì˜ êµ¬ì¡°

AIì˜ ì‘ë‹µì€ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ì •ë³´ë¥¼ ë‹´ì€ "ìƒì"ì…ë‹ˆë‹¤.

```python
from pyhub.llm import LLM

# AIì—ê²Œ ì§ˆë¬¸
ai = LLM.create("gpt-4o-mini")
response = ai.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# response ì•ˆì—ëŠ” ë­ê°€ ìˆì„ê¹Œ?
print(type(response))  # <class 'CompletionResponse'>
```

### ì‘ë‹µì— í¬í•¨ëœ ì •ë³´ë“¤

```python
# 1. ë‹µë³€ í…ìŠ¤íŠ¸
print("ë‹µë³€:", response.text)

# 2. ì‚¬ìš©ëŸ‰ ì •ë³´
print("ì…ë ¥ í† í°:", response.usage.prompt_tokens)
print("ì¶œë ¥ í† í°:", response.usage.completion_tokens)
print("ì´ í† í°:", response.usage.total_tokens)

# 3. ëª¨ë¸ ì •ë³´
print("ì‚¬ìš©í•œ ëª¨ë¸:", response.model)

# 4. ì›ë³¸ ì‘ë‹µ (ê³ ê¸‰)
print("ì›ë³¸ ë°ì´í„°:", response.raw)
```

## ğŸ’° í† í°ê³¼ ë¹„ìš© ì´í•´í•˜ê¸°

### í† í°ì´ë€?
í† í°ì€ AIê°€ í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ëŠ” ë‹¨ìœ„ì…ë‹ˆë‹¤.

```python
# í† í° ì˜ˆì‹œ ì‹œê°í™”
examples = [
    "ì•ˆë…•",        # ì•½ 2í† í°
    "Hello",       # ì•½ 1í† í°
    "ì•ˆë…•í•˜ì„¸ìš”",   # ì•½ 5í† í°
    "I love you",  # ì•½ 3í† í°
]

for text in examples:
    response = ai.ask(f"'{text}'ëŠ” ëª‡ ê¸€ìì•¼?")
    print(f"'{text}' = {response.usage.prompt_tokens}í† í°")
```

### í† í° ê³„ì‚° ê·œì¹™
- **í•œê¸€**: 1ê¸€ì â‰ˆ 2-3í† í°
- **ì˜ì–´**: 1ë‹¨ì–´ â‰ˆ 1-2í† í°
- **ìˆ«ì**: 1ìë¦¬ â‰ˆ 1í† í°
- **íŠ¹ìˆ˜ë¬¸ì**: 1ê°œ â‰ˆ 1í† í°

### ë¹„ìš© ê³„ì‚°í•˜ê¸°

```python
def calculate_cost(response, model="gpt-4o-mini"):
    """ì‘ë‹µì˜ ì˜ˆìƒ ë¹„ìš©ì„ ê³„ì‚°í•©ë‹ˆë‹¤"""
    
    # ëª¨ë¸ë³„ 1,000í† í°ë‹¹ ê°€ê²© (ì›í™” ê¸°ì¤€)
    prices = {
        "gpt-4o-mini": {
            "input": 0.2,    # ì…ë ¥ 1,000í† í°ë‹¹ 0.2ì›
            "output": 0.8    # ì¶œë ¥ 1,000í† í°ë‹¹ 0.8ì›
        },
        "gpt-4o": {
            "input": 5,      # ì…ë ¥ 1,000í† í°ë‹¹ 5ì›
            "output": 15     # ì¶œë ¥ 1,000í† í°ë‹¹ 15ì›
        }
    }
    
    price = prices.get(model, prices["gpt-4o-mini"])
    
    # ë¹„ìš© ê³„ì‚°
    input_cost = (response.usage.prompt_tokens / 1000) * price["input"]
    output_cost = (response.usage.completion_tokens / 1000) * price["output"]
    total_cost = input_cost + output_cost
    
    return {
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": total_cost
    }

# ì‚¬ìš© ì˜ˆì‹œ
response = ai.ask("íŒŒì´ì¬ìœ¼ë¡œ ê³„ì‚°ê¸° ë§Œë“œëŠ” ë²• ì•Œë ¤ì¤˜")
cost = calculate_cost(response)

print(f"ğŸ’° ë¹„ìš© ë¶„ì„:")
print(f"- ì§ˆë¬¸ ë¹„ìš©: {cost['input_cost']:.3f}ì›")
print(f"- ë‹µë³€ ë¹„ìš©: {cost['output_cost']:.3f}ì›")
print(f"- ì´ ë¹„ìš©: {cost['total_cost']:.3f}ì›")
```

## ğŸ¯ ì¢‹ì€ ì§ˆë¬¸í•˜ëŠ” ë°©ë²•

### 1. ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ

```python
# âŒ ë‚˜ìœ ì˜ˆ
response = ai.ask("ì½”ë“œ ì¨ì¤˜")

# âœ… ì¢‹ì€ ì˜ˆ
response = ai.ask("""
Pythonìœ¼ë¡œ ê°„ë‹¨í•œ ê³„ì‚°ê¸°ë¥¼ ë§Œë“¤ì–´ì¤˜.
- ë”í•˜ê¸°, ë¹¼ê¸°, ê³±í•˜ê¸°, ë‚˜ëˆ„ê¸° ê¸°ëŠ¥
- ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
- ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨
""")
```

### 2. ë§¥ë½ ì œê³µí•˜ê¸°

```python
# âŒ ë‚˜ìœ ì˜ˆ
response = ai.ask("ì´ê±° ê³ ì³ì¤˜")

# âœ… ì¢‹ì€ ì˜ˆ
response = ai.ask("""
ë‹¤ìŒ Python ì½”ë“œì—ì„œ ZeroDivisionErrorê°€ ë°œìƒí•©ë‹ˆë‹¤.
ì–´ë–»ê²Œ ê³ ì¹  ìˆ˜ ìˆì„ê¹Œìš”?

def divide(a, b):
    return a / b
    
result = divide(10, 0)
""")
```

### 3. ì›í•˜ëŠ” í˜•ì‹ ì§€ì •í•˜ê¸°

```python
# ë‹µë³€ í˜•ì‹ì„ ì§€ì •í•˜ë©´ ë” ìœ ìš©í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
response = ai.ask("""
í•œêµ­ì˜ ì£¼ìš” ë„ì‹œ 5ê°œë¥¼ ì•Œë ¤ì¤˜.
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µí•´ì¤˜:
1. ë„ì‹œëª… - ì¸êµ¬ìˆ˜ - íŠ¹ì§•
""")

print(response.text)
```

## ğŸ“Š ì‘ë‹µ í’ˆì§ˆ ê°œì„ í•˜ê¸°

### 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í™œìš©

```python
# AIì—ê²Œ ì—­í•  ë¶€ì—¬í•˜ê¸°
response = ai.ask(
    "for ë£¨í”„ ì„¤ëª…í•´ì¤˜",
    system="ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì„¤ëª…í•˜ëŠ” ì¹œì ˆí•œ ì„ ìƒë‹˜ì…ë‹ˆë‹¤."
)
```

### 2. ì˜¨ë„(Temperature) ì¡°ì ˆ

```python
# ì˜¨ë„: 0 = ì¼ê´€ëœ ë‹µë³€, 1 = ì°½ì˜ì ì¸ ë‹µë³€

# ì‚¬ì‹¤ì ì¸ ì •ë³´ (ë‚®ì€ ì˜¨ë„)
response = ai.ask(
    "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?",
    temperature=0.1
)

# ì°½ì˜ì ì¸ ë‹µë³€ (ë†’ì€ ì˜¨ë„)
response = ai.ask(
    "ì™¸ê³„ì¸ì´ ì§€êµ¬ì— ì˜¨ë‹¤ë©´ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚ ê¹Œ?",
    temperature=0.9
)
```

### 3. ìµœëŒ€ í† í° ì„¤ì •

```python
# ì§§ì€ ë‹µë³€
response = ai.ask(
    "ì¸ê³µì§€ëŠ¥ì´ ë­ì•¼?",
    max_tokens=50  # ì•½ 20-30 ê¸€ì
)

# ê¸´ ë‹µë³€
response = ai.ask(
    "ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì¤˜",
    max_tokens=1000  # ì•½ 400-500 ê¸€ì
)
```

## ğŸ”§ ë””ë²„ê¹…ê³¼ ë¬¸ì œ í•´ê²°

### ì‘ë‹µ ìƒíƒœ í™•ì¸í•˜ê¸°

```python
def analyze_response(response):
    """ì‘ë‹µì„ ë¶„ì„í•˜ê³  ë¬¸ì œë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤"""
    
    print("ğŸ” ì‘ë‹µ ë¶„ì„:")
    print(f"- ëª¨ë¸: {response.model}")
    print(f"- ë‹µë³€ ê¸¸ì´: {len(response.text)}ì")
    print(f"- í† í° ì‚¬ìš©ëŸ‰: {response.usage.total_tokens}")
    
    # ë¹„ìš© ê³„ì‚°
    cost = response.usage.total_tokens * 0.002
    print(f"- ì˜ˆìƒ ë¹„ìš©: {cost:.2f}ì›")
    
    # ë‹µë³€ í’ˆì§ˆ ì²´í¬
    if len(response.text) < 10:
        print("âš ï¸ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ë³´ì„¸ìš”.")
    
    if response.usage.total_tokens > 1000:
        print("âš ï¸ í† í°ì„ ë§ì´ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ê°„ë‹¨í•˜ê²Œ í•´ë³´ì„¸ìš”.")
    
    return response

# ì‚¬ìš© ì˜ˆì‹œ
response = ai.ask("ì•ˆë…•")
analyze_response(response)
```

## ğŸ“ ì‹¤ì „ ì˜ˆì œ: ìŠ¤ë§ˆíŠ¸ ì§ˆë¬¸ ë„ìš°ë¯¸

```python
class SmartAI:
    """ë˜‘ë˜‘í•˜ê²Œ ì§ˆë¬¸í•˜ê³  ë¹„ìš©ì„ ê´€ë¦¬í•˜ëŠ” AI ë„ìš°ë¯¸"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.ai = LLM.create(model)
        self.total_cost = 0
        self.history = []
    
    def ask(self, question, save_money=True):
        """ì§ˆë¬¸í•˜ê³  ë¹„ìš©ì„ ì¶”ì í•©ë‹ˆë‹¤"""
        
        # ë¹„ìš© ì ˆì•½ ëª¨ë“œ
        if save_money:
            # ì§§ì€ ë‹µë³€ ìš”ì²­
            full_question = f"{question}\n(í•œ ë¬¸ë‹¨ìœ¼ë¡œ ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”)"
            response = self.ai.ask(full_question, max_tokens=200)
        else:
            response = self.ai.ask(question)
        
        # ë¹„ìš© ê³„ì‚°
        cost = response.usage.total_tokens * 0.002
        self.total_cost += cost
        
        # ê¸°ë¡ ì €ì¥
        self.history.append({
            "question": question,
            "answer": response.text,
            "tokens": response.usage.total_tokens,
            "cost": cost
        })
        
        return response
    
    def show_summary(self):
        """ì‚¬ìš© ìš”ì•½ì„ ë³´ì—¬ì¤ë‹ˆë‹¤"""
        print("\nğŸ“Š ì‚¬ìš© ìš”ì•½:")
        print(f"- ì´ ì§ˆë¬¸ ìˆ˜: {len(self.history)}")
        print(f"- ì´ ë¹„ìš©: {self.total_cost:.2f}ì›")
        print(f"- í‰ê·  ë¹„ìš©: {self.total_cost/len(self.history):.2f}ì›")
        
        # ê°€ì¥ ë¹„ì‹¼ ì§ˆë¬¸
        expensive = max(self.history, key=lambda x: x['cost'])
        print(f"\nğŸ’¸ ê°€ì¥ ë¹„ì‹¼ ì§ˆë¬¸: {expensive['cost']:.2f}ì›")
        print(f"   '{expensive['question'][:30]}...'")

# ì‚¬ìš©í•´ë³´ê¸°
smart_ai = SmartAI()

# ì—¬ëŸ¬ ì§ˆë¬¸í•˜ê¸°
questions = [
    "íŒŒì´ì¬ì´ ë­ì•¼?",
    "ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?",
    "ì›¹ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ë°©ë²• ì•Œë ¤ì¤˜"
]

for q in questions:
    response = smart_ai.ask(q)
    print(f"\nQ: {q}")
    print(f"A: {response.text[:100]}...")

# ìš”ì•½ ë³´ê¸°
smart_ai.show_summary()
```

## âœ… í•µì‹¬ ì •ë¦¬

1. **ì‘ë‹µ ê°ì²´**ëŠ” í…ìŠ¤íŠ¸ ì™¸ì—ë„ ë§ì€ ì •ë³´ í¬í•¨
2. **í† í°**ì€ AIê°€ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‹¨ìœ„
3. **ì¢‹ì€ ì§ˆë¬¸**ì´ ì¢‹ì€ ë‹µë³€ì„ ë§Œë“­ë‹ˆë‹¤
4. **ë¹„ìš© ê´€ë¦¬**ëŠ” ì²˜ìŒë¶€í„° ìŠµê´€í™”í•˜ì„¸ìš”

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì´ì œ AI ì‘ë‹µì„ ì™„ë²½íˆ ì´í•´í–ˆìœ¼ë‹ˆ, [ì¼ìƒ ì‘ì—… ìë™í™”](../02-everyday-tasks/)ë¡œ ë„˜ì–´ê°€ì„œ ì‹¤ìš©ì ì¸ ì˜ˆì œë“¤ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤!