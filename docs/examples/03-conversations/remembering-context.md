# ğŸ§  AIê°€ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ëŠ” ë°©ë²•

AIê°€ ì–´ë–»ê²Œ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ë§¥ë½ì„ ì´í•´í•˜ëŠ”ì§€ ì•Œì•„ë´…ì‹œë‹¤!

## ğŸ¯ ëŒ€í™” ê¸°ì–µì˜ ì›ë¦¬

### AIëŠ” ì‚¬ì‹¤ ê¸°ì–µí•˜ì§€ ëª»í•©ë‹ˆë‹¤!

```python
from pyhub.llm import LLM

ai = LLM.create("gpt-4o-mini")

# ì²« ë²ˆì§¸ ëŒ€í™”
response1 = ai.ask("ë‚´ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤")
print(response1.text)  # "ì•ˆë…•í•˜ì„¸ìš”, ê¹€ì² ìˆ˜ë‹˜!"

# ë‘ ë²ˆì§¸ ëŒ€í™” - AIëŠ” ì´ì „ ëŒ€í™”ë¥¼ ëª¨ë¦…ë‹ˆë‹¤!
response2 = ai.ask("ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì£ ?")
print(response2.text)  # "ì£„ì†¡í•˜ì§€ë§Œ, ì´ë¦„ì„ ì•Œë ¤ì£¼ì§€ ì•Šìœ¼ì…¨ìŠµë‹ˆë‹¤."
```

### ê·¸ëŸ¼ ì–´ë–»ê²Œ ê¸°ì–µí•˜ê²Œ ë§Œë“¤ê¹Œìš”?

**ë¹„ë°€ì€ "ëŒ€í™” ê¸°ë¡ì„ í•¨ê»˜ ë³´ë‚´ê¸°"ì…ë‹ˆë‹¤!**

```python
# ëŒ€í™” ê¸°ë¡ì„ ë§Œë“¤ì–´ì„œ í•¨ê»˜ ë³´ëƒ…ë‹ˆë‹¤
conversation = """
ì‚¬ìš©ì: ë‚´ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤
AI: ì•ˆë…•í•˜ì„¸ìš”, ê¹€ì² ìˆ˜ë‹˜!
ì‚¬ìš©ì: ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì£ ?
"""

response = ai.ask(conversation + "\nAI:")
print(response.text)  # "ê¹€ì² ìˆ˜ë‹˜ì´ë¼ê³  ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤."
```

## ğŸ“ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬í•˜ê¸°

### Step 1: ê°„ë‹¨í•œ ëŒ€í™” ê¸°ë¡ ì‹œìŠ¤í…œ

```python
class SimpleChat:
    """ê°„ë‹¨í•œ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.ai = LLM.create(model)
        self.history = []  # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    def chat(self, user_message):
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë°›ì•„ AI ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤"""
        
        # 1. ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë¡ì— ì¶”ê°€
        self.history.append(f"ì‚¬ìš©ì: {user_message}")
        
        # 2. ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ë§Œë“¤ê¸°
        full_conversation = "\n".join(self.history)
        
        # 3. AIì—ê²Œ ì „ì²´ ëŒ€í™”ë¥¼ ë³´ë‚´ê¸°
        prompt = f"{full_conversation}\nAI:"
        response = self.ai.ask(prompt)
        
        # 4. AI ì‘ë‹µì„ ê¸°ë¡ì— ì¶”ê°€
        self.history.append(f"AI: {response.text}")
        
        return response.text
    
    def show_history(self):
        """ëŒ€í™” ê¸°ë¡ì„ ë³´ì—¬ì¤ë‹ˆë‹¤"""
        print("\n=== ëŒ€í™” ê¸°ë¡ ===")
        for message in self.history:
            print(message)
        print("================\n")

# ì‚¬ìš© ì˜ˆì‹œ
chat = SimpleChat()

# ëŒ€í™”í•˜ê¸°
print("ğŸ¤–:", chat.chat("ì•ˆë…•! ë‚˜ëŠ” íŒŒì´ì¬ì„ ë°°ìš°ëŠ” í•™ìƒì´ì•¼"))
print("ğŸ¤–:", chat.chat("ë‚´ê°€ ë­˜ ë°°ìš´ë‹¤ê³  í–ˆì§€?"))
print("ğŸ¤–:", chat.chat("íŒŒì´ì¬ìœ¼ë¡œ ë­˜ ë§Œë“¤ ìˆ˜ ìˆì–´?"))

# ëŒ€í™” ê¸°ë¡ í™•ì¸
chat.show_history()
```

### Step 2: ë©”ì‹œì§€ í˜•ì‹ ê°œì„ í•˜ê¸°

```python
from datetime import datetime

class ImprovedChat:
    """ê°œì„ ëœ ëŒ€í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.ai = LLM.create(model)
        self.messages = []  # êµ¬ì¡°í™”ëœ ë©”ì‹œì§€ ì €ì¥
    
    def add_message(self, role, content):
        """ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤"""
        self.messages.append({
            "role": role,  # "user" ë˜ëŠ” "assistant"
            "content": content,
            "timestamp": datetime.now()
        })
    
    def chat(self, user_message):
        """ëŒ€í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.add_message("user", user_message)
        
        # OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_messages = []
        for msg in self.messages:
            if msg["role"] == "user":
                formatted_messages.append(f"Human: {msg['content']}")
            else:
                formatted_messages.append(f"Assistant: {msg['content']}")
        
        # ëŒ€í™” ìƒì„±
        conversation = "\n".join(formatted_messages)
        prompt = f"{conversation}\nAssistant:"
        
        # AI ì‘ë‹µ ë°›ê¸°
        response = self.ai.ask(prompt)
        
        # ì‘ë‹µ ì €ì¥
        self.add_message("assistant", response.text)
        
        return response.text
    
    def get_context_summary(self):
        """ëŒ€í™” ë§¥ë½ì„ ìš”ì•½í•©ë‹ˆë‹¤"""
        if len(self.messages) < 2:
            return "ëŒ€í™”ê°€ ë§‰ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
        recent = self.messages[-5:]
        summary_prompt = "ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œ ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n"
        
        for msg in recent:
            summary_prompt += f"{msg['role']}: {msg['content']}\n"
        
        summary = self.ai.ask(summary_prompt)
        return summary.text

# ì‚¬ìš© ì˜ˆì‹œ
chat = ImprovedChat()

# ëŒ€í™” ì§„í–‰
responses = [
    chat.chat("ì•ˆë…•! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤"),
    chat.chat("ì‚°ì±…í•˜ê¸° ì¢‹ì€ ê³³ ì¶”ì²œí•´ì¤„ ìˆ˜ ìˆì–´?"),
    chat.chat("ì„œìš¸ì— ìˆëŠ” ê³³ìœ¼ë¡œ ë¶€íƒí•´")
]

for i, response in enumerate(responses, 1):
    print(f"ì‘ë‹µ {i}: {response}\n")

# ëŒ€í™” ìš”ì•½
print("ğŸ“ ëŒ€í™” ìš”ì•½:", chat.get_context_summary())
```

## ğŸ¨ ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬

### 1. ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œí•˜ê¸°

```python
class MemoryLimitedChat:
    """ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ ì œí•œí•˜ëŠ” ì±—ë´‡"""
    
    def __init__(self, model="gpt-4o-mini", max_messages=10):
        self.ai = LLM.create(model)
        self.messages = []
        self.max_messages = max_messages  # ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜
        
    def chat(self, user_message):
        """ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ ì œí•œí•˜ë©´ì„œ ëŒ€í™”í•©ë‹ˆë‹¤"""
        # ë©”ì‹œì§€ ì¶”ê°€
        self.messages.append({"role": "user", "content": user_message})
        
        # ë©”ëª¨ë¦¬ í¬ê¸° ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ë©”ì‹œì§€ ì œê±°
        if len(self.messages) > self.max_messages:
            # ì²« ë²ˆì§¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì œê±°
            self.messages = self.messages[-(self.max_messages):]
        
        # ëŒ€í™” ìƒì„±
        conversation = self._format_conversation()
        response = self.ai.ask(conversation)
        
        # ì‘ë‹µ ì €ì¥
        self.messages.append({"role": "assistant", "content": response.text})
        
        return response.text
    
    def _format_conversation(self):
        """ëŒ€í™”ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤"""
        formatted = []
        for msg in self.messages:
            role = "Human" if msg["role"] == "user" else "Assistant"
            formatted.append(f"{role}: {msg['content']}")
        
        return "\n".join(formatted) + "\nAssistant:"
    
    def get_memory_usage(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•©ë‹ˆë‹¤"""
        total_chars = sum(len(msg["content"]) for msg in self.messages)
        return {
            "messages": len(self.messages),
            "characters": total_chars,
            "estimated_tokens": total_chars // 4  # ëŒ€ëµì ì¸ í† í° ìˆ˜
        }

# ì‚¬ìš© ì˜ˆì‹œ
chat = MemoryLimitedChat(max_messages=6)

# ê¸´ ëŒ€í™” ì§„í–‰
for i in range(10):
    response = chat.chat(f"ì´ê²ƒì€ {i+1}ë²ˆì§¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤")
    print(f"ë©”ì‹œì§€ {i+1}: {response[:50]}...")
    
    # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    usage = chat.get_memory_usage()
    print(f"  â†’ ë©”ëª¨ë¦¬: {usage['messages']}ê°œ ë©”ì‹œì§€, ì•½ {usage['estimated_tokens']} í† í°\n")
```

### 2. ì¤‘ìš”í•œ ì •ë³´ë§Œ ê¸°ì–µí•˜ê¸°

```python
class SmartMemoryChat:
    """ì¤‘ìš”í•œ ì •ë³´ë§Œ ê¸°ì–µí•˜ëŠ” ë˜‘ë˜‘í•œ ì±—ë´‡"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.ai = LLM.create(model)
        self.short_term_memory = []  # ë‹¨ê¸° ê¸°ì–µ
        self.long_term_memory = {}   # ì¥ê¸° ê¸°ì–µ (ì¤‘ìš” ì •ë³´)
        
    def chat(self, user_message):
        """ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë©´ì„œ ëŒ€í™”í•©ë‹ˆë‹¤"""
        # ì¤‘ìš” ì •ë³´ ì¶”ì¶œ
        important_info = self._extract_important_info(user_message)
        if important_info:
            self.long_term_memory.update(important_info)
        
        # ë‹¨ê¸° ê¸°ì–µì— ì¶”ê°€
        self.short_term_memory.append({"role": "user", "content": user_message})
        
        # ë§¥ë½ ìƒì„±
        context = self._create_context()
        response = self.ai.ask(context)
        
        # ì‘ë‹µ ì €ì¥
        self.short_term_memory.append({"role": "assistant", "content": response.text})
        
        # ë‹¨ê¸° ê¸°ì–µ ì •ë¦¬ (ìµœê·¼ 5ê°œë§Œ ìœ ì§€)
        if len(self.short_term_memory) > 10:
            self.short_term_memory = self.short_term_memory[-10:]
        
        return response.text
    
    def _extract_important_info(self, message):
        """ë©”ì‹œì§€ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
        extract_prompt = f"""
        ë‹¤ìŒ ë©”ì‹œì§€ì—ì„œ ê¸°ì–µí•´ì•¼ í•  ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        "{message}"
        
        ì˜ˆì‹œ: ì´ë¦„, ë‚˜ì´, ì§ì—…, ì·¨ë¯¸, ì„ í˜¸ì‚¬í•­ ë“±
        ì—†ìœ¼ë©´ "ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
        ìˆìœ¼ë©´ "í‚¤: ê°’" í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.
        """
        
        result = self.ai.ask(extract_prompt)
        
        # ê²°ê³¼ íŒŒì‹±
        if "ì—†ìŒ" in result.text:
            return None
        
        # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•˜ê²Œ)
        info = {}
        lines = result.text.strip().split('\n')
        for line in lines:
            if ':' in line:
                key, value = line.split(':', 1)
                info[key.strip()] = value.strip()
        
        return info if info else None
    
    def _create_context(self):
        """ëŒ€í™” ë§¥ë½ì„ ìƒì„±í•©ë‹ˆë‹¤"""
        context_parts = []
        
        # ì¥ê¸° ê¸°ì–µ ì¶”ê°€
        if self.long_term_memory:
            context_parts.append("ê¸°ì–µí•˜ê³  ìˆëŠ” ì •ë³´:")
            for key, value in self.long_term_memory.items():
                context_parts.append(f"- {key}: {value}")
            context_parts.append("")
        
        # ìµœê·¼ ëŒ€í™” ì¶”ê°€
        context_parts.append("ìµœê·¼ ëŒ€í™”:")
        for msg in self.short_term_memory:
            role = "Human" if msg["role"] == "user" else "Assistant"
            context_parts.append(f"{role}: {msg['content']}")
        
        context_parts.append("Assistant:")
        return "\n".join(context_parts)
    
    def show_memory(self):
        """í˜„ì¬ ê¸°ì–µ ìƒíƒœë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤"""
        print("\n=== ì¥ê¸° ê¸°ì–µ ===")
        for key, value in self.long_term_memory.items():
            print(f"{key}: {value}")
        
        print(f"\n=== ë‹¨ê¸° ê¸°ì–µ ===")
        print(f"ìµœê·¼ {len(self.short_term_memory)}ê°œ ë©”ì‹œì§€ ì €ì¥ ì¤‘")

# ì‚¬ìš© ì˜ˆì‹œ
smart_chat = SmartMemoryChat()

# ì •ë³´ê°€ í¬í•¨ëœ ëŒ€í™”
conversations = [
    "ì•ˆë…•! ë‚´ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì•¼",
    "ë‚˜ëŠ” 25ì‚´ì´ê³  ê°œë°œìë¡œ ì¼í•˜ê³  ìˆì–´",
    "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?",
    "ë‚´ê°€ ëª‡ ì‚´ì´ë¼ê³  í–ˆì§€?",
    "í”„ë¡œê·¸ë˜ë° ê´€ë ¨ ì±… ì¶”ì²œí•´ì¤„ ìˆ˜ ìˆì–´?"
]

for msg in conversations:
    print(f"\nğŸ‘¤ You: {msg}")
    response = smart_chat.chat(msg)
    print(f"ğŸ¤– Bot: {response}")

# ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
smart_chat.show_memory()
```

### 3. ëŒ€í™” ìš”ì•½ì„ í†µí•œ ë©”ëª¨ë¦¬ ì••ì¶•

```python
class CompressedMemoryChat:
    """ëŒ€í™”ë¥¼ ìš”ì•½í•´ì„œ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.ai = LLM.create(model)
        self.current_conversation = []
        self.conversation_summary = ""
        self.summary_threshold = 10  # 10ê°œ ë©”ì‹œì§€ë§ˆë‹¤ ìš”ì•½
        
    def chat(self, user_message):
        """ëŒ€í™”ë¥¼ ì••ì¶•í•˜ë©´ì„œ ì§„í–‰í•©ë‹ˆë‹¤"""
        # í˜„ì¬ ëŒ€í™”ì— ì¶”ê°€
        self.current_conversation.append(f"User: {user_message}")
        
        # ì„ê³„ê°’ ë„ë‹¬ ì‹œ ìš”ì•½
        if len(self.current_conversation) >= self.summary_threshold:
            self._compress_conversation()
        
        # ì „ì²´ ë§¥ë½ ìƒì„±
        context = self._build_context()
        context += f"\nUser: {user_message}\nAssistant:"
        
        # ì‘ë‹µ ìƒì„±
        response = self.ai.ask(context)
        self.current_conversation.append(f"Assistant: {response.text}")
        
        return response.text
    
    def _compress_conversation(self):
        """í˜„ì¬ ëŒ€í™”ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤"""
        # ì´ì „ ìš”ì•½ê³¼ í˜„ì¬ ëŒ€í™”ë¥¼ í•©ì³ì„œ ìƒˆë¡œìš´ ìš”ì•½ ìƒì„±
        compress_prompt = f"""
        ì´ì „ ëŒ€í™” ìš”ì•½:
        {self.conversation_summary}
        
        ìµœê·¼ ëŒ€í™”:
        {chr(10).join(self.current_conversation)}
        
        ìœ„ ë‚´ìš©ì„ ëª¨ë‘ í¬í•¨í•˜ì—¬ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”.
        ì¤‘ìš”í•œ ì •ë³´ëŠ” ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”.
        """
        
        new_summary = self.ai.ask(compress_prompt)
        self.conversation_summary = new_summary.text
        
        # ìµœê·¼ 2ê°œ ë©”ì‹œì§€ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
        self.current_conversation = self.current_conversation[-2:]
        
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì••ì¶• ì™„ë£Œ! (ìš”ì•½ ê¸¸ì´: {len(self.conversation_summary)}ì)")
    
    def _build_context(self):
        """ëŒ€í™” ë§¥ë½ì„ êµ¬ì„±í•©ë‹ˆë‹¤"""
        parts = []
        
        # ìš”ì•½ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if self.conversation_summary:
            parts.append(f"ì´ì „ ëŒ€í™” ìš”ì•½:\n{self.conversation_summary}\n")
        
        # í˜„ì¬ ëŒ€í™” ì¶”ê°€
        if self.current_conversation:
            parts.append("ìµœê·¼ ëŒ€í™”:")
            parts.extend(self.current_conversation)
        
        return "\n".join(parts)
    
    def get_memory_stats(self):
        """ë©”ëª¨ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
        return {
            "summary_length": len(self.conversation_summary),
            "current_messages": len(self.current_conversation),
            "total_chars": len(self.conversation_summary) + 
                          sum(len(msg) for msg in self.current_conversation)
        }

# ì‚¬ìš© ì˜ˆì‹œ
compressed_chat = CompressedMemoryChat()

# ê¸´ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜
topics = [
    "íŒŒì´ì¬ í•™ìŠµ", "ì›¹ ê°œë°œ", "ë°ì´í„°ë² ì´ìŠ¤", "API ì„¤ê³„",
    "í”„ë¡ íŠ¸ì—”ë“œ", "ë°±ì—”ë“œ", "í´ë¼ìš°ë“œ", "DevOps",
    "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ìì—°ì–´ì²˜ë¦¬", "ì»´í“¨í„°ë¹„ì „"
]

for i, topic in enumerate(topics):
    print(f"\n--- ëŒ€í™” {i+1}: {topic} ---")
    
    # ê° ì£¼ì œì— ëŒ€í•´ ëŒ€í™”
    response = compressed_chat.chat(f"{topic}ì— ëŒ€í•´ ì•Œë ¤ì¤˜")
    print(f"Bot: {response[:100]}...")
    
    # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    stats = compressed_chat.get_memory_stats()
    print(f"ë©”ëª¨ë¦¬: {stats['current_messages']}ê°œ ë©”ì‹œì§€, ì´ {stats['total_chars']}ì")
```

## ğŸ“Š ë©”ëª¨ë¦¬ ì „ëµ ë¹„êµ

| ì „ëµ | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œê¸° |
|-----|------|------|----------|
| **ì „ì²´ ê¸°ë¡** | ì™„ë²½í•œ ë§¥ë½ | ë¹„ìš© ì¦ê°€ | ì§§ì€ ëŒ€í™” |
| **í¬ê¸° ì œí•œ** | ë¹„ìš© ì ˆì•½ | ì˜¤ë˜ëœ ì •ë³´ ì†ì‹¤ | ì¼ë°˜ì ì¸ ëŒ€í™” |
| **ì¤‘ìš” ì •ë³´ ì¶”ì¶œ** | íš¨ìœ¨ì  | êµ¬í˜„ ë³µì¡ | ì¥ê¸° ëŒ€í™” |
| **ëŒ€í™” ì••ì¶•** | ê· í˜•ì  | ìš”ì•½ ì‹œ ì •ë³´ ì†ì‹¤ | ë§¤ìš° ê¸´ ëŒ€í™” |

## ğŸ”§ ì‹¤ì „ íŒ

### 1. ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™œìš©

```python
def create_chat_with_personality(personality):
    """íŠ¹ì • ì„±ê²©ì„ ê°€ì§„ ì±—ë´‡ì„ ë§Œë“­ë‹ˆë‹¤"""
    chat = ImprovedChat()
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì„±ê²© ì„¤ì •
    system_message = f"ë‹¹ì‹ ì€ {personality} ì±—ë´‡ì…ë‹ˆë‹¤. ì´ ì„±ê²©ì— ë§ê²Œ ëŒ€í™”í•˜ì„¸ìš”."
    chat.add_message("system", system_message)
    
    return chat

# ë‹¤ì–‘í•œ ì„±ê²©ì˜ ì±—ë´‡
friendly_bot = create_chat_with_personality("ì¹œì ˆí•˜ê³  ìœ ë¨¸ëŸ¬ìŠ¤í•œ")
professional_bot = create_chat_with_personality("ì „ë¬¸ì ì´ê³  ì •í™•í•œ")
```

### 2. ëŒ€í™” ì €ì¥ê³¼ ë³µì›

```python
import json

def save_conversation(chat, filename):
    """ëŒ€í™”ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(chat.messages, f, ensure_ascii=False, default=str, indent=2)

def load_conversation(chat, filename):
    """ì €ì¥ëœ ëŒ€í™”ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤"""
    with open(filename, 'r', encoding='utf-8') as f:
        chat.messages = json.load(f)
    return chat
```

## âœ… í•µì‹¬ ì •ë¦¬

1. **AIëŠ” ìŠ¤ìŠ¤ë¡œ ê¸°ì–µí•˜ì§€ ëª»í•¨** - ìš°ë¦¬ê°€ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•´ì•¼ í•¨
2. **ë©”ëª¨ë¦¬ ê´€ë¦¬ê°€ ì¤‘ìš”** - ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ê· í˜•
3. **ë‹¤ì–‘í•œ ì „ëµ ì¡´ì¬** - ìƒí™©ì— ë§ëŠ” ë°©ë²• ì„ íƒ
4. **êµ¬ì¡°í™”ëœ ë°ì´í„°** - ì²´ê³„ì ì¸ ë©”ì‹œì§€ ê´€ë¦¬

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ëŒ€í™” ê¸°ì–µ ë°©ë²•ì„ ë°°ì› ìœ¼ë‹ˆ, ì´ì œ [ì±—ë´‡ ë§Œë“¤ê¸°](chatbot-basics.md)ì—ì„œ ì‹¤ì œë¡œ ëŒ€í™”í˜• AIë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤!