# ë¹ ë¥¸ ì‹œì‘

5ë¶„ ì•ˆì— pyhub-llmìœ¼ë¡œ ì²« ë²ˆì§¸ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤!

## 1. ì²« ë²ˆì§¸ ëŒ€í™”

ê°€ì¥ ê°„ë‹¨í•œ ì˜ˆì œë¶€í„° ì‹œì‘í•´ë´…ì‹œë‹¤:

```python
from pyhub.llm import LLM

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ëª¨ë¸ëª…ìœ¼ë¡œ ìë™ í”„ë¡œë°”ì´ë” ê°ì§€)
llm = LLM.create("gpt-4o-mini")

# ì§ˆë¬¸í•˜ê¸°
reply = llm.ask("íŒŒì´ì¬ì˜ ì¥ì ì„ 3ê°€ì§€ë§Œ ì•Œë ¤ì£¼ì„¸ìš”")
print(reply.text)
```

!!! success "ì¶œë ¥ ì˜ˆì‹œ"
    ```
    íŒŒì´ì¬ì˜ ì£¼ìš” ì¥ì  3ê°€ì§€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    
    1. **ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•**: ìì—°ì–´ì— ê°€ê¹Œìš´ ë¬¸ë²•ìœ¼ë¡œ ì´ˆë³´ìë„ ì‰½ê²Œ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    2. **í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬**: ë°ì´í„° ë¶„ì„, ì›¹ ê°œë°œ, AI/ML ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤.
    3. **ë†’ì€ ìƒì‚°ì„±**: ë¹ ë¥¸ ê°œë°œê³¼ í”„ë¡œí† íƒ€ì´í•‘ì´ ê°€ëŠ¥í•˜ì—¬ ê°œë°œ ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ```

## 2. í”„ë¡œë°”ì´ë” ì „í™˜í•˜ê¸°

ë‹¤ë¥¸ LLM í”„ë¡œë°”ì´ë”ë¡œ ì‰½ê²Œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# OpenAI
openai_llm = LLM.create("gpt-4o-mini")

# Anthropic Claude
claude_llm = LLM.create("claude-3-5-haiku-latest")

# Google Gemini
gemini_llm = LLM.create("gemini-2.0-flash-exp")

# ëª¨ë‘ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
question = "ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
for llm in [openai_llm, claude_llm, gemini_llm]:
    reply = llm.ask(question)
    print(f"\n{llm.model}ì˜ ë‹µë³€:")
    print(reply.text[:100] + "...")
```

## 3. ëŒ€í™” ì´ì–´ê°€ê¸°

pyhub-llmì€ ìë™ìœ¼ë¡œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤:

```python
llm = LLM.create("gpt-4o-mini")

# ì²« ë²ˆì§¸ ì§ˆë¬¸
llm.ask("ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤.")

# ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ë‹µë³€
reply = llm.ask("ì œ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì£ ?")
print(reply.text)  # "ê¹€ì² ìˆ˜ë‹˜ì´ë¼ê³  ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤."

# ëŒ€í™” íˆìŠ¤í† ë¦¬ í™•ì¸
print(f"ì´ {len(llm.history)}ê°œì˜ ë©”ì‹œì§€")

# ëŒ€í™” ì´ˆê¸°í™”
llm.clear()
```

## 4. êµ¬ì¡°í™”ëœ ì¶œë ¥ ë°›ê¸°

ì‘ë‹µì„ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### ì„ íƒì§€ì—ì„œ ê³ ë¥´ê¸°

```python
# ê°ì • ë¶„ì„
reply = llm.ask(
    "ë‹¤ìŒ ë¬¸ì¥ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: 'ì˜¤ëŠ˜ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”!'",
    choices=["ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½"]
)

print(f"ê°ì •: {reply.choice}")  # "ê¸ì •"
print(f"í™•ì‹ ë„: {reply.confidence}")  # 0.95
```

### Pydantic ìŠ¤í‚¤ë§ˆ ì‚¬ìš©

```python
from pydantic import BaseModel
from typing import List

class MovieReview(BaseModel):
    title: str
    rating: float  # 0-10
    pros: List[str]
    cons: List[str]
    recommend: bool

reply = llm.ask(
    "ì˜í™” 'ì¸í„°ìŠ¤í…”ë¼'ì— ëŒ€í•œ ë¦¬ë·°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”",
    schema=MovieReview
)

review = reply.structured_data
print(f"ì œëª©: {review.title}")
print(f"í‰ì : {review.rating}/10")
print(f"ì¥ì : {', '.join(review.pros)}")
print(f"ì¶”ì²œ ì—¬ë¶€: {'ì¶”ì²œ' if review.recommend else 'ë¹„ì¶”ì²œ'}")
```

## 5. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ ë°›ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ë°›ê¸°
for chunk in llm.ask("íŒŒì´ì¬ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ì¼ë“¤ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)
```

## 6. ë¹„ë™ê¸° ì²˜ë¦¬

ë¹„ë™ê¸°ë¡œ ì—¬ëŸ¬ ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
import asyncio

async def ask_multiple():
    llm = LLM.create("gpt-4o-mini")
    
    questions = [
        "Pythonì˜ ì¥ì ì€?",
        "JavaScriptì˜ ì¥ì ì€?",
        "Goì˜ ì¥ì ì€?"
    ]
    
    # ë™ì‹œì— ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬
    tasks = [llm.ask_async(q) for q in questions]
    replies = await asyncio.gather(*tasks)
    
    for q, r in zip(questions, replies):
        print(f"Q: {q}")
        print(f"A: {r.text[:50]}...\n")

# ì‹¤í–‰
asyncio.run(ask_multiple())
```

## 7. ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì§ˆë¬¸í•˜ê¸°

ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ ë©€í‹°ëª¨ë‹¬ ì§ˆë¬¸ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤:

```python
# ì´ë¯¸ì§€ íŒŒì¼ê³¼ í•¨ê»˜ ì§ˆë¬¸
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?",
    files=["image.jpg"]
)
print(reply.text)

# ì—¬ëŸ¬ ì´ë¯¸ì§€ ë¹„êµ
reply = llm.ask(
    "ì´ ë‘ ì´ë¯¸ì§€ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    files=["before.png", "after.png"]
)
```

## 8. ë…ë¦½ì ì¸ ì‘ì—… ì²˜ë¦¬ (Stateless ëª¨ë“œ)

ë°˜ë³µì ì¸ ë…ë¦½ ì‘ì—…ì—ëŠ” Stateless ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```python
# Stateless ëª¨ë“œ - íˆìŠ¤í† ë¦¬ ì €ì¥ ì•ˆ í•¨
classifier = LLM.create("gpt-4o-mini", stateless=True)

# ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜
texts = [
    "ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”!",
    "ë°°ì†¡ì´ ë„ˆë¬´ ëŠ¦ì–´ìš”",
    "í’ˆì§ˆì´ ê¸°ëŒ€ ì´í•˜ë„¤ìš”",
    "ë‹¤ì‹œ êµ¬ë§¤í•  ì˜ˆì •ì…ë‹ˆë‹¤"
]

for text in texts:
    reply = classifier.ask(
        f"ê°ì • ë¶„ì„: {text}",
        choices=["ê¸ì •", "ë¶€ì •"]
    )
    print(f"{text} -> {reply.choice}")
```

## ë‹¤ìŒ ë‹¨ê³„

ì¶•í•˜í•©ë‹ˆë‹¤! ğŸ‰ pyhub-llmì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ë‚´ìš©ì„ ì•Œì•„ë³´ë ¤ë©´:

- [ê¸°ë³¸ ì‚¬ìš©ë²• ê°€ì´ë“œ](../guides/basic-usage.md) - ë” ë§ì€ ê¸°ë³¸ ê¸°ëŠ¥
- [ëŒ€í™” ê´€ë¦¬](../guides/conversation.md) - ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- [êµ¬ì¡°í™”ëœ ì¶œë ¥](../guides/structured-output.md) - ë³µì¡í•œ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
- [API ë ˆí¼ëŸ°ìŠ¤](../api-reference/index.md) - ì „ì²´ API ë¬¸ì„œ

## ë„ì›€ë§

ë¬¸ì œê°€ ë°œìƒí–ˆë‚˜ìš”?

- ğŸ“§ ì´ë©”ì¼: me@pyhub.kr
- ğŸ’¬ [GitHub Discussions](https://github.com/pyhub-kr/pyhub-llm/discussions)
- ğŸ› [GitHub Issues](https://github.com/pyhub-kr/pyhub-llm/issues)