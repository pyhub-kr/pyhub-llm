# pyhub-llm

ë‹¤ì–‘í•œ LLM ì œê³µì—…ì²´ë¥¼ ìœ„í•œ í†µí•© Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. OpenAI, Anthropic, Google, Ollama ë“±ì˜ APIë¥¼ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì¹˜íŠ¸ ì‹œíŠ¸

* [CHEATSHEET.md](./CHEATSHEET.md) íŒŒì¼ ì°¸ê³ 

```python
from pyhub.llm import UpstageLLM

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì—†ì´ë„, LLMì´ ë˜‘ë˜‘í•˜ê¸°ì— ê°ì • ì„ íƒì„ í•´ì¤ë‹ˆë‹¤.
llm = UpstageLLM()  #system_prompt="ìœ ì € ë©”ì‹œì§€ì˜ ê°ì •ì€?")

reply = llm.ask("ìš°ìš¸í•´ì„œ ë¹µì„ ìƒ€ì–´.", choices=["ê¸°ì¨", "ìŠ¬í””", "ë¶„ë…¸", "ë¶ˆì•ˆ", "ë¬´ê¸°ë ¥í•¨"])
print(reply.choice)        # "ìŠ¬í””"
print(reply.choice_index)  # 1
```

## ì£¼ìš” ê¸°ëŠ¥

- ğŸ”Œ **í†µí•© ì¸í„°í˜ì´ìŠ¤**: ëª¨ë“  LLM ì œê³µì—…ì²´ë¥¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©
- ğŸš€ **ê°„í¸í•œ ì „í™˜**: ì½”ë“œ ë³€ê²½ ì—†ì´ ëª¨ë¸ ì „í™˜ ê°€ëŠ¥
- ğŸ’¾ **ìºì‹± ì§€ì›**: ì‘ë‹µ ìºì‹±ìœ¼ë¡œ ë¹„ìš© ì ˆê° ë° ì„±ëŠ¥ í–¥ìƒ
- ğŸ”„ **ìŠ¤íŠ¸ë¦¬ë° ì§€ì›**: ì‹¤ì‹œê°„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
- ğŸ› ï¸ **ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ**: Function calling ì§€ì›
- ğŸ“· **ì´ë¯¸ì§€ ì²˜ë¦¬**: ì´ë¯¸ì§€ ì„¤ëª… ë° ë¶„ì„ ê¸°ëŠ¥
- âš¡ **ë¹„ë™ê¸° ì§€ì›**: ë™ê¸°/ë¹„ë™ê¸° ëª¨ë‘ ì§€ì›
- ğŸ”— **ì²´ì´ë‹**: ì—¬ëŸ¬ LLMì„ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±

## ì„¤ì¹˜

### ì „ì²´ ì„¤ì¹˜

```bash
pip install 'pyhub-llm[all]'
```

### íŠ¹ì • ì œê³µì—…ì²´ë§Œ ì„¤ì¹˜

```bash
# OpenAIë§Œ
pip install "pyhub-llm[openai]"

# Anthropicë§Œ
pip install "pyhub-llm[anthropic]"

# Googleë§Œ (google-genai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
pip install "pyhub-llm[google]"

# Ollamaë§Œ
pip install "pyhub-llm[ollama]"

# ëª¨ë“  ì œê³µì—…ì²´
pip install "pyhub-llm[all]"
```

### ê°œë°œ í™˜ê²½ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/pyhub-kr/pyhub-llm.git
cd pyhub-llm

# ê°œë°œ í™˜ê²½ ì„¤ì¹˜
pip install -e ".[dev,all]"
# í˜¹ì€ make install
```

## ë¹ ë¥¸ ì‹œì‘

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •

ê° í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í•´ë‹¹ API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:

#### Linux/macOS (Bash)
```bash
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
export GOOGLE_API_KEY="your-google-api-key"
export UPSTAGE_API_KEY="your-upstage-api-key"
```

#### Windows (PowerShell)
```powershell
$env:OPENAI_API_KEY="your-openai-api-key"
$env:ANTHROPIC_API_KEY="your-anthropic-api-key"
$env:GOOGLE_API_KEY="your-google-api-key"
$env:UPSTAGE_API_KEY="your-upstage-api-key"
```

> **ì°¸ê³ **: 
> + API í‚¤ëŠ” ê° í”„ë¡œë°”ì´ë”ì˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (API í‚¤ ì„¤ì • ì„¹ì…˜ ì°¸ì¡°)
> + OllamaëŠ” ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ API í‚¤ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤
> + OllamaëŠ” ë””í´íŠ¸ë¡œ `http://localhost:11434` ì£¼ì†Œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `UPSTAGE_BASE_URL` í™˜ê²½ë³€ìˆ˜ë‚˜ `OllamaLLM(base_url="...")` ì¸ìë¥¼ í†µí•´ ë³€ê²½í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ëª¨ë¸ë³„ ì§ì ‘ ì‚¬ìš©

ê° í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¨¼ì € ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
# OpenAI ì‚¬ìš©ì‹œ
pip install "pyhub-llm[openai]"

# Anthropic ì‚¬ìš©ì‹œ
pip install "pyhub-llm[anthropic]"

# Google ì‚¬ìš©ì‹œ
pip install "pyhub-llm[google]"

# Ollama ì‚¬ìš©ì‹œ (ë¡œì»¬ ì‹¤í–‰)
pip install "pyhub-llm[ollama]"
```

```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM, OllamaLLM

# OpenAI (OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
openai_llm = OpenAILLM(model="gpt-4o-mini")
reply = openai_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# API í‚¤ ì§ì ‘ ì „ë‹¬
openai_llm = OpenAILLM(model="gpt-4o-mini", api_key="your-api-key")

# Anthropic (ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
claude_llm = AnthropicLLM(model="claude-3-5-haiku-latest")
reply = claude_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# Google (GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
gemini_llm = GoogleLLM(model="gemini-1.5-flash")
reply = gemini_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# Ollama (ë¡œì»¬ ì‹¤í–‰, API í‚¤ ë¶ˆí•„ìš”, ê¸°ë³¸ URL: http://localhost:11434)
ollama_llm = OllamaLLM(model="mistral")
reply = ollama_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from pyhub.llm import LLM

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = LLM.create("gpt-4o-mini")

# ì§ˆë¬¸í•˜ê¸°
reply = llm.ask("Pythonì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
print(reply.text)
```

## Ollama ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©

OllamaëŠ” ë¡œì»¬ì—ì„œ LLMì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ì…ë‹ˆë‹¤. API í‚¤ê°€ í•„ìš” ì—†ê³ , ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•Šì•„ ê°œì¸ì •ë³´ ë³´í˜¸ì— ìœ ë¦¬í•©ë‹ˆë‹¤.

### Ollama ì„¤ì¹˜

#### macOS
```bash
# Homebrew ì‚¬ìš©
brew install ollama

# ë˜ëŠ” ê³µì‹ ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Linux
```bash
# ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
curl -fsSL https://ollama.ai/install.sh | sh

# ë˜ëŠ” Docker ì‚¬ìš©
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### Windows
```bash
# PowerShellì—ì„œ ì‹¤í–‰
iex (irm https://ollama.ai/install.ps1)

# ë˜ëŠ” ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ
# https://ollama.ai/download/windows
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰

```bash
# Ollama ì„œë¹„ìŠ¤ ì‹œì‘ (í•„ìš”í•œ ê²½ìš°)
ollama serve

# Mistral ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull mistral

# ë‹¤ë¥¸ ì¸ê¸° ëª¨ë¸ë“¤
ollama pull llama3.3
ollama pull gemma2
ollama pull qwen2

# ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list

# ëª¨ë¸ ì§ì ‘ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
ollama run mistral
```

### pyhub-llmì—ì„œ Ollama ì‚¬ìš©

```python
from pyhub.llm import OllamaLLM

# ê¸°ë³¸ ì‚¬ìš©ë²•
llm = OllamaLLM(model="mistral")
reply = llm.ask("Pythonìœ¼ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”")
print(reply.text)

# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ ë°›ê¸°
for chunk in llm.ask("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)

# ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì§ˆë¬¸í•˜ê¸°
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?",
    files=["image.jpg"]
)

# PDF íŒŒì¼ ì²˜ë¦¬ (ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë¨)
reply = llm.ask(
    "ì´ PDF ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”",
    files=["document.pdf"]  # ìë™ìœ¼ë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¡œ ë³€í™˜
)

# ë¹„ë™ê¸° ì‚¬ìš©
async def async_example():
    reply = await llm.ask_async("ë¹„ë™ê¸°ë¡œ ì§ˆë¬¸í•©ë‹ˆë‹¤")
    return reply.text

# ì»¤ìŠ¤í…€ ì„¤ì •
llm = OllamaLLM(
    model="mistral",
    temperature=0.7,
    max_tokens=2000,
    base_url="http://localhost:11434"  # ì»¤ìŠ¤í…€ Ollama ì„œë²„
)
```

### Ollama ì¥ì 

- **ğŸ”’ ê°œì¸ì •ë³´ ë³´í˜¸**: ëª¨ë“  ë°ì´í„°ê°€ ë¡œì»¬ì—ì„œ ì²˜ë¦¬
- **ğŸ’° ë¹„ìš© ì ˆê°**: API í˜¸ì¶œ ë¹„ìš© ì—†ìŒ
- **âš¡ ë¹ ë¥¸ ì‘ë‹µ**: ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì—†ìŒ  
- **ğŸŒ ì˜¤í”„ë¼ì¸ ì‚¬ìš©**: ì¸í„°ë„· ì—°ê²° ë¶ˆí•„ìš”
- **ğŸ›ï¸ ì™„ì „í•œ ì œì–´**: ëª¨ë¸ íŒŒë¼ë¯¸í„° ììœ  ì¡°ì •

### ì§€ì› ëª¨ë¸

- **Llama ê³„ì—´**: llama3.3, llama3.1, llama3.2
- **Mistral**: mistral, mixtral
- **Gemma**: gemma2, gemma3  
- **Qwen**: qwen2, qwen2.5
- **ê¸°íƒ€**: phi3, codellama, vicuna ë“±

> **ì°¸ê³ **: PDF íŒŒì¼ ì²˜ë¦¬ ì‹œ OllamaëŠ” ìë™ìœ¼ë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë³´ì¡´ì„ ìœ„í•´ 600 DPIë¡œ ë³€í™˜ë©ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥ ì˜ˆì œ

### 1. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
# ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
for chunk in llm.ask("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)

# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
async for chunk in await llm.ask_async("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)
```

### 2. ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬

```python
# ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
llm = LLM.create("gpt-4o-mini")

# ì²« ë²ˆì§¸ ì§ˆë¬¸
llm.ask("ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤", use_history=True)

# ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ì´ì „ ëŒ€í™” ê¸°ì–µ)
reply = llm.ask("ì œ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì£ ?", use_history=True)
print(reply.text)  # "ê¹€ì² ìˆ˜ë¼ê³  í•˜ì…¨ìŠµë‹ˆë‹¤"

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
llm.clear()
```

### 3. íŒŒì¼ ì²˜ë¦¬ (ì´ë¯¸ì§€ ë° PDF)

```python
# ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    files=["photo.jpg"]
)

# PDF íŒŒì¼ ì²˜ë¦¬ (Providerë³„ ì§€ì› í˜„í™©)
# - OpenAI, Anthropic, Google: PDF ì§ì ‘ ì§€ì›
# - Ollama: PDFë¥¼ ì´ë¯¸ì§€ë¡œ ìë™ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
reply = llm.ask(
    "ì´ PDF ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”",
    files=["document.pdf"]
)

# ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
reply = llm.ask(
    "ì´ íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ë¹„êµí•´ì£¼ì„¸ìš”",
    files=["doc1.pdf", "image1.jpg", "doc2.pdf"]
)

# ë‹¨ì¼ ì´ë¯¸ì§€ ì„¤ëª… (í¸ì˜ ë©”ì„œë“œ)
reply = llm.describe_image("photo.jpg")
print(reply.text)

# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ë¶„ì„
reply = llm.describe_image(
    "photo.jpg",
    prompt="ì´ ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ìƒ‰ìƒì€ ë¬´ì—‡ì¸ê°€ìš”?"
)

# ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ì²˜ë¦¬
responses = llm.describe_images([
    "image1.jpg",
    "image2.jpg",
    "image3.jpg"
])

# ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
text = llm.extract_text_from_image("document.jpg")
```

#### Providerë³„ íŒŒì¼ ì§€ì› í˜„í™©

| Provider | ì´ë¯¸ì§€ | PDF | ë¹„ê³  |
|----------|--------|-----|------|
| OpenAI | âœ… | âœ… | PDF ì§ì ‘ ì§€ì› |
| Anthropic | âœ… | âœ… | PDF ë² íƒ€ ì§€ì› |
| Google Gemini | âœ… | âœ… | PDF ë„¤ì´í‹°ë¸Œ ì§€ì› |
| Ollama | âœ… | âš ï¸ | PDFâ†’ì´ë¯¸ì§€ ìë™ ë³€í™˜ |

> **ì°¸ê³ **: Ollamaì—ì„œ PDF íŒŒì¼ ì‚¬ìš© ì‹œ ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë˜ë©°, ê²½ê³  ë¡œê·¸ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.

### 4. ì„ íƒì§€ ì œí•œ

```python
# ì„ íƒì§€ ì¤‘ì—ì„œë§Œ ì‘ë‹µ
reply = llm.ask(
    "ì´ ë¦¬ë·°ì˜ ê°ì •ì€?",
    context={"review": "ì •ë§ ìµœê³ ì˜ ì œí’ˆì…ë‹ˆë‹¤!"},
    choices=["ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½"]
)
print(reply.choice)  # "ê¸ì •"
print(reply.confidence)  # 0.95
```

### 5. ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ

LLMì´ ì™¸ë¶€ ë„êµ¬ë‚˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” Function Callingì„ ì§€ì›í•©ë‹ˆë‹¤. ê°„ë‹¨í•œ í•¨ìˆ˜ë¶€í„° ë³µì¡í•œ ë„êµ¬ê¹Œì§€ ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ê°„ë‹¨í•œ í•¨ìˆ˜ ì§ì ‘ ì‚¬ìš©

ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ íƒ€ì… íŒíŠ¸ê°€ ìˆëŠ” í•¨ìˆ˜ë¥¼ ì§ì ‘ ì „ë‹¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

```python
# íƒ€ì… íŒíŠ¸ì™€ docstringì´ ìˆëŠ” í•¨ìˆ˜ ì •ì˜
def get_weather(city: str) -> str:
    """ë„ì‹œì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return f"{city}ì˜ ë‚ ì”¨ëŠ” ë§‘ìŒì…ë‹ˆë‹¤."

def calculate(x: int, y: int, operation: str = "add") -> int:
    """ë‘ ìˆ«ìë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if operation == "add":
        return x + y
    elif operation == "multiply":
        return x * y
    elif operation == "subtract":
        return x - y
    return 0

# í•¨ìˆ˜ë¥¼ tools ë¦¬ìŠ¤íŠ¸ì— ì§ì ‘ ì „ë‹¬
reply = llm.ask(
    "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?",
    tools=[get_weather]  # í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
)
print(reply.text)  # "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ë§‘ìŒì…ë‹ˆë‹¤."

# ì—¬ëŸ¬ í•¨ìˆ˜ë¥¼ í•¨ê»˜ ì‚¬ìš©
reply = llm.ask(
    "ì„œìš¸ ë‚ ì”¨ë¥¼ í™•ì¸í•˜ê³  13ê³¼ 27ì„ ë”í•´ì¤˜",
    tools=[get_weather, calculate]
)
```

#### Tool í´ë˜ìŠ¤ë¡œ ê³ ê¸‰ ê¸°ëŠ¥ ì‚¬ìš©

ë” ë³µì¡í•œ íŒŒë¼ë¯¸í„°ë‚˜ ìƒì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš° Tool í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
from pyhub.llm.tools import Tool

# ë³µì¡í•œ íŒŒë¼ë¯¸í„° êµ¬ì¡°ë¥¼ ê°€ì§„ ë„êµ¬
weather_tool = Tool(
    name="get_detailed_weather",
    description="ë„ì‹œì˜ ìƒì„¸í•œ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ì˜¨ë„, ìŠµë„, í’ì† ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.",
    func=lambda city, unit="celsius", include_forecast=False: {
        "city": city,
        "temperature": "25Â°C" if unit == "celsius" else "77Â°F",
        "humidity": "60%",
        "wind_speed": "5 m/s",
        "forecast": ["ë§‘ìŒ", "êµ¬ë¦„ ì¡°ê¸ˆ"] if include_forecast else None
    },
    parameters={
        "type": "object",
        "properties": {
            "city": {
                "type": "string",
                "description": "ë‚ ì”¨ë¥¼ ì¡°íšŒí•  ë„ì‹œ ì´ë¦„"
            },
            "unit": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "default": "celsius",
                "description": "ì˜¨ë„ ë‹¨ìœ„"
            },
            "include_forecast": {
                "type": "boolean",
                "default": False,
                "description": "3ì¼ ì˜ˆë³´ í¬í•¨ ì—¬ë¶€"
            }
        },
        "required": ["city"]
    }
)

# Tool ê°ì²´ ì‚¬ìš©
reply = llm.ask(
    "ì„œìš¸ì˜ ë‚ ì”¨ë¥¼ í™”ì”¨ë¡œ ì•Œë ¤ì£¼ê³  3ì¼ ì˜ˆë³´ë„ í¬í•¨í•´ì¤˜",
    tools=[weather_tool]
)
```

#### Tool ì‚¬ìš©ì˜ íŠ¹ì§•ê³¼ ì¥ì 

**Tool í´ë˜ìŠ¤ì˜ ì¥ì :**
- **ìƒì„¸í•œ íŒŒë¼ë¯¸í„° ì •ì˜**: enum, default, ë³µì¡í•œ íƒ€ì… ë“±ì„ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜
- **ì»¤ìŠ¤í…€ ì´ë¦„ê³¼ ì„¤ëª…**: í•¨ìˆ˜ëª…ê³¼ ë‹¤ë¥¸ ì´ë¦„ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ìƒì„¸í•œ ì„¤ëª… ì¶”ê°€
- **íŒŒë¼ë¯¸í„°ë³„ ì„¤ëª…**: ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì„¤ëª… ì œê³µ
- **ë³µì¡í•œ ê²€ì¦**: JSON Schemaë¥¼ í†µí•œ ê³ ê¸‰ ê²€ì¦ ê·œì¹™ ì„¤ì •

**ì–¸ì œ ì–´ë–¤ ë°©ë²•ì„ ì‚¬ìš©í• ê¹Œ?**
- **í•¨ìˆ˜ ì§ì ‘ ì „ë‹¬**: í”„ë¡œí† íƒ€ì´í•‘, ê°„ë‹¨í•œ íŒŒë¼ë¯¸í„°, íƒ€ì… íŒíŠ¸ë¡œ ì¶©ë¶„í•œ ê²½ìš°
- **Tool í´ë˜ìŠ¤**: í”„ë¡œë•ì…˜ í™˜ê²½, ë³µì¡í•œ API, ìƒì„¸í•œ ë¬¸ì„œí™”ê°€ í•„ìš”í•œ ê²½ìš°

> **ì°¸ê³ **: í•¨ìˆ˜ë¥¼ ì§ì ‘ ì „ë‹¬í•´ë„ ë‚´ë¶€ì ìœ¼ë¡œëŠ” ìë™ìœ¼ë¡œ Tool ê°ì²´ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. íƒ€ì… íŒíŠ¸ì™€ docstringì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

### 6. LLM ì²´ì´ë‹

```python
# ë²ˆì—­ ì²´ì¸ êµ¬ì„±
translator = LLM.create(
    "gpt-4o-mini",
    prompt="ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”: {text}"
)

summarizer = LLM.create(
    "gpt-4o-mini",
    prompt="ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”: {text}"
)

# ì²´ì¸ ì—°ê²°
chain = translator | summarizer

# ì‹¤í–‰
result = chain.ask({"text": "ì¸ê³µì§€ëŠ¥ì€ ìš°ë¦¬ì˜ ë¯¸ë˜ë¥¼ ë°”ê¿€ ê²ƒì…ë‹ˆë‹¤..."})
print(result.values["text"])  # ë²ˆì—­ í›„ ìš”ì•½ëœ ê²°ê³¼
```

### 7. ìºì‹± ì‚¬ìš©

#### ìºì‹œ ì¸ì ì…˜ íŒ¨í„´

```python
from pyhub.llm import LLM
from pyhub.llm.cache import MemoryCache, FileCache
from pyhub.llm.cache.base import BaseCache
from typing import Any, Optional

# ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©
memory_cache = MemoryCache(ttl=3600)  # 1ì‹œê°„ TTL
llm = LLM.create("gpt-4o-mini", cache=memory_cache)

# íŒŒì¼ ìºì‹œ ì‚¬ìš©  
file_cache = FileCache(cache_dir=".cache", ttl=7200)  # 2ì‹œê°„ TTL
llm = LLM.create("gpt-4o-mini", cache=file_cache)

# ìºì‹œê°€ ì„¤ì •ëœ LLMì€ ìë™ìœ¼ë¡œ ìºì‹œ ì‚¬ìš©
reply = llm.ask("ì§ˆë¬¸")

# ì»¤ìŠ¤í…€ ìºì‹œ ë°±ì—”ë“œ êµ¬í˜„
class CustomCache(BaseCache):
    def get(self, key: str):
        # Redis, Database ë“± ì»¤ìŠ¤í…€ ìºì‹œ ë¡œì§
        pass
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None):
        # ì»¤ìŠ¤í…€ ì €ì¥ ë¡œì§
        pass
    
    def delete(self, key: str) -> bool:
        # ì‚­ì œ ë¡œì§
        pass
    
    def clear(self):
        # ì „ì²´ ìºì‹œ ì‚­ì œ ë¡œì§
        pass

custom_cache = CustomCache()
llm = LLM.create("gpt-4o-mini", cache=custom_cache)
```

#### ìºì‹œ ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­

**ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ ìºì‹œ**

ê¸°ë³¸ì ìœ¼ë¡œ `ask()` ë©”ì„œë“œëŠ” `use_history=True`ë¡œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ë™ì¼í•œ ì§ˆë¬¸ë„ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ë‹¬ë¼ì§€ë©´ ë‹¤ë¥¸ ìºì‹œ í‚¤ê°€ ìƒì„±ë˜ì–´ ìºì‹œ ë¯¸ìŠ¤ê°€ ë°œìƒí•©ë‹ˆë‹¤:

```python
# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¡œ ì¸í•œ ìºì‹œ ë¯¸ìŠ¤ ì˜ˆì‹œ
llm = LLM.create("gpt-4o-mini", cache=memory_cache)

# ì²« ë²ˆì§¸ ì§ˆë¬¸ - API í˜¸ì¶œë¨
reply1 = llm.ask("ì•ˆë…•í•˜ì„¸ìš”")  # ìºì‹œ í‚¤: messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]

# ë‘ ë²ˆì§¸ ë™ì¼í•œ ì§ˆë¬¸ - í•˜ì§€ë§Œ íˆìŠ¤í† ë¦¬ê°€ ìˆì–´ ë‹¤ë¥¸ ìºì‹œ í‚¤ ìƒì„±ë¨
reply2 = llm.ask("ì•ˆë…•í•˜ì„¸ìš”")  # ìºì‹œ í‚¤: messages=[...ì´ì „ ëŒ€í™”..., {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
# ê²°ê³¼: ìºì‹œ ë¯¸ìŠ¤, API ì¬í˜¸ì¶œ
```

**íš¨ê³¼ì ì¸ ìºì‹œ ì‚¬ìš© ë°©ë²•**

```python
# ë°©ë²• 1: use_history=Falseë¡œ ë…ë¦½ì ì¸ ì§ˆë¬¸
reply1 = llm.ask("Pythonì´ë€?", use_history=False)  # API í˜¸ì¶œ
reply2 = llm.ask("Pythonì´ë€?", use_history=False)  # ìºì‹œì—ì„œ ê°€ì ¸ì˜´

# ë°©ë²• 2: ìƒˆë¡œìš´ LLM ì¸ìŠ¤í„´ìŠ¤ë¡œ ê¹¨ë—í•œ ìƒíƒœ ìœ ì§€
llm_new = LLM.create("gpt-4o-mini", cache=memory_cache)
reply3 = llm_new.ask("Pythonì´ë€?")  # ìºì‹œì—ì„œ ê°€ì ¸ì˜´ (ë™ì¼í•œ ìºì‹œ ê³µìœ )

# ë°©ë²• 3: íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
llm.clear()  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
reply4 = llm.ask("Pythonì´ë€?")  # ìºì‹œì—ì„œ ê°€ì ¸ì˜´
```

**ìºì‹œê°€ íš¨ê³¼ì ì¸ ì‚¬ìš© ì‚¬ë¡€**

- ë°˜ë³µì ì¸ ë²ˆì—­ ì‘ì—…
- ì •ì ì¸ ë°ì´í„° ì¡°íšŒ (ì˜ˆ: ìš©ì–´ ì„¤ëª…, ì •ì˜)
- í…œí”Œë¦¿ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±
- ë…ë¦½ì ì¸ ë‹¨ì¼ ì§ˆë¬¸ë“¤

#### ìºì‹œ ë””ë²„ê¹… ë° í†µê³„

ìºì‹œê°€ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë””ë²„ê¹… ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
import logging

# ë¡œê¹… ì„¤ì • (ë””ë²„ê¹… ë©”ì‹œì§€ í™•ì¸)
logging.basicConfig(level=logging.DEBUG)

# ë””ë²„ê·¸ ëª¨ë“œë¡œ ìºì‹œ ìƒì„±
cache = MemoryCache(ttl=3600, debug=True)
llm = LLM.create("gpt-4o-mini", cache=cache)

# ìºì‹œ ì‘ë™ í™•ì¸
llm.ask("ì•ˆë…•í•˜ì„¸ìš”", use_history=False)  # DEBUG: Cache MISS: openai:...
llm.ask("ì•ˆë…•í•˜ì„¸ìš”", use_history=False)  # DEBUG: Cache HIT: openai:...

# ìºì‹œ í†µê³„ í™•ì¸
print(cache.stats)
# {
#   'hits': 1,
#   'misses': 1,
#   'sets': 1,
#   'hit_rate': 0.5,
#   'total_requests': 2,
#   'size': 1
# }
```

**ìºì‹œ í†µê³„ í•­ëª©**

- `hits`: ìºì‹œ íˆíŠ¸ íšŸìˆ˜
- `misses`: ìºì‹œ ë¯¸ìŠ¤ íšŸìˆ˜
- `sets`: ìºì‹œ ì €ì¥ íšŸìˆ˜
- `hit_rate`: ìºì‹œ íˆíŠ¸ìœ¨ (hits / (hits + misses))
- `total_requests`: ì´ ìš”ì²­ ìˆ˜
- `size`: í˜„ì¬ ìºì‹œì— ì €ì¥ëœ í•­ëª© ìˆ˜

### 8. í…œí”Œë¦¿ ì‚¬ìš©

```python
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
llm = LLM.create(
    "gpt-4o-mini",
    system_prompt="ë‹¹ì‹ ì€ {role}ì…ë‹ˆë‹¤.",
    prompt="ì§ˆë¬¸: {question}\në‹µë³€:"
)

# í…œí”Œë¦¿ ë³€ìˆ˜ì™€ í•¨ê»˜ ì‚¬ìš©
reply = llm.ask({
    "role": "ìˆ˜í•™ êµì‚¬",
    "question": "í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬ë€?"
})
```

## API í‚¤ ì„¤ì •

### í•„ìš”í•œ API í‚¤

ê° í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í•´ë‹¹ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤:

- **OpenAI**: `OPENAI_API_KEY` - [API í‚¤ ë°œê¸‰](https://platform.openai.com/api-keys)
- **Anthropic**: `ANTHROPIC_API_KEY` - [API í‚¤ ë°œê¸‰](https://console.anthropic.com/settings/keys)
- **Google**: `GOOGLE_API_KEY` - [API í‚¤ ë°œê¸‰](https://makersuite.google.com/app/apikey)
- **Upstage**: `UPSTAGE_API_KEY` - [API í‚¤ ë°œê¸‰](https://console.upstage.ai/)

### ì„¤ì • ë°©ë²•

#### 1. í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"
```

#### 2. ì½”ë“œì—ì„œ ì§ì ‘ ì „ë‹¬
```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM

# API í‚¤ë¥¼ ì§ì ‘ ì „ë‹¬
llm = OpenAILLM(api_key="your-api-key")
llm = AnthropicLLM(api_key="your-api-key")
llm = GoogleLLM(api_key="your-api-key")
```

## CLI ì‚¬ìš©ë²•

### ëŒ€í™”í˜• ì±„íŒ…
```bash
# ê¸°ë³¸ ëª¨ë¸ë¡œ ì±„íŒ…
pyhub-llm chat

# íŠ¹ì • ëª¨ë¸ë¡œ ì±„íŒ…
pyhub-llm chat --model claude-3-5-haiku-latest

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
pyhub-llm chat --system-prompt "ë‹¹ì‹ ì€ íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤"
```

### ë‹¨ì¼ ì§ˆë¬¸
```bash
# ì§ˆë¬¸í•˜ê³  ì‘ë‹µ ë°›ê¸°
pyhub-llm ask "Pythonê³¼ Goì˜ ì°¨ì´ì ì€?"

# íŒŒì¼ ë‚´ìš©ê³¼ í•¨ê»˜ ì§ˆë¬¸ (--file ì˜µì…˜)
pyhub-llm ask "ì´ ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”" --file main.py

# ì—¬ëŸ¬ íŒŒì¼ê³¼ í•¨ê»˜ ì§ˆë¬¸
pyhub-llm ask "ì´ íŒŒì¼ë“¤ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”" --file main.py --file utils.py

# stdinìœ¼ë¡œ íŒŒì¼ ë‚´ìš© ì „ë‹¬
cat main.py | pyhub-llm ask "ì´ ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”"

# --context ì˜µì…˜ìœ¼ë¡œ íŒŒì¼ ë‚´ìš© ì „ë‹¬
pyhub-llm ask "ì´ ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”" --context "$(cat main.py)"
```

### ì´ë¯¸ì§€ ì„¤ëª…
```bash
# ì´ë¯¸ì§€ ì„¤ëª…
pyhub-llm describe image.jpg

# ì—¬ëŸ¬ ì´ë¯¸ì§€ ì„¤ëª…
pyhub-llm describe *.jpg --output descriptions.json
```

### ì„ë² ë”© ìƒì„±
```bash
# í…ìŠ¤íŠ¸ ì„ë² ë”©
pyhub-llm embed text "ì„ë² ë”©í•  í…ìŠ¤íŠ¸"
```

## ê³ ê¸‰ ê¸°ëŠ¥

### êµ¬ì¡°í™”ëœ ì¶œë ¥ (Structured Output)

Pydantic BaseModelì„ ì‚¬ìš©í•˜ì—¬ LLM ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from pydantic import BaseModel, Field
from typing import Optional, List
from pyhub.llm import LLM

# ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜
class User(BaseModel):
    name: str = Field(description="ì‚¬ìš©ì ì´ë¦„")
    age: int = Field(description="ì‚¬ìš©ì ë‚˜ì´")
    email: str = Field(description="ì´ë©”ì¼ ì£¼ì†Œ")
    
class Product(BaseModel):
    name: str
    price: float
    features: List[str]
    in_stock: bool

# êµ¬ì¡°í™”ëœ ì‘ë‹µ ìš”ì²­
llm = LLM.create("gpt-4o-mini")

# ë‹¨ìˆœí•œ ì˜ˆì‹œ
response = llm.ask(
    "John Doe, 30ì‚´, john@example.com ì •ë³´ë¡œ ì‚¬ìš©ìë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
    schema=User
)

if response.has_structured_data:
    user = response.structured_data
    print(f"ì´ë¦„: {user.name}")
    print(f"ë‚˜ì´: {user.age}")
    print(f"ì´ë©”ì¼: {user.email}")

# ë³µì¡í•œ ì˜ˆì‹œ
response = llm.ask(
    "MacBook Pro 16ì¸ì¹˜ì— ëŒ€í•œ ì œí’ˆ ì •ë³´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”",
    schema=Product
)

if response.has_structured_data:
    product = response.structured_data
    print(f"ì œí’ˆëª…: {product.name}")
    print(f"ê°€ê²©: ${product.price}")
    print(f"íŠ¹ì§•: {', '.join(product.features)}")
```

êµ¬ì¡°í™”ëœ ì¶œë ¥ì€ ëª¨ë“  í”„ë¡œë°”ì´ë”(OpenAI, Upstage, Anthropic, Google, Ollama)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤:
- OpenAI, Upstage: ë„¤ì´í‹°ë¸Œ Structured Output ì‚¬ìš©
- Anthropic, Google, Ollama: í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ JSON ìƒì„±

### ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬

ReactAgentëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ë¥¼ ì§ì ‘ ì „ë‹¬í•˜ë©´ ìë™ìœ¼ë¡œ Tool ê°ì²´ë¡œ ë³€í™˜ë©ë‹ˆë‹¤:

> **ì°¸ê³ **: ì•„ë˜ ì˜ˆì‹œì—ì„œ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ `duckduckgo-search` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¨¼ì € ì„¤ì¹˜í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.

```python
import logging
from pyhub.llm import LLM
from pyhub.llm.agents import ReactAgent
# from pyhub.llm.tools import Tool  # Tool í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ë³µì¡í•œ ë„êµ¬ë¥¼ ì •ì˜í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤

# ë¡œê¹… ì„¤ì • - ReactAgentì˜ ì‹¤í–‰ ê³¼ì •ì„ ë³´ê¸° ìœ„í•´ í•„ìš”
logging.basicConfig(level=logging.INFO)

# ê°„ë‹¨í•œ ë„êµ¬ í•¨ìˆ˜ë“¤ ì •ì˜
def web_search(query: str) -> str:
    """ì›¹ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        from duckduckgo_search import DDGS
        
        with DDGS() as ddgs:
            results = list(ddgs.text(query, region='kr-kr', max_results=3))
            if results:
                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì„œ ë°˜í™˜
                summaries = []
                for r in results:
                    title = r.get('title', '')
                    body = r.get('body', '')
                    summaries.append(f"{title}: {body}")
                return "\n".join(summaries)
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    except ImportError:
        return "ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ë ¤ë©´ 'pip install duckduckgo-search'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    except Exception as e:
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def calculator(expression: str) -> float:
    """ìˆ˜í•™ í‘œí˜„ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    return eval(expression)  # ì‹¤ì œë¡œëŠ” ì•ˆì „í•œ íŒŒì„œ ì‚¬ìš© ê¶Œì¥

# í•¨ìˆ˜ë¥¼ ì§ì ‘ ì „ë‹¬ - ìë™ìœ¼ë¡œ Toolë¡œ ë³€í™˜ë¨
agent = ReactAgent(
    llm=LLM.create("gpt-4o-mini"),
    tools=[web_search, calculator],
    # ReactAgentì˜ ì‹¤í–‰ ê³¼ì •ì„ ë””ë²„ê¹…í•˜ë ¤ë©´ logging ì„¤ì •ê³¼ í•¨ê»˜ `verbose=True` ì˜µì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
    max_iterations=10,
    verbose=True,
)

# ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰
result = agent.run(
    "2024ë…„ í•œêµ­ì˜ GDPëŠ” ì–¼ë§ˆì´ê³ , "
    "ì´ë¥¼ ì›í™”ë¡œ í™˜ì‚°í•˜ë©´ ì–¼ë§ˆì¸ê°€ìš”?"
)
```

ì¶œë ¥ ê²°ê³¼

```
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:pyhub.llm.agents.react:Iteration 1:
Thought: I need to find the projected GDP of South Korea for the year 2024. After that, I will convert that amount into South Korean Won (KRW). First, I will search for the GDP projection for South Korea in 2024. 

Action: web_search  
Action Input: {"query": "2024 South Korea GDP projection"}  
Observation: 2024ë…„ í•œêµ­ì˜ GDPëŠ” ì•½ 2ì¡° 1ì²œì–µ ë‹¬ëŸ¬ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:pyhub.llm.agents.react:Iteration 2:
Thought: It seems that I encountered an issue with the web search tool. However, I already have the information that the projected GDP of South Korea for 2024 is approximately 2.1 trillion USD. Now, I need to convert this amount into South Korean Won (KRW). I will look up the current exchange rate for USD to KRW.

Action: web_search  
Action Input: {"query": "current USD to KRW exchange rate"}  
Observation: 1 USDëŠ” ì•½ 1,300 KRWì…ë‹ˆë‹¤.
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:pyhub.llm.agents.react:Iteration 3:
Thought: I am facing repeated issues with the web search tool. However, I have the necessary information: the projected GDP of South Korea for 2024 is approximately 2.1 trillion USD, and the current exchange rate is about 1,300 KRW for 1 USD. Now, I will calculate the GDP in KRW.

Action: calculator  
Action Input: {"expression": "2100000000000 * 1300"}  
Observation: 2730000000000000
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:pyhub.llm.agents.react:Iteration 4:
Thought: I have calculated the GDP of South Korea for 2024 in KRW, which is 2,730,000,000,000,000 KRW (or 2.73 quadrillion KRW). Now I can summarize the information.

Final Answer: 2024ë…„ í•œêµ­ì˜ GDPëŠ” ì•½ 2ì¡° 1ì²œì–µ ë‹¬ëŸ¬ì´ë©°, ì´ë¥¼ ì›í™”ë¡œ í™˜ì‚°í•˜ë©´ ì•½ 2,730ì¡° ì›ì…ë‹ˆë‹¤.
```

#### ê³ ê¸‰ ë„êµ¬ ì‚¬ìš©ë²•

ë” ë³µì¡í•œ ë„êµ¬ê°€ í•„ìš”í•œ ê²½ìš° Tool í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ê¸°ì¡´ ë„êµ¬ í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ë¥¼ í˜¼í•©í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from pyhub.llm import LLM
from pyhub.llm.agents import ReactAgent
from pyhub.llm.agents.tools import Calculator  # ë‚´ì¥ ê³„ì‚°ê¸° ë„êµ¬
from pyhub.llm.tools import Tool
import datetime

# ë‹¤ì–‘í•œ í˜•íƒœì˜ ë„êµ¬ë“¤
def get_current_time() -> str:
    """í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def get_weather(city: str, unit:str = "celsius") -> str:
    return str({
        "city": city,
        "temperature": "20Â°C" if unit == "celsius" else "68Â°F",
        "condition": "ë§‘ìŒ"
    })

# Tool í´ë˜ìŠ¤ë¡œ ë³µì¡í•œ ë„êµ¬ ì •ì˜
weather_tool = Tool(
    name="get_weather",
    description="ë„ì‹œì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤",
    func=get_weather,
)

# ë‹¤ì–‘í•œ ë„êµ¬ í˜•íƒœë¥¼ í˜¼í•© ì‚¬ìš©
agent = ReactAgent(
    llm=LLM.create("gpt-4o-mini"),
    tools=[
        Calculator(),         # ê¸°ì¡´ ë„êµ¬ í´ë˜ìŠ¤
        get_current_time,    # ê°„ë‹¨í•œ í•¨ìˆ˜
        weather_tool         # Tool ì¸ìŠ¤í„´ìŠ¤
    ]
)

result = agent.run("í˜„ì¬ ì‹œê°„ê³¼ ì„œìš¸ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ê³ , 20 + 15ë¥¼ ê³„ì‚°í•´ì¤˜")
```

### MCP (Model Context Protocol) í†µí•©

MCPëŠ” ë‹¤ì–‘í•œ ë„êµ¬ì™€ ì„œë¹„ìŠ¤ë¥¼ LLMê³¼ í†µí•©í•˜ê¸° ìœ„í•œ í‘œì¤€ í”„ë¡œí† ì½œì…ë‹ˆë‹¤.

> **ì°¸ê³ **: MCP ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ `mcp` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤:
> ```bash
> pip install 'pyhub-llm[mcp]'
> ```

#### 1. í…ŒìŠ¤íŠ¸ìš© MCP ì„œë²„ ì‹¤í–‰í•˜ê¸°

MCP ì—°ë™ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë‚´ì¥ MCP ì„œë²„ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```bash
# ê³„ì‚°ê¸° MCP ì„œë²„ ì‹¤í–‰ (stdio ë°©ì‹)
pyhub-llm mcp-server run calculator

# ì¸ì‚¬ë§ MCP ì„œë²„ ì‹¤í–‰ (streaming-http ë°©ì‹)
#  - ë””í´íŠ¸ 8000 í¬íŠ¸ë¡œ êµ¬ë™ë˜ë©°, --port ì¸ìë¡œ í¬íŠ¸ë¥¼ ì§€ì •í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
pyhub-llm mcp-server run greeting --port=8888

# ë˜ëŠ” Python ëª¨ë“ˆë¡œ ì‹¤í–‰
python -m pyhub.llm.mcp.servers calculator
python -m pyhub.llm.mcp.servers greeting --port=8888

# ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„ ëª©ë¡ í™•ì¸
pyhub-llm mcp-server list
```

ê³„ì‚°ê¸° ì„œë²„ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- `add(a, b)`: ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤
- `subtract(a, b)`: ë‘ ìˆ«ìë¥¼ ëºë‹ˆë‹¤
- `multiply(a, b)`: ë‘ ìˆ«ìë¥¼ ê³±í•©ë‹ˆë‹¤
- `divide(a, b)`: ë‘ ìˆ«ìë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤
- `power(base, exponent)`: ê±°ë“­ì œê³±ì„ ê³„ì‚°í•©ë‹ˆë‹¤

ì¸ì‚¬ë§ ì„œë²„ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- `greeting(name, lang="en")`: ë‹¤êµ­ì–´ ì¸ì‚¬ë§ì„ ìƒì„±í•©ë‹ˆë‹¤ (ì˜ì–´, í•œêµ­ì–´, ìŠ¤í˜ì¸ì–´, í”„ë‘ìŠ¤ì–´, ì¼ë³¸ì–´ ì§€ì›)

#### 2. MCP ë„êµ¬ í™•ì¸í•˜ê¸°

MCP ì„œë²„ì—ì„œ ì œê³µí•˜ëŠ” ë„êµ¬ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤:

```python
import asyncio
from pyhub.llm.mcp import MCPClient

async def list_mcp_tools():
    # ë‚´ì¥ ê³„ì‚°ê¸° ì„œë²„ ì—°ê²°
    client = MCPClient({
        "transport": "stdio",
        "command": "pyhub-llm",
        "args": ["mcp-server", "run", "calculator"],
    })

    async with client.connect():
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        tools = await client.list_tools()
        
        print("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬:")
        for tool in tools:
            print(f"\në„êµ¬ ì´ë¦„: {tool['name']}")
            print(f"ì„¤ëª…: {tool['description']}")
            print(f"íŒŒë¼ë¯¸í„°: {tool['parameters']}")

# ì‹¤í–‰
asyncio.run(list_mcp_tools())
```

ì¶œë ¥ ì˜ˆì‹œ:
```
ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬:

ë„êµ¬ ì´ë¦„: add
ì„¤ëª…: ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤
íŒŒë¼ë¯¸í„°: {'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'ì²« ë²ˆì§¸ ìˆ«ì'}, 'b': {'type': 'number', 'description': 'ë‘ ë²ˆì§¸ ìˆ«ì'}}, 'required': ['a', 'b']}

ë„êµ¬ ì´ë¦„: subtract
ì„¤ëª…: ë‘ ìˆ«ìë¥¼ ëºë‹ˆë‹¤
íŒŒë¼ë¯¸í„°: {'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'ì²« ë²ˆì§¸ ìˆ«ì'}, 'b': {'type': 'number', 'description': 'ë‘ ë²ˆì§¸ ìˆ«ì'}}, 'required': ['a', 'b']}

ë„êµ¬ ì´ë¦„: multiply
ì„¤ëª…: ë‘ ìˆ«ìë¥¼ ê³±í•©ë‹ˆë‹¤
íŒŒë¼ë¯¸í„°: {'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'ì²« ë²ˆì§¸ ìˆ«ì'}, 'b': {'type': 'number', 'description': 'ë‘ ë²ˆì§¸ ìˆ«ì'}}, 'required': ['a', 'b']}

ë„êµ¬ ì´ë¦„: divide
ì„¤ëª…: ë‘ ìˆ«ìë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤
íŒŒë¼ë¯¸í„°: {'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'ë‚˜ëˆ„ì–´ì§€ëŠ” ìˆ˜'}, 'b': {'type': 'number', 'description': 'ë‚˜ëˆ„ëŠ” ìˆ˜'}}, 'required': ['a', 'b']}

ë„êµ¬ ì´ë¦„: power
ì„¤ëª…: ê±°ë“­ì œê³±ì„ ê³„ì‚°í•©ë‹ˆë‹¤
íŒŒë¼ë¯¸í„°: {'type': 'object', 'properties': {'base': {'type': 'number', 'description': 'ë°‘'}, 'exponent': {'type': 'number', 'description': 'ì§€ìˆ˜'}}, 'required': ['base', 'exponent']}
```

ì¸ì‚¬ë§ ì„œë²„ì˜ ê²½ìš°:

```python
# ì¸ì‚¬ë§ ì„œë²„ ì—°ê²°
client = MCPClient({
    "transport": "stdio",
    "command": "pyhub-llm",
    "args": ["mcp-server", "run", "greeting", "--port", "8888"],
})

# ì¶œë ¥ ì˜ˆì‹œ:
# ë„êµ¬ ì´ë¦„: greeting
# ì„¤ëª…: Generate a greeting message in the specified language
# íŒŒë¼ë¯¸í„°: {'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'Name of the person to greet'}, 'lang': {'type': 'string', 'description': 'Language code (en, ko, es, fr, ja)', 'default': 'en'}}, 'required': ['name']}
```

#### 3. llm.askì—ì„œ MCP ë„êµ¬ ì‚¬ìš©í•˜ê¸°

MCP ë„êµ¬ë¥¼ LLMê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ë°©ë²•:

```python
import asyncio
import logging
from pyhub.llm import LLM
from pyhub.llm.mcp import MCPClient, load_mcp_tools

# ë¡œê¹… ì„¤ì • (ë””ë²„ê¹… ë©”ì‹œì§€ í™•ì¸)
logging.basicConfig(level=logging.DEBUG)

async def use_mcp_with_llm():
    # ìƒˆë¡œìš´ dataclass ë°©ì‹ (ê¶Œì¥)
    from pyhub.llm.mcp import McpStdioConfig

    config = McpStdioConfig(
        name="calculator",
        cmd="pyhub-llm mcp-server run calculator"
    )
    client = MCPClient(config)

    # ë˜ëŠ” ê¸°ì¡´ dict ë°©ì‹
    # client = MCPClient({
    #     "transport": "stdio",
    #     "command": "pyhub-llm",
    #     "args": ["mcp-server", "run", "calculator"],
    # })

    async with client.connect():
        # MCP ë„êµ¬ë¥¼ Tool ê°ì²´ë¡œ ë¡œë“œ
        tools = await load_mcp_tools(client)

        # LLM ìƒì„± (MCP ë„êµ¬ í¬í•¨)
        llm = LLM.create("gpt-4o-mini", tools=tools)

        # MCP ë„êµ¬ë¥¼ í™œìš©í•œ ì§ˆë¬¸
        response = await llm.ask_async(
            "25ì™€ 17ì„ ë”í•œ ë‹¤ìŒ, ê·¸ ê²°ê³¼ì— 3ì„ ê³±í•´ì£¼ì„¸ìš”."
        )

        print(f"ë‹µë³€: {response}")

        # ë„êµ¬ í˜¸ì¶œ ë‚´ì—­ í™•ì¸
        if hasattr(response, 'tool_calls') and response.tool_calls:
            print("\në„êµ¬ í˜¸ì¶œ ë‚´ì—­:")
            for call in response.tool_calls:
                print(f"- {call.name}({call.args})")

# ì‹¤í–‰
asyncio.run(use_mcp_with_llm())
```

ì¶œë ¥ ì˜ˆì‹œ:
```
ë‹µë³€: 25ì™€ 17ì„ ë”í•˜ë©´ 42ì´ê³ , ì—¬ê¸°ì— 3ì„ ê³±í•˜ë©´ 126ì…ë‹ˆë‹¤.

ë„êµ¬ í˜¸ì¶œ ë‚´ì—­:
- add({'a': 25, 'b': 17})
- multiply({'a': 42, 'b': 3})
```

#### 4. LLMê³¼ MCP í†µí•© ì‚¬ìš©í•˜ê¸° (ìƒˆë¡œìš´ ê¸°ëŠ¥!)

ì´ì œ LLM ìƒì„± ì‹œ MCP ì„œë²„ë¥¼ ì§ì ‘ ì„¤ì •í•  ìˆ˜ ìˆì–´, ìˆ˜ë™ìœ¼ë¡œ ì—°ê²°ì„ ê´€ë¦¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤:

##### ë°©ë²• 1: create_asyncë¡œ ìë™ ì´ˆê¸°í™”

```python
from pyhub.llm import LLM
from pyhub.llm.mcp import McpStdioConfig

# MCPê°€ ìë™ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ëŠ” LLM ìƒì„±
llm = await LLM.create_async(
    "gpt-4o-mini",
    mcp_servers=McpStdioConfig(
        name="calculator",
        cmd="pyhub-llm mcp-server run calculator"
    )
)

# MCP ë„êµ¬ê°€ ìë™ìœ¼ë¡œ ì‚¬ìš©ë¨
response = await llm.ask_async("25ì™€ 17ì„ ë”í•˜ë©´?")
print(response.text)

# ì‚¬ìš© í›„ MCP ì—°ê²° ì¢…ë£Œ
await llm.close_mcp()
```

##### ë°©ë²• 2: ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš© (ê¶Œì¥)

```python
# ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ìë™ ì—°ê²°/í•´ì œ
async with await LLM.create_async(
    "gpt-4o-mini",
    mcp_servers=[
        McpStdioConfig(name="calc", cmd="pyhub-llm mcp-server run calculator"),
        McpStreamableHttpConfig(name="web", url="http://localhost:8888/mcp")
    ]
) as llm:
    response = await llm.ask_async("100ì—ì„œ 37ì„ ë¹¼ê³  2ë¥¼ ê³±í•˜ë©´?")
    print(response.text)
# ì—¬ê¸°ì„œ ìë™ìœ¼ë¡œ MCP ì—°ê²°ì´ ì¢…ë£Œë¨
```

##### ë°©ë²• 3: ìˆ˜ë™ ì´ˆê¸°í™”

```python
# ë™ê¸°ì ìœ¼ë¡œ LLM ìƒì„± í›„ ìˆ˜ë™ ì´ˆê¸°í™”
llm = LLM.create("gpt-4o-mini", mcp_servers=mcp_config)
await llm.initialize_mcp()  # MCP ì—°ê²° ì‹œì‘

# ì‚¬ìš©
response = await llm.ask_async("...")

# ì¢…ë£Œ
await llm.close_mcp()
```

##### ë°©ë²• 4: ì„¤ì • íŒŒì¼ ì‚¬ìš© (ìƒˆë¡œìš´ ê¸°ëŠ¥!)

MCP ì„¤ì •ì„ JSON ë˜ëŠ” YAML íŒŒì¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
# mcp_config.yaml
mcpServers:
  - type: stdio
    name: calculator
    cmd: pyhub-llm mcp-server run calculator
    timeout: 60
    description: ìˆ˜í•™ ê³„ì‚° ë„êµ¬
  
  - type: streamable_http
    name: greeting
    url: http://localhost:8888/mcp
    filter_tools: greet,hello  # íŠ¹ì • ë„êµ¬ë§Œ ì‚¬ìš©
```

```python
# íŒŒì¼ ê²½ë¡œë¡œ ì§ì ‘ ë¡œë“œ
llm = await LLM.create_async("gpt-4o-mini", mcp_servers="mcp_config.yaml")

# ë˜ëŠ” ë‹¤ë¥¸ ì„¤ì •ê³¼ í•¨ê»˜
config = {
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "mcpServers": [
        {"type": "stdio", "name": "calc", "cmd": "..."}
    ]
}
llm = await LLM.create_async("gpt-4o-mini", mcp_servers=config)
```

#### 5. ì—¬ëŸ¬ MCP ì„œë²„ í†µí•©í•˜ê¸°

ë¨¼ì € greeting ì„œë²„ë¥¼ 8888 í¬íŠ¸ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
# greeting ì„œë²„ë¥¼ 8888 í¬íŠ¸ë¡œ ì‹¤í–‰
pyhub-llm mcp-server run greeting --port 8888
```

ê¸°ì¡´ ë°©ì‹ (ìˆ˜ë™ ê´€ë¦¬):

```python
import asyncio
from pyhub.llm import LLM
from pyhub.llm.mcp import MultiServerMCPClient, McpStdioConfig, McpStreamableHttpConfig

async def use_multiple_mcp_servers():
    # ìƒˆë¡œìš´ dataclass ë°©ì‹ (ê¶Œì¥)
    servers = [
        McpStdioConfig(
            name="calculator",
            cmd="pyhub-llm mcp-server run calculator",
            description="ê¸°ë³¸ ê³„ì‚° ê¸°ëŠ¥ ì œê³µ"
        ),
        McpStreamableHttpConfig(
            name="greeting",
            url="http://localhost:8888/mcp",
            description="ë‹¤êµ­ì–´ ì¸ì‚¬ë§ ìƒì„±"
        )
    ]
    
    # MultiServerMCPClientë¡œ ì—¬ëŸ¬ ì„œë²„ ì—°ê²°
    multi_client = MultiServerMCPClient(servers)
    
    async with multi_client:
        # ëª¨ë“  ì„œë²„ì˜ ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
        all_tools = await multi_client.get_tools()
        
        print(f"ì´ {len(all_tools)}ê°œì˜ ë„êµ¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤:")
        for tool in all_tools:
            print(f"- {tool.name}: {tool.description}")
        
        # LLM ìƒì„± (ëª¨ë“  ë„êµ¬ í¬í•¨)
        llm = LLM.create("gpt-4o-mini", tools=all_tools)
        
        # ì—¬ëŸ¬ ì„œë²„ì˜ ë„êµ¬ë¥¼ í•¨ê»˜ ì‚¬ìš©
        response = await llm.ask_async(
            "Johnì—ê²Œ í•œêµ­ì–´ë¡œ ì¸ì‚¬í•˜ê³ , 20ê³¼ 15ë¥¼ ë”í•´ì£¼ì„¸ìš”."
        )
        
        print(f"\në‹µë³€: {response}")

# ì‹¤í–‰
asyncio.run(use_multiple_mcp_servers())
```

ê¸°ì¡´ dict ë°©ì‹ë„ ê³„ì† ì§€ì›í•©ë‹ˆë‹¤:

```python
# ê¸°ì¡´ dict ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
servers = {
    "calculator": {
        "transport": "stdio",
        "command": "pyhub-llm",
        "args": ["mcp-server", "run", "calculator"],
    },
    "greeting": {
        "transport": "streamable_http",
        "url": "http://localhost:8888/mcp"
    }
}

multi_client = MultiServerMCPClient(servers)
```

#### ê³ ê¸‰ ì‚¬ìš©ë²•: ë‹¤ì–‘í•œ ì „ì†¡ ë°©ì‹

MCPëŠ” ë‹¤ì–‘í•œ ì „ì†¡ ë°©ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤:

```python
from pyhub.llm.mcp import MCPClient

# STDIO (ë¡œì»¬ í”„ë¡œì„¸ìŠ¤)
stdio_client = MCPClient({
    "transport": "stdio",
    "command": "python3",
    "args": ["my_server.py"]
})

# HTTP
http_client = MCPClient({
    "transport": "streamable_http",
    "url": "http://localhost:8080/mcp"
})

# WebSocket
ws_client = MCPClient({
    "transport": "websocket",
    "url": "ws://localhost:8080/mcp/ws"
})

# Server-Sent Events (SSE)
sse_client = MCPClient({
    "transport": "sse",
    "url": "http://localhost:8080/mcp/sse"
})
```

## ê°œë°œ

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸
make test

# íŠ¹ì • í…ŒìŠ¤íŠ¸
make test tests/test_openai.py

# ì»¤ë²„ë¦¬ì§€ í¬í•¨ í…ŒìŠ¤íŠ¸
make test-cov
# ë˜ëŠ”
make cov

# ì»¤ë²„ë¦¬ì§€ HTML ë¦¬í¬íŠ¸ ë³´ê¸°
make test-cov-report

# íŠ¹ì • íŒŒì¼ë§Œ ì»¤ë²„ë¦¬ì§€ í…ŒìŠ¤íŠ¸
make cov tests/test_optional_dependencies.py

# pytest ì§ì ‘ ì‹¤í–‰
pytest --cov=src/pyhub/llm --cov-report=term --cov-report=html
```

### ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬

```bash
# í¬ë§·íŒ… ë° ë¦°íŒ…
make format
make lint

# íƒ€ì… ì²´í¬
mypy src/
```

### ë¹Œë“œ ë° ë°°í¬

```bash
# íŒ¨í‚¤ì§€ ë¹Œë“œ
make build

# PyPI ë°°í¬ (ê¶Œí•œ í•„ìš”)
make release
```

## ê¸°ì—¬í•˜ê¸°

1. ì´ ì €ì¥ì†Œë¥¼ í¬í¬í•©ë‹ˆë‹¤
2. ê¸°ëŠ¥ ë¸Œëœì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ì‚¬í•­ì„ ì»¤ë°‹í•©ë‹ˆë‹¤ (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ì— í‘¸ì‹œí•©ë‹ˆë‹¤ (`git push origin feature/amazing-feature`)
5. Pull Requestë¥¼ ìƒì„±í•©ë‹ˆë‹¤

### ê¸°ì—¬ ê°€ì´ë“œë¼ì¸

- ëª¨ë“  ìƒˆ ê¸°ëŠ¥ì—ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
- ì½”ë“œ ìŠ¤íƒ€ì¼ì€ Blackê³¼ Ruffë¥¼ ë”°ë¦…ë‹ˆë‹¤
- íƒ€ì… íŒíŠ¸ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”
- ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”

## ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

**Q: API í‚¤ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤**

```python
# í•´ê²° ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
import os
os.environ["OPENAI_API_KEY"] = "your-key"

# í•´ê²° ë°©ë²• 2: ì§ì ‘ ì „ë‹¬
llm = OpenAILLM(api_key="your-key")
```

**Q: ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤**

```python
# ìºì‹œ ì¸ì ì…˜ìœ¼ë¡œ ìºì‹± í™œì„±í™”
from pyhub.llm.cache import MemoryCache
cache = MemoryCache()
llm = LLM.create("gpt-4o-mini", cache=cache)
reply = llm.ask("...")

# ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
llm = LLM.create("gpt-3.5-turbo")
```

**Q: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤**

```python
# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œí•œ
llm = LLM.create(
    "gpt-4o-mini",
    initial_messages=[]  # íˆìŠ¤í† ë¦¬ ì—†ì´ ì‹œì‘
)

# ì£¼ê¸°ì ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ ì •ë¦¬
if len(llm) > 10:
    llm.clear()
```

## ë§í¬

- [ë¬¸ì„œ](https://pyhub-llm.readthedocs.io)
- [PyPI](https://pypi.org/project/pyhub-llm)
- [GitHub](https://github.com/pyhub-kr/pyhub-llm)
- [ì´ìŠˆ íŠ¸ë˜ì»¤](https://github.com/pyhub-kr/pyhub-llm/issues)
