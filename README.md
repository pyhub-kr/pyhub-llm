# pyhub-llm

ë‹¤ì–‘í•œ LLM ì œê³µì—…ì²´ë¥¼ ìœ„í•œ í†µí•© Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. OpenAI, Anthropic, Google, Ollama ë“±ì˜ APIë¥¼ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥

- ğŸ”Œ **í†µí•© ì¸í„°í˜ì´ìŠ¤**: ëª¨ë“  LLM ì œê³µì—…ì²´ë¥¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©
- ğŸš€ **ê°„í¸í•œ ì „í™˜**: ì½”ë“œ ë³€ê²½ ì—†ì´ ëª¨ë¸ ì „í™˜ ê°€ëŠ¥
- ğŸ’¾ **ìºì‹± ì§€ì›**: ì‘ë‹µ ìºì‹±ìœ¼ë¡œ ë¹„ìš© ì ˆê° ë° ì„±ëŠ¥ í–¥ìƒ
- ğŸ”„ **ìŠ¤íŠ¸ë¦¬ë° ì§€ì›**: ì‹¤ì‹œê°„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
- ğŸ› ï¸ **ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ**: Function calling ì§€ì›
- ğŸ“· **ì´ë¯¸ì§€ ì²˜ë¦¬**: ì´ë¯¸ì§€ ì„¤ëª… ë° ë¶„ì„ ê¸°ëŠ¥
- âš¡ **ë¹„ë™ê¸° ì§€ì›**: ë™ê¸°/ë¹„ë™ê¸° ëª¨ë‘ ì§€ì›
- ğŸ”— **ì²´ì´ë‹**: ì—¬ëŸ¬ LLMì„ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±

## ì„¤ì¹˜

### ê¸°ë³¸ ì„¤ì¹˜

```bash
pip install pyhub-llm
```

### íŠ¹ì • ì œê³µì—…ì²´ë§Œ ì„¤ì¹˜

```bash
# OpenAIë§Œ
pip install "pyhub-llm[openai]"

# Anthropicë§Œ
pip install "pyhub-llm[anthropic]"

# Googleë§Œ
pip install "pyhub-llm[google]"

# Ollamaë§Œ
pip install "pyhub-llm[ollama]"

# ëª¨ë“  ì œê³µì—…ì²´
pip install "pyhub-llm[all]"
```

### ê°œë°œ í™˜ê²½ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/pyhub-kr/pyhub-llm.git
cd pyhub-llm

# ê°œë°œ í™˜ê²½ ì„¤ì¹˜
pip install -e ".[dev,all]"
# í˜¹ì€ make install
```

## ë¹ ë¥¸ ì‹œì‘

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from pyhub.llm import LLM

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = LLM.create("gpt-4o-mini")

# ì§ˆë¬¸í•˜ê¸°
reply = llm.ask("Pythonì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
print(reply.text)
```

### ëª¨ë¸ë³„ ì§ì ‘ ì‚¬ìš©

ê° í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¨¼ì € ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
# OpenAI ì‚¬ìš©ì‹œ
pip install "pyhub-llm[openai]"

# Anthropic ì‚¬ìš©ì‹œ
pip install "pyhub-llm[anthropic]"

# Google ì‚¬ìš©ì‹œ
pip install "pyhub-llm[google]"

# Ollama ì‚¬ìš©ì‹œ (ë¡œì»¬ ì‹¤í–‰)
pip install "pyhub-llm[ollama]"
```

```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM, OllamaLLM

# OpenAI (OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
openai_llm = OpenAILLM(model="gpt-4o-mini")
reply = openai_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# API í‚¤ ì§ì ‘ ì „ë‹¬
openai_llm = OpenAILLM(model="gpt-4o-mini", api_key="your-api-key")

# Anthropic (ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
claude_llm = AnthropicLLM(model="claude-3-haiku-20240307")
reply = claude_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# Google (GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
gemini_llm = GoogleLLM(model="gemini-1.5-flash")
reply = gemini_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")

# Ollama (ë¡œì»¬ ì‹¤í–‰, API í‚¤ ë¶ˆí•„ìš”)
ollama_llm = OllamaLLM(model="mistral")
reply = ollama_llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")
```

## Ollama ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©

OllamaëŠ” ë¡œì»¬ì—ì„œ LLMì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ì…ë‹ˆë‹¤. API í‚¤ê°€ í•„ìš” ì—†ê³ , ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•Šì•„ ê°œì¸ì •ë³´ ë³´í˜¸ì— ìœ ë¦¬í•©ë‹ˆë‹¤.

### Ollama ì„¤ì¹˜

#### macOS
```bash
# Homebrew ì‚¬ìš©
brew install ollama

# ë˜ëŠ” ê³µì‹ ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Linux
```bash
# ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
curl -fsSL https://ollama.ai/install.sh | sh

# ë˜ëŠ” Docker ì‚¬ìš©
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### Windows
```bash
# PowerShellì—ì„œ ì‹¤í–‰
iex (irm https://ollama.ai/install.ps1)

# ë˜ëŠ” ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ
# https://ollama.ai/download/windows
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰

```bash
# Ollama ì„œë¹„ìŠ¤ ì‹œì‘ (í•„ìš”í•œ ê²½ìš°)
ollama serve

# Mistral ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull mistral

# ë‹¤ë¥¸ ì¸ê¸° ëª¨ë¸ë“¤
ollama pull llama3.3
ollama pull gemma2
ollama pull qwen2

# ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list

# ëª¨ë¸ ì§ì ‘ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
ollama run mistral
```

### pyhub-llmì—ì„œ Ollama ì‚¬ìš©

```python
from pyhub.llm import OllamaLLM

# ê¸°ë³¸ ì‚¬ìš©ë²•
llm = OllamaLLM(model="mistral")
reply = llm.ask("Pythonìœ¼ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”")
print(reply.text)

# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ ë°›ê¸°
for chunk in llm.ask("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)

# ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì§ˆë¬¸í•˜ê¸°
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?",
    files=["image.jpg"]
)

# PDF íŒŒì¼ ì²˜ë¦¬ (ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë¨)
reply = llm.ask(
    "ì´ PDF ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”",
    files=["document.pdf"]  # ìë™ìœ¼ë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¡œ ë³€í™˜
)

# ë¹„ë™ê¸° ì‚¬ìš©
async def async_example():
    reply = await llm.ask_async("ë¹„ë™ê¸°ë¡œ ì§ˆë¬¸í•©ë‹ˆë‹¤")
    return reply.text

# ì»¤ìŠ¤í…€ ì„¤ì •
llm = OllamaLLM(
    model="mistral",
    temperature=0.7,
    max_tokens=2000,
    base_url="http://localhost:11434"  # ì»¤ìŠ¤í…€ Ollama ì„œë²„
)
```

### Ollama ì¥ì 

- **ğŸ”’ ê°œì¸ì •ë³´ ë³´í˜¸**: ëª¨ë“  ë°ì´í„°ê°€ ë¡œì»¬ì—ì„œ ì²˜ë¦¬
- **ğŸ’° ë¹„ìš© ì ˆê°**: API í˜¸ì¶œ ë¹„ìš© ì—†ìŒ
- **âš¡ ë¹ ë¥¸ ì‘ë‹µ**: ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì—†ìŒ  
- **ğŸŒ ì˜¤í”„ë¼ì¸ ì‚¬ìš©**: ì¸í„°ë„· ì—°ê²° ë¶ˆí•„ìš”
- **ğŸ›ï¸ ì™„ì „í•œ ì œì–´**: ëª¨ë¸ íŒŒë¼ë¯¸í„° ììœ  ì¡°ì •

### ì§€ì› ëª¨ë¸

- **Llama ê³„ì—´**: llama3.3, llama3.1, llama3.2
- **Mistral**: mistral, mixtral
- **Gemma**: gemma2, gemma3  
- **Qwen**: qwen2, qwen2.5
- **ê¸°íƒ€**: phi3, codellama, vicuna ë“±

> **ì°¸ê³ **: PDF íŒŒì¼ ì²˜ë¦¬ ì‹œ OllamaëŠ” ìë™ìœ¼ë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë³´ì¡´ì„ ìœ„í•´ 600 DPIë¡œ ë³€í™˜ë©ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥ ì˜ˆì œ

### 1. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
# ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
for chunk in llm.ask("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)

# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
async for chunk in await llm.ask_async("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)
```

### 2. ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬

```python
# ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
llm = LLM.create("gpt-4o-mini")

# ì²« ë²ˆì§¸ ì§ˆë¬¸
llm.ask("ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤", use_history=True)

# ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ì´ì „ ëŒ€í™” ê¸°ì–µ)
reply = llm.ask("ì œ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì£ ?", use_history=True)
print(reply.text)  # "ê¹€ì² ìˆ˜ë¼ê³  í•˜ì…¨ìŠµë‹ˆë‹¤"

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
llm.clear()
```

### 3. íŒŒì¼ ì²˜ë¦¬ (ì´ë¯¸ì§€ ë° PDF)

```python
# ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    files=["photo.jpg"]
)

# PDF íŒŒì¼ ì²˜ë¦¬ (Providerë³„ ì§€ì› í˜„í™©)
# - OpenAI, Anthropic, Google: PDF ì§ì ‘ ì§€ì›
# - Ollama: PDFë¥¼ ì´ë¯¸ì§€ë¡œ ìë™ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
reply = llm.ask(
    "ì´ PDF ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”",
    files=["document.pdf"]
)

# ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
reply = llm.ask(
    "ì´ íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ë¹„êµí•´ì£¼ì„¸ìš”",
    files=["doc1.pdf", "image1.jpg", "doc2.pdf"]
)

# ë‹¨ì¼ ì´ë¯¸ì§€ ì„¤ëª… (í¸ì˜ ë©”ì„œë“œ)
reply = llm.describe_image("photo.jpg")
print(reply.text)

# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ë¶„ì„
reply = llm.describe_image(
    "photo.jpg",
    prompt="ì´ ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ìƒ‰ìƒì€ ë¬´ì—‡ì¸ê°€ìš”?"
)

# ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ì²˜ë¦¬
responses = llm.describe_images([
    "image1.jpg",
    "image2.jpg",
    "image3.jpg"
])

# ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
text = llm.extract_text_from_image("document.jpg")
```

#### Providerë³„ íŒŒì¼ ì§€ì› í˜„í™©

| Provider | ì´ë¯¸ì§€ | PDF | ë¹„ê³  |
|----------|--------|-----|------|
| OpenAI | âœ… | âœ… | PDF ì§ì ‘ ì§€ì› |
| Anthropic | âœ… | âœ… | PDF ë² íƒ€ ì§€ì› |
| Google Gemini | âœ… | âœ… | PDF ë„¤ì´í‹°ë¸Œ ì§€ì› |
| Ollama | âœ… | âš ï¸ | PDFâ†’ì´ë¯¸ì§€ ìë™ ë³€í™˜ |

> **ì°¸ê³ **: Ollamaì—ì„œ PDF íŒŒì¼ ì‚¬ìš© ì‹œ ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë˜ë©°, ê²½ê³  ë¡œê·¸ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.

### 4. ì„ íƒì§€ ì œí•œ

```python
# ì„ íƒì§€ ì¤‘ì—ì„œë§Œ ì‘ë‹µ
reply = llm.ask(
    "ì´ ë¦¬ë·°ì˜ ê°ì •ì€?",
    context={"review": "ì •ë§ ìµœê³ ì˜ ì œí’ˆì…ë‹ˆë‹¤!"},
    choices=["ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½"]
)
print(reply.choice)  # "ê¸ì •"
print(reply.confidence)  # 0.95
```

### 5. ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ

```python
from pyhub.llm.tools import Tool

# ë„êµ¬ ì •ì˜
def get_weather(city: str) -> str:
    return f"{city}ì˜ ë‚ ì”¨ëŠ” ë§‘ìŒì…ë‹ˆë‹¤."

weather_tool = Tool(
    name="get_weather",
    description="ë„ì‹œì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤",
    func=get_weather,
    parameters={
        "type": "object",
        "properties": {
            "city": {"type": "string", "description": "ë„ì‹œ ì´ë¦„"}
        },
        "required": ["city"]
    }
)

# ë„êµ¬ì™€ í•¨ê»˜ LLM ì‚¬ìš©
reply = llm.ask(
    "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?",
    tools=[weather_tool]
)
print(reply.text)  # "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ë§‘ìŒì…ë‹ˆë‹¤."
```

### 6. LLM ì²´ì´ë‹

```python
# ë²ˆì—­ ì²´ì¸ êµ¬ì„±
translator = LLM.create(
    "gpt-4o-mini",
    prompt="ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”: {text}"
)

summarizer = LLM.create(
    "gpt-4o-mini",
    prompt="ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”: {text}"
)

# ì²´ì¸ ì—°ê²°
chain = translator | summarizer

# ì‹¤í–‰
result = chain.ask({"text": "ì¸ê³µì§€ëŠ¥ì€ ìš°ë¦¬ì˜ ë¯¸ë˜ë¥¼ ë°”ê¿€ ê²ƒì…ë‹ˆë‹¤..."})
print(result.values["text"])  # ë²ˆì—­ í›„ ìš”ì•½ëœ ê²°ê³¼
```

### 7. ìºì‹± ì‚¬ìš©

#### ìºì‹œ ì¸ì ì…˜ íŒ¨í„´

```python
from pyhub.llm.cache import MemoryCache, FileCache

# ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©
memory_cache = MemoryCache(ttl=3600)  # 1ì‹œê°„ TTL
llm = LLM.create("gpt-4o-mini", cache=memory_cache)

# íŒŒì¼ ìºì‹œ ì‚¬ìš©  
file_cache = FileCache(cache_dir=".cache", ttl=7200)  # 2ì‹œê°„ TTL
llm = LLM.create("gpt-4o-mini", cache=file_cache)

# ìºì‹œê°€ ì„¤ì •ëœ LLMì€ ìë™ìœ¼ë¡œ ìºì‹œ ì‚¬ìš©
reply = llm.ask("ì§ˆë¬¸")

# ì»¤ìŠ¤í…€ ìºì‹œ ë°±ì—”ë“œ êµ¬í˜„
class CustomCache(BaseCache):
    def get(self, key: str):
        # Redis, Database ë“± ì»¤ìŠ¤í…€ ìºì‹œ ë¡œì§
        pass
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None):
        # ì»¤ìŠ¤í…€ ì €ì¥ ë¡œì§
        pass

custom_cache = CustomCache()
llm = LLM.create("gpt-4o-mini", cache=custom_cache)
```

### 8. í…œí”Œë¦¿ ì‚¬ìš©

```python
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
llm = LLM.create(
    "gpt-4o-mini",
    system_prompt="ë‹¹ì‹ ì€ {role}ì…ë‹ˆë‹¤.",
    prompt="ì§ˆë¬¸: {question}\në‹µë³€:"
)

# í…œí”Œë¦¿ ë³€ìˆ˜ì™€ í•¨ê»˜ ì‚¬ìš©
reply = llm.ask({
    "role": "ìˆ˜í•™ êµì‚¬",
    "question": "í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬ë€?"
})
```

## API í‚¤ ì„¤ì •

### í•„ìš”í•œ API í‚¤

ê° í”„ë¡œë°”ì´ë”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í•´ë‹¹ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤:

- **OpenAI**: `OPENAI_API_KEY` - [API í‚¤ ë°œê¸‰](https://platform.openai.com/api-keys)
- **Anthropic**: `ANTHROPIC_API_KEY` - [API í‚¤ ë°œê¸‰](https://console.anthropic.com/settings/keys)
- **Google**: `GOOGLE_API_KEY` - [API í‚¤ ë°œê¸‰](https://makersuite.google.com/app/apikey)
- **Upstage**: `UPSTAGE_API_KEY` - [API í‚¤ ë°œê¸‰](https://console.upstage.ai/)

### ì„¤ì • ë°©ë²•

#### 1. í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"
```

#### 2. .env íŒŒì¼ ì‚¬ìš©
```bash
# .env íŒŒì¼ ìƒì„±
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
GOOGLE_API_KEY=your-google-key
```

#### 3. ì½”ë“œì—ì„œ ì§ì ‘ ì „ë‹¬
```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM

# API í‚¤ë¥¼ ì§ì ‘ ì „ë‹¬
llm = OpenAILLM(api_key="your-api-key")
llm = AnthropicLLM(api_key="your-api-key")
llm = GoogleLLM(api_key="your-api-key")
```

## í™˜ê²½ ì„¤ì •

### pyproject.toml ì„¤ì •

```toml
[tool.pyhub.llm]
# ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
default_model = "gpt-4o-mini"
default_embedding_model = "text-embedding-3-small"

# ê¸°ë³¸ íŒŒë¼ë¯¸í„°
temperature = 0.7
max_tokens = 1000

# ìºì‹œ ì„¤ì •
cache_ttl = 3600
cache_dir = ".cache/llm"
```

## CLI ì‚¬ìš©ë²•

### ëŒ€í™”í˜• ì±„íŒ…
```bash
# ê¸°ë³¸ ëª¨ë¸ë¡œ ì±„íŒ…
pyhub-llm chat

# íŠ¹ì • ëª¨ë¸ë¡œ ì±„íŒ…
pyhub-llm chat --model claude-3-haiku-20240307

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
pyhub-llm chat --system "ë‹¹ì‹ ì€ íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤"
```

### ë‹¨ì¼ ì§ˆë¬¸
```bash
# ì§ˆë¬¸í•˜ê³  ì‘ë‹µ ë°›ê¸°
pyhub-llm ask "Pythonê³¼ Goì˜ ì°¨ì´ì ì€?"

# íŒŒì¼ ë‚´ìš©ê³¼ í•¨ê»˜ ì§ˆë¬¸
pyhub-llm ask "ì´ ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”" --file main.py
```

### ì´ë¯¸ì§€ ì„¤ëª…
```bash
# ì´ë¯¸ì§€ ì„¤ëª…
pyhub-llm describe image.jpg

# ì—¬ëŸ¬ ì´ë¯¸ì§€ ì„¤ëª…
pyhub-llm describe *.jpg --output descriptions.json
```

### ì„ë² ë”© ìƒì„±
```bash
# í…ìŠ¤íŠ¸ ì„ë² ë”©
pyhub-llm embed "ì„ë² ë”©í•  í…ìŠ¤íŠ¸"

# íŒŒì¼ ë‚´ìš© ì„ë² ë”©
pyhub-llm embed --file document.txt
```

## ê³ ê¸‰ ê¸°ëŠ¥

### êµ¬ì¡°í™”ëœ ì¶œë ¥ (Structured Output)

Pydantic BaseModelì„ ì‚¬ìš©í•˜ì—¬ LLM ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from pydantic import BaseModel, Field
from typing import Optional, List
from pyhub.llm import LLM

# ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜
class User(BaseModel):
    name: str = Field(description="ì‚¬ìš©ì ì´ë¦„")
    age: int = Field(description="ì‚¬ìš©ì ë‚˜ì´")
    email: str = Field(description="ì´ë©”ì¼ ì£¼ì†Œ")
    
class Product(BaseModel):
    name: str
    price: float
    features: List[str]
    in_stock: bool

# êµ¬ì¡°í™”ëœ ì‘ë‹µ ìš”ì²­
llm = LLM.create("gpt-4o-mini")

# ë‹¨ìˆœí•œ ì˜ˆì‹œ
response = llm.ask(
    "John Doe, 30ì‚´, john@example.com ì •ë³´ë¡œ ì‚¬ìš©ìë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
    schema=User
)

if response.has_structured_data:
    user = response.structured_data
    print(f"ì´ë¦„: {user.name}")
    print(f"ë‚˜ì´: {user.age}")
    print(f"ì´ë©”ì¼: {user.email}")

# ë³µì¡í•œ ì˜ˆì‹œ
response = llm.ask(
    "MacBook Pro 16ì¸ì¹˜ì— ëŒ€í•œ ì œí’ˆ ì •ë³´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”",
    schema=Product
)

if response.has_structured_data:
    product = response.structured_data
    print(f"ì œí’ˆëª…: {product.name}")
    print(f"ê°€ê²©: ${product.price}")
    print(f"íŠ¹ì§•: {', '.join(product.features)}")
```

êµ¬ì¡°í™”ëœ ì¶œë ¥ì€ ëª¨ë“  í”„ë¡œë°”ì´ë”(OpenAI, Anthropic, Google, Ollama)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤:
- OpenAI: ë„¤ì´í‹°ë¸Œ Structured Output ì‚¬ìš©
- Anthropic, Google, Ollama: í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ JSON ìƒì„±

### ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬

```python
from pyhub.llm.agents import ReactAgent
from pyhub.llm.tools import WebSearchTool, CalculatorTool

# ë„êµ¬ë¥¼ ê°€ì§„ ì—ì´ì „íŠ¸ ìƒì„±
agent = ReactAgent(
    llm=LLM.create("gpt-4o"),
    tools=[WebSearchTool(), CalculatorTool()],
    max_iterations=5
)

# ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰
result = agent.run(
    "2024ë…„ í•œêµ­ì˜ GDPëŠ” ì–¼ë§ˆì´ê³ , "
    "ì´ë¥¼ ì›í™”ë¡œ í™˜ì‚°í•˜ë©´ ì–¼ë§ˆì¸ê°€ìš”?"
)
```

### MCP (Model Context Protocol) í†µí•©

```python
from pyhub.llm.agents.mcp import MCPClient

# MCP ì„œë²„ ì—°ê²°
mcp_client = MCPClient("localhost:8080")

# MCP ë„êµ¬ë¥¼ LLMê³¼ í•¨ê»˜ ì‚¬ìš©
llm = LLM.create("gpt-4o", tools=mcp_client.get_tools())
reply = llm.ask("í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”")
```

## ê°œë°œ

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸
make test

# íŠ¹ì • í…ŒìŠ¤íŠ¸
make test tests/test_openai.py

# ì»¤ë²„ë¦¬ì§€ í¬í•¨ í…ŒìŠ¤íŠ¸
make test-cov
# ë˜ëŠ”
make cov

# ì»¤ë²„ë¦¬ì§€ HTML ë¦¬í¬íŠ¸ ë³´ê¸°
make test-cov-report

# íŠ¹ì • íŒŒì¼ë§Œ ì»¤ë²„ë¦¬ì§€ í…ŒìŠ¤íŠ¸
make cov tests/test_optional_dependencies.py

# pytest ì§ì ‘ ì‹¤í–‰
pytest --cov=src/pyhub/llm --cov-report=term --cov-report=html
```

### ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬

```bash
# í¬ë§·íŒ… ë° ë¦°íŒ…
make format
make lint

# íƒ€ì… ì²´í¬
mypy src/
```

### ë¹Œë“œ ë° ë°°í¬

```bash
# íŒ¨í‚¤ì§€ ë¹Œë“œ
make build

# PyPI ë°°í¬ (ê¶Œí•œ í•„ìš”)
make release
```

## ê¸°ì—¬í•˜ê¸°

1. ì´ ì €ì¥ì†Œë¥¼ í¬í¬í•©ë‹ˆë‹¤
2. ê¸°ëŠ¥ ë¸Œëœì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ì‚¬í•­ì„ ì»¤ë°‹í•©ë‹ˆë‹¤ (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ì— í‘¸ì‹œí•©ë‹ˆë‹¤ (`git push origin feature/amazing-feature`)
5. Pull Requestë¥¼ ìƒì„±í•©ë‹ˆë‹¤

### ê¸°ì—¬ ê°€ì´ë“œë¼ì¸

- ëª¨ë“  ìƒˆ ê¸°ëŠ¥ì—ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
- ì½”ë“œ ìŠ¤íƒ€ì¼ì€ Blackê³¼ Ruffë¥¼ ë”°ë¦…ë‹ˆë‹¤
- íƒ€ì… íŒíŠ¸ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”
- ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”

## ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

**Q: API í‚¤ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤**

```python
# í•´ê²° ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
import os
os.environ["OPENAI_API_KEY"] = "your-key"

# í•´ê²° ë°©ë²• 2: ì§ì ‘ ì „ë‹¬
llm = OpenAILLM(api_key="your-key")
```

**Q: ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤**

```python
# ìºì‹œ ì¸ì ì…˜ìœ¼ë¡œ ìºì‹± í™œì„±í™”
from pyhub.llm.cache import MemoryCache
cache = MemoryCache()
llm = LLM.create("gpt-4o-mini", cache=cache)
reply = llm.ask("...")

# ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
llm = LLM.create("gpt-3.5-turbo")
```

**Q: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤**

```python
# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œí•œ
llm = LLM.create(
    "gpt-4o-mini",
    initial_messages=[]  # íˆìŠ¤í† ë¦¬ ì—†ì´ ì‹œì‘
)

# ì£¼ê¸°ì ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ ì •ë¦¬
if len(llm) > 10:
    llm.clear()
```

## ë§í¬

- [ë¬¸ì„œ](https://pyhub-llm.readthedocs.io)
- [PyPI](https://pypi.org/project/pyhub-llm)
- [GitHub](https://github.com/pyhub-kr/pyhub-llm)
- [ì´ìŠˆ íŠ¸ë˜ì»¤](https://github.com/pyhub-kr/pyhub-llm/issues)

