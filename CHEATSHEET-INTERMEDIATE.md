# pyhub-llm ì¤‘ê¸‰ ê°€ì´ë“œ

pyhub-llmì˜ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ í™œìš©í•˜ì—¬ ë” ë³µì¡í•˜ê³  íš¨ìœ¨ì ì¸ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ì„¸ìš”. ì´ ê°€ì´ë“œëŠ” êµ¬ì¡°í™”ëœ ì¶œë ¥, ë¹„ë™ê¸° ì²˜ë¦¬, ìºì‹±, ë„êµ¬ í˜¸ì¶œ ë“± ì¤‘ê¸‰ ìˆ˜ì¤€ì˜ ê¸°ëŠ¥ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.

> ğŸ’¡ ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ me@pyhub.krë¡œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.

## ëª©ì°¨

- [êµ¬ì¡°í™”ëœ ì¶œë ¥](#êµ¬ì¡°í™”ëœ-ì¶œë ¥)
- [ë¶„ë¥˜ ë° ì„ íƒ](#ë¶„ë¥˜-ë°-ì„ íƒ)
- [ë¹„ë™ê¸° ì²˜ë¦¬](#ë¹„ë™ê¸°-ì²˜ë¦¬)
- [ìºì‹±](#ìºì‹±)
- [ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ](#ë„êµ¬í•¨ìˆ˜-í˜¸ì¶œ)
- [í…œí”Œë¦¿ í™œìš©](#í…œí”Œë¦¿-í™œìš©)
- [í”„ë¡¬í”„íŠ¸ í—ˆë¸Œ (Hub)](#í”„ë¡¬í”„íŠ¸-í—ˆë¸Œ-hub)
- [History Backup](#history-backup)
- [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)
## êµ¬ì¡°í™”ëœ ì¶œë ¥

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/intermediate/01_structured_output.py)

### Pydantic ìŠ¤í‚¤ë§ˆ ì‚¬ìš©

```python
from pydantic import BaseModel, Field
from typing import List
from pyhub.llm import OpenAILLM

class BookInfo(BaseModel):
    title: str = Field(description="ì±… ì œëª©")
    author: str = Field(description="ì €ì")
    year: int = Field(description="ì¶œíŒ ì—°ë„")
    genres: List[str] = Field(description="ì¥ë¥´ ëª©ë¡")
    summary: str = Field(description="ê°„ë‹¨í•œ ì¤„ê±°ë¦¬")

llm = OpenAILLM(model="gpt-4o-mini")
reply = llm.ask(
    "í•´ë¦¬í¬í„°ì™€ ë§ˆë²•ì‚¬ì˜ ëŒì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
    schema=BookInfo
)

book: BookInfo = reply.structured_data
print(f"ì œëª©: {book.title}")
print(f"ì €ì: {book.author}")
print(f"ì¥ë¥´: {', '.join(book.genres)}")
```

### ë³µì¡í•œ êµ¬ì¡°

```python
class Company(BaseModel):
    name: str
    founded: int
    headquarters: str

class ProductAnalysis(BaseModel):
    product_name: str
    manufacturer: Company
    pros: List[str]
    cons: List[str]
    rating: float = Field(ge=0, le=5)
    recommendation: bool

llm = OpenAILLM(model="gpt-4o-mini")
reply = llm.ask(
    "iPhone 15 Proì— ëŒ€í•œ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”",
    schema=ProductAnalysis
)

analysis: ProductAnalysis = reply.structured_data
print(f"ì œì¡°ì‚¬: {analysis.manufacturer.name}")
print(f"í‰ì : {analysis.rating}/5.0")
```

### ë‹¤êµ­ì–´ ì‘ë‹µ

```python
class Translation(BaseModel):
    korean: str
    english: str
    japanese: str
    chinese: str

llm = OpenAILLM(model="gpt-4o-mini", system_prompt="ë‹¤êµ­ì–´ ë²ˆì—­ ì „ë¬¸ê°€")
reply = llm.ask("'ì¸ê³µì§€ëŠ¥'ì„ 4ê°œ ì–¸ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”", schema=Translation)

trans: Translation = reply.structured_data
print(f"í•œêµ­ì–´: {trans.korean}")
print(f"ì˜ì–´: {trans.english}")
print(f"ì¼ë³¸ì–´: {trans.japanese}")
print(f"ì¤‘êµ­ì–´: {trans.chinese}")
```


## ë¶„ë¥˜ ë° ì„ íƒ

### ê°ì • ë¶„ì„

```python
from pyhub.llm import OpenAILLM

llm = OpenAILLM(model="gpt-4o-mini")

# ë‹¨ì¼ ì„ íƒ
emotions = ["ê¸°ì¨", "ìŠ¬í””", "ë¶„ë…¸", "ê³µí¬", "ë†€ëŒ", "í˜ì˜¤"]
reply = llm.ask("ì˜¤ëŠ˜ ìŠ¹ì§„í–ˆì–´ìš”! ì¶•í•˜ íŒŒí‹°ë„ í–ˆë‹µë‹ˆë‹¤.", choices=emotions)
print(f"ê°ì •: {reply.choice}")  # "ê¸°ì¨"
print(f"ì¸ë±ìŠ¤: {reply.choice_index}")  # 0

# ì—¬ëŸ¬ ë¬¸ì¥ ì¼ê´„ ì²˜ë¦¬
texts = [
    "í”„ë¡œì íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
    "ë³µê¶Œì— ë‹¹ì²¨ëì–´ìš”!",
    "ë˜ ì•¼ê·¼ì´ë„¤ìš”..."
]

for text in texts:
    reply = llm.ask(text, choices=emotions)
    print(f"{text} â†’ {reply.choice}")
```

### ì˜ë„ ë¶„ë¥˜

```python
class IntentClassifier:
    def __init__(self):
        self.llm = OpenAILLM(model="gpt-4o-mini")
        self.intents = [
            "ì§ˆë¬¸",
            "ìš”ì²­",
            "ë¶ˆë§Œ",
            "ì¹­ì°¬",
            "ì •ë³´ì œê³µ",
            "ê¸°íƒ€"
        ]
    
    def classify(self, text: str) -> str:
        reply = self.llm.ask(text, choices=self.intents)
        return reply.choice

classifier = IntentClassifier()
print(classifier.classify("ì´ ì œí’ˆ í™˜ë¶ˆ ê°€ëŠ¥í•œê°€ìš”?"))  # "ì§ˆë¬¸"
print(classifier.classify("ì •ë§ ìµœê³ ì˜ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤!"))  # "ì¹­ì°¬"
```

### ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜

```python
from pydantic import BaseModel
from typing import List

class TopicLabels(BaseModel):
    topics: List[str] = Field(description="í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì£¼ì œ")

llm = OpenAILLM(model="gpt-4o-mini", system_prompt="í…ìŠ¤íŠ¸ì˜ ì£¼ì œë¥¼ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€")

available_topics = ["ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ë¬¸í™”", "ìŠ¤í¬ì¸ ", "IT", "ê³¼í•™", "ê±´ê°•"]

prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì£¼ì œë¥¼ ì„ íƒí•˜ì„¸ìš”.
ê°€ëŠ¥í•œ ì£¼ì œ: {', '.join(available_topics)}

í…ìŠ¤íŠ¸: 'AI ê¸°ìˆ ì´ ì˜ë£Œ ë¶„ì•¼ì— í˜ëª…ì„ ì¼ìœ¼í‚¤ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì•” ì§„ë‹¨ì˜ ì •í™•ë„ê°€ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.'
"""

reply = llm.ask(prompt, schema=TopicLabels)
print(f"ë¶„ë¥˜ëœ ì£¼ì œ: {', '.join(reply.structured_data.topics)}")  # "IT, ê³¼í•™, ê±´ê°•"
```


## ë¹„ë™ê¸° ì²˜ë¦¬

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/intermediate/02_async_processing.py)

### ê¸°ë³¸ ë¹„ë™ê¸° ì‚¬ìš©

```python
import asyncio
from pyhub.llm import OpenAILLM

async def main():
    llm = OpenAILLM(model="gpt-4o-mini")
    
    # ë¹„ë™ê¸° ìš”ì²­
    reply = await llm.ask_async("ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì˜ ì¥ì ì€?")
    print(reply.text)
    
    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
    async for chunk in llm.ask_async("ê¸´ ì„¤ëª…ì„ í•´ì£¼ì„¸ìš”", stream=True):
        print(chunk.text, end="", flush=True)

# ì‹¤í–‰
asyncio.run(main())
```

### ë™ì‹œ ìš”ì²­ ì²˜ë¦¬

```python
async def process_multiple_queries():
    llm = OpenAILLM(model="gpt-4o-mini")
    
    queries = [
        "Pythonì˜ ì¥ì ì€?",
        "JavaScriptì˜ ì¥ì ì€?",
        "Rustì˜ ì¥ì ì€?"
    ]
    
    # ëª¨ë“  ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬
    tasks = [llm.ask_async(q) for q in queries]
    replies = await asyncio.gather(*tasks)
    
    for query, reply in zip(queries, replies):
        print(f"\nQ: {query}")
        print(f"A: {reply.text[:100]}...")

asyncio.run(process_multiple_queries())
```

### MCPì™€ í•¨ê»˜ ë¹„ë™ê¸° ì‚¬ìš©

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/mcp_integration_example.py)

```python
from pyhub.llm import OpenAILLM

async def main():
    # ê°„í¸í•œ ë¬¸ìì—´ ì„¤ì •ìœ¼ë¡œ MCP ì„œë²„ì™€ í•¨ê»˜ LLM ìƒì„±
    llm = await LLM.create_async(
        "gpt-4o-mini",
        mcp_servers="python calculator.py"  # ë¬¸ìì—´ë¡œ ê°„í¸ ì„¤ì •
    )
    
    # ë˜ëŠ” ë” ìƒì„¸í•œ ì„¤ì •
    # from pyhub.llm.mcp import McpConfig
    # llm = await LLM.create_async(
    #     "gpt-4o-mini",
    #     mcp_servers=McpConfig(
    #         cmd="calculator-server",
    #         name="my-calc"
    #     )
    # )
    
    # MCP ë„êµ¬ ì‚¬ìš©
    reply = await llm.ask_async("25 ê³±í•˜ê¸° 17ì€?")
    print(reply.text)
    
    # ì •ë¦¬
    await llm.close_mcp()

asyncio.run(main())
```


## ìºì‹±

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/intermediate/03_caching.py)

### ì¸ë©”ëª¨ë¦¬ ìºì‹±

```python
from pyhub.llm import OpenAILLM
from pyhub.llm.cache import InMemoryCache

# ìºì‹œ ì„¤ì •
cache = InMemoryCache(ttl=3600)  # 1ì‹œê°„ TTL
llm = OpenAILLM(model="gpt-4o-mini", cache=cache)

# ì²« ë²ˆì§¸ ìš”ì²­ (API í˜¸ì¶œ)
reply1 = llm.ask("íŒŒì´ì¬ì˜ ì—­ì‚¬ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”")
print("ì²« ë²ˆì§¸ ìš”ì²­ ì™„ë£Œ")

# ë‘ ë²ˆì§¸ ìš”ì²­ (ìºì‹œì—ì„œ ë°˜í™˜)
reply2 = llm.ask("íŒŒì´ì¬ì˜ ì—­ì‚¬ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”")
print("ìºì‹œëœ ì‘ë‹µ:", reply1.text == reply2.text)  # True
```

### íŒŒì¼ ê¸°ë°˜ ìºì‹±

```python
from pyhub.llm.cache import FileCache
from pathlib import Path

# íŒŒì¼ ìºì‹œ ì„¤ì •
cache_dir = Path("./llm_cache")
cache = FileCache(cache_dir=cache_dir, ttl=86400)  # 24ì‹œê°„ TTL

llm = OpenAILLM(model="gpt-4o-mini", cache=cache)

# ìºì‹œ í†µê³„
print(f"ìºì‹œ íˆíŠ¸ìœ¨: {cache.hit_rate:.2%}")
print(f"ìºì‹œ í¬ê¸°: {cache.size_bytes / 1024 / 1024:.2f} MB")

# ìºì‹œ ì •ë¦¬
cache.clear_expired()  # ë§Œë£Œëœ í•­ëª© ì‚­ì œ
# cache.clear()  # ì „ì²´ ìºì‹œ ì‚­ì œ
```

### ì¡°ê±´ë¶€ ìºì‹±

```python
class SmartCache:
    def __init__(self, llm):
        self.llm = llm
        self.cache = InMemoryCache(ttl=3600)
        self.llm_with_cache = LLM.create(llm.model, cache=self.cache)
    
    def ask(self, prompt: str, use_cache: bool = True):
        """ìºì‹œ ì‚¬ìš© ì—¬ë¶€ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •"""
        if use_cache and not self._is_dynamic_content(prompt):
            return self.llm_with_cache.ask(prompt)
        else:
            return self.llm.ask(prompt)
    
    def _is_dynamic_content(self, prompt: str) -> bool:
        """ë™ì  ì»¨í…ì¸  ì—¬ë¶€ íŒë‹¨"""
        dynamic_keywords = ["í˜„ì¬", "ì˜¤ëŠ˜", "ì§€ê¸ˆ", "ì‹¤ì‹œê°„", "ìµœì‹ "]
        return any(keyword in prompt for keyword in dynamic_keywords)

# ì‚¬ìš© ì˜ˆ
smart_llm = SmartCache(LLM.create("gpt-4o-mini"))
reply1 = smart_llm.ask("íŒŒì´ì¬ì´ë€?")  # ìºì‹œë¨
reply2 = smart_llm.ask("í˜„ì¬ ì‹œê°ì€?")  # ìºì‹œ ì•ˆë¨
```


## ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ

### ê¸°ë³¸ í•¨ìˆ˜ ì •ì˜

```python
from pyhub.llm import OpenAILLM
from typing import Dict, Any
import json

# í•¨ìˆ˜ ì •ì˜
def get_weather(location: str, unit: str = "celsius") -> Dict[str, Any]:
    """í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ
    return {
        "location": location,
        "temperature": 25,
        "unit": unit,
        "condition": "ë§‘ìŒ"
    }

def calculate(expression: str) -> float:
    """ìˆ˜í•™ í‘œí˜„ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    # ì•ˆì „í•œ eval ì‚¬ìš©
    import ast
    import operator as op
    
    ops = {
        ast.Add: op.add,
        ast.Sub: op.sub,
        ast.Mult: op.mul,
        ast.Div: op.truediv,
        ast.Pow: op.pow
    }
    
    def eval_expr(expr):
        return eval(expr, {"__builtins__": {}}, {})
    
    try:
        return eval_expr(expression)
    except:
        return "ê³„ì‚°í•  ìˆ˜ ì—†ëŠ” í‘œí˜„ì‹ì…ë‹ˆë‹¤."

# ë„êµ¬ ì •ì˜
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "íŠ¹ì • ìœ„ì¹˜ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "ë„ì‹œ ì´ë¦„ (ì˜ˆ: ì„œìš¸, ë‰´ìš•)"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "ì˜¨ë„ ë‹¨ìœ„"
                    }
                },
                "required": ["location"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "calculate",
            "description": "ìˆ˜í•™ í‘œí˜„ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "ê³„ì‚°í•  ìˆ˜í•™ í‘œí˜„ì‹ (ì˜ˆ: 2+2, 10*5)"
                    }
                },
                "required": ["expression"]
            }
        }
    }
]

# LLMê³¼ í•¨ê»˜ ì‚¬ìš©
llm = OpenAILLM(model="gpt-4o-mini")

# ë„êµ¬ì™€ í•¨ê»˜ ì§ˆë¬¸
response = llm.ask_with_tools(
    "ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ëŠ” ì–´ë•Œ? ê·¸ë¦¬ê³  ì„­ì”¨ 25ë„ëŠ” í™”ì”¨ë¡œ ëª‡ ë„ì•¼?",
    tools=tools
)

# í•¨ìˆ˜ í˜¸ì¶œ ì²˜ë¦¬
if response.tool_calls:
    for tool_call in response.tool_calls:
        function_name = tool_call.function.name
        arguments = json.loads(tool_call.function.arguments)
        
        # ì‹¤ì œ í•¨ìˆ˜ í˜¸ì¶œ
        if function_name == "get_weather":
            result = get_weather(**arguments)
        elif function_name == "calculate":
            result = calculate(**arguments)
        
        print(f"í•¨ìˆ˜ {function_name} í˜¸ì¶œ: {arguments}")
        print(f"ê²°ê³¼: {result}")
```

### í´ë˜ìŠ¤ ê¸°ë°˜ ë„êµ¬

```python
class ToolHandler:
    """ë„êµ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.tools = []
        self.functions = {}
    
    def register(self, func, description: str, parameters: dict):
        """í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë“±ë¡"""
        tool = {
            "type": "function",
            "function": {
                "name": func.__name__,
                "description": description,
                "parameters": parameters
            }
        }
        self.tools.append(tool)
        self.functions[func.__name__] = func
        return func
    
    def execute(self, tool_call):
        """ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰"""
        func_name = tool_call.function.name
        if func_name in self.functions:
            args = json.loads(tool_call.function.arguments)
            return self.functions[func_name](**args)
        else:
            raise ValueError(f"Unknown function: {func_name}")

# ì‚¬ìš© ì˜ˆ
handler = ToolHandler()

@handler.register(
    description="ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤",
    parameters={
        "type": "object",
        "properties": {
            "a": {"type": "number", "description": "ì²« ë²ˆì§¸ ìˆ«ì"},
            "b": {"type": "number", "description": "ë‘ ë²ˆì§¸ ìˆ«ì"}
        },
        "required": ["a", "b"]
    }
)
def add(a: float, b: float) -> float:
    return a + b

@handler.register(
    description="í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤",
    parameters={
        "type": "object",
        "properties": {
            "text": {"type": "string", "description": "ë²ˆì—­í•  í…ìŠ¤íŠ¸"},
            "target_lang": {"type": "string", "description": "ëª©í‘œ ì–¸ì–´ ì½”ë“œ"}
        },
        "required": ["text", "target_lang"]
    }
)
def translate(text: str, target_lang: str) -> str:
    # ì‹¤ì œë¡œëŠ” ë²ˆì—­ API í˜¸ì¶œ
    translations = {
        "ko": {"Hello": "ì•ˆë…•í•˜ì„¸ìš”", "Thank you": "ê°ì‚¬í•©ë‹ˆë‹¤"},
        "ja": {"Hello": "ã“ã‚“ã«ã¡ã¯", "Thank you": "ã‚ã‚ŠãŒã¨ã†"},
        "es": {"Hello": "Hola", "Thank you": "Gracias"}
    }
    return translations.get(target_lang, {}).get(text, text)

# LLMê³¼ í†µí•©
llm = OpenAILLM(model="gpt-4o-mini")
response = llm.ask_with_tools(
    "Helloë¥¼ í•œêµ­ì–´ì™€ ì¼ë³¸ì–´ë¡œ ë²ˆì—­í•´ì£¼ê³ , 5 ë”í•˜ê¸° 3ì€ ì–¼ë§ˆì¸ì§€ ê³„ì‚°í•´ì¤˜",
    tools=handler.tools
)

# ì‘ë‹µ ì²˜ë¦¬
for tool_call in response.tool_calls:
    result = handler.execute(tool_call)
    print(f"{tool_call.function.name}: {result}")
```

### ë¹„ë™ê¸° ë„êµ¬ í˜¸ì¶œ

```python
import aiohttp
import asyncio

class AsyncToolHandler:
    """ë¹„ë™ê¸° ë„êµ¬ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.tools = []
        self.async_functions = {}
    
    def register_async(self, func, description: str, parameters: dict):
        """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë“±ë¡"""
        tool = {
            "type": "function",
            "function": {
                "name": func.__name__,
                "description": description,
                "parameters": parameters
            }
        }
        self.tools.append(tool)
        self.async_functions[func.__name__] = func
        return func
    
    async def execute_async(self, tool_call):
        """ë¹„ë™ê¸° ë„êµ¬ ì‹¤í–‰"""
        func_name = tool_call.function.name
        if func_name in self.async_functions:
            args = json.loads(tool_call.function.arguments)
            return await self.async_functions[func_name](**args)
        else:
            raise ValueError(f"Unknown function: {func_name}")

# ë¹„ë™ê¸° ë„êµ¬ ì •ì˜
async_handler = AsyncToolHandler()

@async_handler.register_async(
    description="ì›¹í˜ì´ì§€ì˜ ì œëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤",
    parameters={
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "ì›¹í˜ì´ì§€ URL"}
        },
        "required": ["url"]
    }
)
async def get_page_title(url: str) -> str:
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            text = await response.text()
            # ê°„ë‹¨í•œ ì œëª© ì¶”ì¶œ
            import re
            match = re.search(r'<title>(.*?)</title>', text, re.IGNORECASE)
            return match.group(1) if match else "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

# ë¹„ë™ê¸° ì‹¤í–‰
async def main():
    llm = OpenAILLM(model="gpt-4o-mini")
    response = await llm.ask_with_tools_async(
        "Python ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì˜ ì œëª©ì„ ì•Œë ¤ì¤˜",
        tools=async_handler.tools
    )
    
    for tool_call in response.tool_calls:
        result = await async_handler.execute_async(tool_call)
        print(f"ê²°ê³¼: {result}")

asyncio.run(main())
```


## í…œí”Œë¦¿ í™œìš©

### Jinja2 í…œí”Œë¦¿

```python
from pyhub.llm import OpenAILLM
from pyhub.llm.templates import PromptTemplate

# í…œí”Œë¦¿ ì •ì˜
template = PromptTemplate("""
ë‹¹ì‹ ì€ {{ role }}ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ìš”ì²­: {{ request }}

ë‹¤ìŒ ì¡°ê±´ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
{% for condition in conditions %}
- {{ condition }}
{% endfor %}
""")

llm = OpenAILLM(model="gpt-4o-mini")

# í…œí”Œë¦¿ ì‚¬ìš©
prompt = template.render(
    role="ì „ë¬¸ ìš”ë¦¬ì‚¬",
    request="íŒŒìŠ¤íƒ€ ë§Œë“œëŠ” ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
    conditions=[
        "ì´ˆë³´ìë„ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆë„ë¡",
        "ì¬ë£ŒëŠ” ë§ˆíŠ¸ì—ì„œ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒìœ¼ë¡œ",
        "30ë¶„ ì´ë‚´ì— ì™„ì„± ê°€ëŠ¥í•œ ë ˆì‹œí”¼"
    ]
)

reply = llm.ask(prompt)
print(reply.text)
```

### Few-shot í…œí”Œë¦¿

```python
few_shot_template = PromptTemplate("""
ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

{% for example in examples %}
ì…ë ¥: {{ example.input }}
ì¶œë ¥: {{ example.output }}

{% endfor %}
ì…ë ¥: {{ input }}
ì¶œë ¥:""")

# ë²ˆì—­ ì˜ˆì‹œ
examples = [
    {"input": "Hello", "output": "ì•ˆë…•í•˜ì„¸ìš”"},
    {"input": "Thank you", "output": "ê°ì‚¬í•©ë‹ˆë‹¤"},
    {"input": "Good morning", "output": "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤"}
]

prompt = few_shot_template.render(
    examples=examples,
    input="How are you?"
)

reply = llm.ask(prompt)
print(reply.text)  # "ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?" ë˜ëŠ” ìœ ì‚¬í•œ ë²ˆì—­
```

### ë™ì  í…œí”Œë¦¿

```python
class DynamicPromptBuilder:
    def __init__(self):
        self.templates = {
            "technical": PromptTemplate("ê¸°ìˆ ì  ê´€ì ì—ì„œ {{ topic }}ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”."),
            "simple": PromptTemplate("5ì‚´ ì•„ì´ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ {{ topic }}ì„ ì„¤ëª…í•˜ì„¸ìš”."),
            "business": PromptTemplate("ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ {{ topic }}ì˜ ê°€ì¹˜ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.")
        }
    
    def build(self, style: str, topic: str) -> str:
        template = self.templates.get(style, self.templates["simple"])
        return template.render(topic=topic)

builder = DynamicPromptBuilder()
llm = OpenAILLM(model="gpt-4o-mini")

# ê°™ì€ ì£¼ì œë¥¼ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ
topic = "ë¸”ë¡ì²´ì¸ ê¸°ìˆ "
for style in ["technical", "simple", "business"]:
    prompt = builder.build(style, topic)
    reply = llm.ask(prompt)
    print(f"\n[{style.upper()}]\n{reply.text[:200]}...")
```


## í”„ë¡¬í”„íŠ¸ í—ˆë¸Œ (Hub)

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/advanced/15_hub_prompts.py)

pyhub-llmì€ LangChain Hubì™€ í˜¸í™˜ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤. ì¸ê¸° ìˆëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì‰½ê²Œ ê°€ì ¸ì™€ ì‚¬ìš©í•˜ê³ , ìì‹ ë§Œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì €ì¥í•˜ê³  ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë‚´ì¥ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©

```python
from pyhub.llm import hub, OpenAILLM

# RAG (Retrieval Augmented Generation) í”„ë¡¬í”„íŠ¸
rag_prompt = hub.pull("rlm/rag-prompt")
print(f"ì…ë ¥ ë³€ìˆ˜: {rag_prompt.input_variables}")  # ['context', 'question']

# í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
formatted = rag_prompt.format(
    context="ì—í íƒ‘ì€ 1889ë…„ íŒŒë¦¬ ë§Œêµ­ë°•ëŒíšŒë¥¼ ìœ„í•´ ê±´ì„¤ëœ ì² ì œ íƒ‘ì…ë‹ˆë‹¤. ë†’ì´ëŠ” 330ë¯¸í„°ì…ë‹ˆë‹¤.",
    question="ì—í íƒ‘ì˜ ë†’ì´ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
)

# LLMê³¼ í•¨ê»˜ ì‚¬ìš©
llm = OpenAILLM(model="gpt-4o-mini")
answer = llm.ask(formatted)
print(answer.text)  # "ì—í íƒ‘ì˜ ë†’ì´ëŠ” 330ë¯¸í„°ì…ë‹ˆë‹¤."
```

### ReAct ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸

```python
# ReAct (Reasoning + Acting) í”„ë¡¬í”„íŠ¸
react_prompt = hub.pull("hwchase17/react")

# ë„êµ¬ ì •ë³´ì™€ í•¨ê»˜ í¬ë§·íŒ…
formatted = react_prompt.format(
    tools="Calculator: ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\nSearch: ì¸í„°ë„·ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    tool_names="Calculator, Search",
    input="ì—í íƒ‘ì´ ì™„ê³µëœ ì—°ë„ì˜ ì œê³±ê·¼ì€ ì–¼ë§ˆì¸ê°€ìš”?"
)

print(formatted[:200] + "...")  # í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°
```

### ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì €ì¥

```python
from pyhub.llm.templates import PromptTemplate

# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„±
custom_prompt = PromptTemplate(
    template="""ë‹¹ì‹ ì€ {language} ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
ë‹¤ìŒ ì½”ë“œë¥¼ ê²€í† í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”:

```{language}
{code}
```

ê²€í†  ê¸°ì¤€:
- ê°€ë…ì„±
- ì„±ëŠ¥
- ë³´ì•ˆ
- ëª¨ë²” ì‚¬ë¡€""",
    input_variables=["language", "code"],
    metadata={
        "description": "ì½”ë“œ ë¦¬ë·° í”„ë¡¬í”„íŠ¸",
        "author": "myteam",
        "tags": ["code-review", "programming"],
        "version": "1.0.0"
    }
)

# ë¡œì»¬ì— ì €ì¥
hub.push("myteam/code-reviewer", custom_prompt)

# ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‚¬ìš©
saved_prompt = hub.pull("myteam/code-reviewer")
```

### Partial í”„ë¡¬í”„íŠ¸

```python
# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
base_prompt = hub.pull("rlm/rag-prompt")

# ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¯¸ë¦¬ ì±„ìš´ partial í”„ë¡¬í”„íŠ¸ ìƒì„±
python_qa_prompt = base_prompt.partial(
    context="""Pythonì€ 1991ë…„ Guido van Rossumì´ ê°œë°œí•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.
    ë™ì  íƒ€ì´í•‘, ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬, ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° íŒ¨ëŸ¬ë‹¤ì„ì„ ì§€ì›í•©ë‹ˆë‹¤."""
)

# ì´ì œ ì§ˆë¬¸ë§Œ ì œê³µí•˜ë©´ ë¨
questions = [
    "Pythonì€ ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?",
    "Pythonì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "Pythonì€ ì–¸ì œ ë§Œë“¤ì–´ì¡Œë‚˜ìš”?"
]

llm = OpenAILLM(model="gpt-4o-mini")
for question in questions:
    formatted = python_qa_prompt.format(question=question)
    answer = llm.ask(formatted)
    print(f"Q: {question}")
    print(f"A: {answer.text}\n")
```

### í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ (í–¥í›„ ì§€ì› ì˜ˆì •)

```python
# í˜„ì¬ëŠ” ìµœì‹  ë²„ì „ë§Œ ì§€ì›
prompt = hub.pull("rlm/rag-prompt")

# í–¥í›„ ë²„ì „ ì§€ì • ì§€ì› ì˜ˆì •
# prompt = hub.pull("rlm/rag-prompt", version="1.0.0")
# prompt = hub.pull("rlm/rag-prompt:v1.0.0")
```

### ì‚¬ìš© ê°€ëŠ¥í•œ ë‚´ì¥ í”„ë¡¬í”„íŠ¸

| í”„ë¡¬í”„íŠ¸ ì´ë¦„ | ì„¤ëª… | ì…ë ¥ ë³€ìˆ˜ |
|-------------|------|----------|
| `rlm/rag-prompt` | RAG ì§ˆë¬¸-ë‹µë³€ | `context`, `question` |
| `hwchase17/react` | ReAct ì—ì´ì „íŠ¸ | `tools`, `tool_names`, `input` |
| `hwchase17/openai-functions-agent` | OpenAI í•¨ìˆ˜ í˜¸ì¶œ | `tools`, `history`, `input` |
| `hwchase17/structured-chat-agent` | JSON ì¶œë ¥ ì—ì´ì „íŠ¸ | `tools`, `tool_names`, `history`, `input`, `agent_scratchpad` |

### í”„ë¡¬í”„íŠ¸ ëª©ë¡ í™•ì¸

```python
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  í”„ë¡¬í”„íŠ¸ í™•ì¸
all_prompts = hub.list_prompts()
for prompt_name in all_prompts:
    print(f"- {prompt_name}")
```

### Jinja2 í…œí”Œë¦¿ í˜•ì‹

```python
# Jinja2 í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸
jinja_prompt = PromptTemplate(
    template="""
{% for item in items %}
{{ loop.index }}. {{ item.name }}: {{ item.description }}
{% endfor %}

ìœ„ í•­ëª© ì¤‘ì—ì„œ {{ criteria }}ì— ê°€ì¥ ì í•©í•œ ê²ƒì„ ì„ íƒí•˜ì„¸ìš”.
""",
    input_variables=["items", "criteria"],
    template_format="jinja2"
)

items = [
    {"name": "Python", "description": "ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ì–¸ì–´"},
    {"name": "Java", "description": "ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©"},
    {"name": "JavaScript", "description": "ì›¹ ê°œë°œì˜ í•„ìˆ˜ ì–¸ì–´"}
]

formatted = jinja_prompt.format(
    items=items,
    criteria="ì´ˆë³´ìê°€ ë°°ìš°ê¸° ì‰¬ìš´ ì–¸ì–´"
)
```


## History Backup

ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì™¸ë¶€ ì €ì¥ì†Œì— ë°±ì—…í•˜ê³  ë³µì›í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ê¸°ë°˜ íˆìŠ¤í† ë¦¬ì™€ ë³„ë„ë¡œ ì˜êµ¬ ì €ì¥ì†Œì— ëŒ€í™” ë‚´ì—­ì„ ë³´ê´€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/history_backup_example.py)

### ê¸°ë³¸ ì‚¬ìš©ë²• (InMemoryHistoryBackup)

```python
from pyhub.llm import OpenAILLM
from pyhub.llm.history import InMemoryHistoryBackup

# ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°±ì—… (í…ŒìŠ¤íŠ¸ìš©)
backup = InMemoryHistoryBackup(
    user_id="user123",
    session_id="session456"
)

# ë°±ì—…ì´ í™œì„±í™”ëœ LLM ìƒì„±
llm = LLM.create("gpt-4o-mini", history_backup=backup)

# ëŒ€í™” ì§„í–‰ (ìë™ìœ¼ë¡œ ë°±ì—…ë¨)
llm.ask("Pythonì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
llm.ask("ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”")

# ë°±ì—…ëœ íˆìŠ¤í† ë¦¬ í™•ì¸
messages = backup.load_history()
for msg in messages:
    print(f"{msg.role}: {msg.content[:50]}...")

# ì‚¬ìš©ëŸ‰ í†µê³„
usage = backup.get_usage_summary()
print(f"ì´ ì…ë ¥ í† í°: {usage.input}")
print(f"ì´ ì¶œë ¥ í† í°: {usage.output}")
```

### SQLAlchemy ë°±ì—… (ì˜êµ¬ ì €ì¥)

```python
from pyhub.llm import OpenAILLM
from pyhub.llm.history import SQLAlchemyHistoryBackup
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
engine = create_engine("sqlite:///chat_history.db")
Session = sessionmaker(bind=engine)
session = Session()

# SQLAlchemy ë°±ì—… ìƒì„±
backup = SQLAlchemyHistoryBackup(
    session=session,
    user_id="user123",
    session_id="session456"
)

# í…Œì´ë¸” ìë™ ìƒì„±
from pyhub.llm.history.sqlalchemy_backup import Base
Base.metadata.create_all(engine)

# ë°±ì—…ì´ í™œì„±í™”ëœ LLM ìƒì„±
llm = LLM.create("gpt-4o-mini", history_backup=backup)

# ëŒ€í™” ì§„í–‰
llm.ask("ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ì›ì¹™ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”")
llm.ask("ì •ê·œí™”ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”")

# ì„¸ì…˜ ì»¤ë°‹ (ì˜êµ¬ ì €ì¥)
session.commit()
```

### ì´ì „ ëŒ€í™” ë³µì›

```python
# ìƒˆë¡œìš´ ì„¸ì…˜ì—ì„œ ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°
new_session = Session()
backup = SQLAlchemyHistoryBackup(
    session=new_session,
    user_id="user123",
    session_id="session456"
)

# ì´ì „ ëŒ€í™”ê°€ ìë™ìœ¼ë¡œ ë³µì›ëœ LLM
llm = LLM.create("gpt-4o-mini", history_backup=backup)

# ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•œ ì±„ ê³„ì† ëŒ€í™”
llm.ask("ì•ì„œ ì„¤ëª…í•œ ì •ê·œí™”ì˜ ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
```

### ì—¬ëŸ¬ ì„¸ì…˜ ê´€ë¦¬

```python
# ì‚¬ìš©ìë³„ ì—¬ëŸ¬ ì„¸ì…˜ ê´€ë¦¬
user_id = "user123"

# ì„¸ì…˜ 1: í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸
session1_backup = SQLAlchemyHistoryBackup(
    session=session,
    user_id=user_id,
    session_id="programming_session"
)
llm1 = LLM.create("gpt-4o-mini", history_backup=session1_backup)
llm1.ask("Pythonê³¼ JavaScriptì˜ ì°¨ì´ì ì€?")

# ì„¸ì…˜ 2: ìˆ˜í•™ ì§ˆë¬¸
session2_backup = SQLAlchemyHistoryBackup(
    session=session,
    user_id=user_id,
    session_id="math_session"
)
llm2 = LLM.create("gpt-4o-mini", history_backup=session2_backup)
llm2.ask("ë¯¸ì ë¶„í•™ì˜ ê¸°ë³¸ ì •ë¦¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”")

# ê° ì„¸ì…˜ì€ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬ë¨
```

### Tool ì‚¬ìš© ë‚´ì—­ ìë™ ì €ì¥

```python
# ë„êµ¬ í˜¸ì¶œ ë‚´ì—­ë„ ìë™ìœ¼ë¡œ ë°±ì—…ë¨
def get_weather(city: str) -> str:
    """ë„ì‹œì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return f"{city}ì˜ ë‚ ì”¨ëŠ” ë§‘ìŒ, 25Â°Cì…ë‹ˆë‹¤."

def get_time(timezone: str = "UTC") -> str:
    """í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

llm = LLM.create("gpt-4o-mini", history_backup=backup)
reply = llm.ask(
    "ì„œìš¸ì˜ ë‚ ì”¨ì™€ í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ì„¸ìš”",
    tools=[get_weather, get_time]
)

# ë°±ì—…ëœ ë©”ì‹œì§€ í™•ì¸
messages = backup.load_history()
assistant_msg = messages[-1]  # ë§ˆì§€ë§‰ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€

# tool_interactions í•„ë“œì— ë„êµ¬ ì‚¬ìš© ë‚´ì—­ì´ ì €ì¥ë¨
if assistant_msg.tool_interactions:
    for interaction in assistant_msg.tool_interactions:
        print(f"ë„êµ¬: {interaction['tool']}")
        print(f"ì¸ì: {interaction['arguments']}")
        print(f"ê²°ê³¼: {interaction.get('result', 'N/A')}")
```

### ì‚¬ìš©ì ì •ì˜ ë°±ì—… êµ¬í˜„

```python
from abc import ABC, abstractmethod
from pyhub.llm.history import HistoryBackup
from pyhub.llm.types import Message, Usage

class MongoDBHistoryBackup(HistoryBackup):
    """MongoDBë¥¼ ì‚¬ìš©í•œ íˆìŠ¤í† ë¦¬ ë°±ì—… ì˜ˆì œ"""
    
    def __init__(self, collection, user_id: str, session_id: str):
        self.collection = collection
        self.user_id = user_id
        self.session_id = session_id
    
    def save_exchange(
        self,
        user_msg: Message,
        assistant_msg: Message,
        usage: Optional[Usage] = None,
        model: Optional[str] = None
    ) -> None:
        """ëŒ€í™” êµí™˜ì„ MongoDBì— ì €ì¥"""
        doc = {
            "user_id": self.user_id,
            "session_id": self.session_id,
            "timestamp": datetime.utcnow(),
            "user_message": {
                "content": user_msg.content,
                "files": user_msg.files
            },
            "assistant_message": {
                "content": assistant_msg.content,
                "tool_interactions": assistant_msg.tool_interactions
            },
            "usage": {
                "input": usage.input if usage else 0,
                "output": usage.output if usage else 0
            },
            "model": model
        }
        self.collection.insert_one(doc)
    
    def load_history(self, limit: Optional[int] = None) -> list[Message]:
        """MongoDBì—ì„œ íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        query = {
            "user_id": self.user_id,
            "session_id": self.session_id
        }
        
        cursor = self.collection.find(query).sort("timestamp", 1)
        if limit:
            cursor = cursor.limit(limit // 2)  # ê° êµí™˜ì€ 2ê°œ ë©”ì‹œì§€
        
        messages = []
        for doc in cursor:
            # ì‚¬ìš©ì ë©”ì‹œì§€
            messages.append(Message(
                role="user",
                content=doc["user_message"]["content"],
                files=doc["user_message"].get("files")
            ))
            
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€
            messages.append(Message(
                role="assistant",
                content=doc["assistant_message"]["content"],
                tool_interactions=doc["assistant_message"].get("tool_interactions")
            ))
        
        return messages
    
    def get_usage_summary(self) -> Usage:
        """ì´ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        pipeline = [
            {"$match": {"user_id": self.user_id, "session_id": self.session_id}},
            {"$group": {
                "_id": None,
                "total_input": {"$sum": "$usage.input"},
                "total_output": {"$sum": "$usage.output"}
            }}
        ]
        
        result = list(self.collection.aggregate(pipeline))
        if result:
            return Usage(
                input=result[0]["total_input"],
                output=result[0]["total_output"]
            )
        return Usage(input=0, output=0)
    
    def clear(self) -> int:
        """íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
        result = self.collection.delete_many({
            "user_id": self.user_id,
            "session_id": self.session_id
        })
        return result.deleted_count * 2  # ê° ë¬¸ì„œëŠ” 2ê°œ ë©”ì‹œì§€
```

### ë°±ì—… ì‹¤íŒ¨ ì²˜ë¦¬

```python
# ë°±ì—… ì‹¤íŒ¨ ì‹œì—ë„ LLMì€ ì •ìƒ ë™ì‘
import logging

class UnreliableBackup(HistoryBackup):
    """ê°„í—ì ìœ¼ë¡œ ì‹¤íŒ¨í•˜ëŠ” ë°±ì—… (ì˜ˆì œ)"""
    
    def save_exchange(self, user_msg, assistant_msg, usage=None, model=None):
        import random
        if random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ì‹¤íŒ¨
            raise Exception("Backup service temporarily unavailable")
        # ì‹¤ì œ ì €ì¥ ë¡œì§...

# ë°±ì—… ì‹¤íŒ¨ëŠ” ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
backup = UnreliableBackup()
llm = LLM.create("gpt-4o-mini", history_backup=backup)

# ë°±ì—…ì´ ì‹¤íŒ¨í•´ë„ ëŒ€í™”ëŠ” ê³„ì†ë¨
reply = llm.ask("ë°±ì—…ì´ ì‹¤íŒ¨í•´ë„ ê´œì°®ë‚˜ìš”?")
# ê²½ê³  ë¡œê·¸ë§Œ ì¶œë ¥ë˜ê³  ì •ìƒ ë™ì‘
```

### ì£¼ìš” ë©”ì„œë“œ ì„¤ëª…

- `save_exchange()`: ì‚¬ìš©ì ë©”ì‹œì§€ì™€ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì„ í•œ ìŒìœ¼ë¡œ ì €ì¥
- `load_history(limit)`: ì €ì¥ëœ íˆìŠ¤í† ë¦¬ë¥¼ Message ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
- `get_usage_summary()`: ì´ í† í° ì‚¬ìš©ëŸ‰ í†µê³„ ë°˜í™˜
- `clear()`: í•´ë‹¹ ì„¸ì…˜ì˜ ëª¨ë“  íˆìŠ¤í† ë¦¬ ì‚­ì œ

> ğŸ’¡ **íŒ**: 
> - ë°±ì—…ì€ ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ì™€ ë³„ê°œë¡œ ë™ì‘í•˜ë©°, ì£¼ë¡œ ì˜êµ¬ ì €ì¥ ìš©ë„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤
> - Tool ì‚¬ìš© ë‚´ì—­ì€ `tool_interactions` í•„ë“œì— ìë™ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤
> - ë°±ì—… ì‹¤íŒ¨ ì‹œì—ë„ LLMì€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ë©°, ê²½ê³  ë¡œê·¸ë§Œ ì¶œë ¥ë©ë‹ˆë‹¤


## ë‹¤ìŒ ë‹¨ê³„

ì´ì œ pyhub-llmì˜ ì¤‘ê¸‰ ê¸°ëŠ¥ë“¤ì„ ìµíˆì…¨ìŠµë‹ˆë‹¤\! ë” ê¹Šì´ ìˆëŠ” í•™ìŠµì„ ìœ„í•´ ë‹¤ìŒì„ ì¶”ì²œí•©ë‹ˆë‹¤:

### ê³ ê¸‰ ê°€ì´ë“œë¡œ ì´ë™
- **MCP (Model Context Protocol) í†µí•©**: ì™¸ë¶€ ë„êµ¬ì™€ ì„œë¹„ìŠ¤ë¥¼ LLMì— ì—°ê²°
- **ì›¹ í”„ë ˆì„ì›Œí¬ í†µí•©**: FastAPI, Djangoì™€ì˜ í†µí•©
- **ì²´ì´ë‹**: ì—¬ëŸ¬ LLM í˜¸ì¶œì„ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±
- **ì—ëŸ¬ ì²˜ë¦¬**: ê°•ë ¥í•œ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ì „ëµ
- **ì‹¤ìš©ì ì¸ ì˜ˆì œ**: ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ í™œìš© ì‚¬ë¡€

### ì¶”ê°€ í•™ìŠµ ìë£Œ
- [ì „ì²´ ê°€ì´ë“œ ë³´ê¸°](CHEATSHEET.md)
- [API ë¬¸ì„œ](https://pyhub-llm.readthedocs.io)
- [ì˜ˆì œ ì½”ë“œ](examples/)
- [GitHub ì €ì¥ì†Œ](https://github.com/pyhub-llm/pyhub-llm)

### ì»¤ë®¤ë‹ˆí‹°
- ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ GitHub Issuesì— ë‚¨ê²¨ì£¼ì„¸ìš”
- ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤\! Contributing ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”

Happy coding with pyhub-llm\! ğŸš€
EOF < /dev/null