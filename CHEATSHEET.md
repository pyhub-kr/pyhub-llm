# pyhub-llm CHEATSHEET

## ëª©ì°¨
- [ì„¤ì¹˜](#ì„¤ì¹˜)
- [ê¸°ë³¸ ì‚¬ìš©ë²•](#ê¸°ë³¸-ì‚¬ìš©ë²•)
- [ìŠ¤íŠ¸ë¦¬ë°](#ìŠ¤íŠ¸ë¦¬ë°)
- [êµ¬ì¡°í™”ëœ ì¶œë ¥](#êµ¬ì¡°í™”ëœ-ì¶œë ¥)
- [ë¶„ë¥˜ ë° ì„ íƒ](#ë¶„ë¥˜-ë°-ì„ íƒ)
- [ë¹„ë™ê¸° ì²˜ë¦¬](#ë¹„ë™ê¸°-ì²˜ë¦¬)
- [ìºì‹±](#ìºì‹±)
- [ëŒ€í™” ê´€ë¦¬](#ëŒ€í™”-ê´€ë¦¬)
- [íŒŒì¼ ì²˜ë¦¬](#íŒŒì¼-ì²˜ë¦¬)
- [ì„ë² ë”©](#ì„ë² ë”©)
- [í…œí”Œë¦¿ í™œìš©](#í…œí”Œë¦¿-í™œìš©)
- [History Backup](#history-backup)
- [MCP í†µí•©](#mcp-í†µí•©)
- [ì›¹ í”„ë ˆì„ì›Œí¬ í†µí•©](#ì›¹-í”„ë ˆì„ì›Œí¬-í†µí•©)
- [ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ](#ë„êµ¬í•¨ìˆ˜-í˜¸ì¶œ)
- [ì²´ì´ë‹](#ì²´ì´ë‹)
- [ì—ëŸ¬ ì²˜ë¦¬](#ì—ëŸ¬-ì²˜ë¦¬)
- [ì‹¤ìš©ì ì¸ ì˜ˆì œ](#ì‹¤ìš©ì ì¸-ì˜ˆì œ)

## ì„¤ì¹˜

```bash
# ì „ì²´ ì„¤ì¹˜ (ëª¨ë“  í”„ë¡œë°”ì´ë”)
pip install "pyhub-llm[all]"

# íŠ¹ì • í”„ë¡œë°”ì´ë”ë§Œ ì„¤ì¹˜
pip install "pyhub-llm[openai]"      # OpenAIë§Œ
pip install "pyhub-llm[anthropic]"   # Anthropicë§Œ
pip install "pyhub-llm[google]"      # Googleë§Œ
pip install "pyhub-llm[ollama]"      # Ollamaë§Œ

# MCP ì§€ì› í¬í•¨
pip install "pyhub-llm[all,mcp]"
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# Linux/macOS
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GOOGLE_API_KEY="..."
export UPSTAGE_API_KEY="..."

# Windows PowerShell
$env:OPENAI_API_KEY="sk-..."
$env:ANTHROPIC_API_KEY="sk-ant-..."
```

### OpenAI

```python
from pyhub.llm import LLM, OpenAILLM

# íŒ©í† ë¦¬ íŒ¨í„´ ì‚¬ìš© (ê¶Œì¥)
llm = LLM.create("gpt-4o-mini")
reply = llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")
print(reply.text)

# ì§ì ‘ ìƒì„±
llm = OpenAILLM(model="gpt-4o-mini", temperature=0.7)
reply = llm.ask("íŒŒì´ì¬ì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”")
print(reply.text)
```

### Anthropic

```python
from pyhub.llm import LLM, AnthropicLLM

# Claude ì‚¬ìš©
llm = LLM.create("claude-3-haiku-20240307")
reply = llm.ask("ì–‘ì ì»´í“¨í„°ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”")
print(reply.text)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
llm = AnthropicLLM(
    model="claude-3-5-sonnet-20241022",
    system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
)
```

### Google

```python
from pyhub.llm import LLM, GoogleLLM

# Gemini ì‚¬ìš©
llm = LLM.create("gemini-1.5-flash")
reply = llm.ask("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”. ì‚°ì±…í•˜ê¸° ì¢‹ì€ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.")
print(reply.text)

# ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
llm = GoogleLLM(model="gemini-1.5-pro", max_tokens=8192)
```

### Ollama (ë¡œì»¬)

```python
from pyhub.llm import LLM, OllamaLLM

# OllamaëŠ” API í‚¤ ë¶ˆí•„ìš”
llm = LLM.create("mistral")
reply = llm.ask("ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” LLMì˜ ì¥ì ì€?")
print(reply.text)

# ì»¤ìŠ¤í…€ ì„œë²„ ì£¼ì†Œ
llm = OllamaLLM(
    model="llama2",
    base_url="http://192.168.1.100:11434"
)
```

### Upstage

```python
from pyhub.llm import UpstageLLM

llm = UpstageLLM(model="solar-mini")
reply = llm.ask("í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ì˜ ì–´ë ¤ìš´ ì ì€?")
print(reply.text)
```

## ìŠ¤íŠ¸ë¦¬ë°

ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ ë°›ì•„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
from pyhub.llm import LLM

llm = LLM.create("gpt-4o-mini")

# ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë°
for chunk in llm.ask("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)
print()

# ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì²˜ë¦¬
def process_stream(llm, prompt):
    full_text = ""
    for i, chunk in enumerate(llm.ask(prompt, stream=True)):
        full_text += chunk.text
        # íŠ¹ì • ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
        if "ì¢…ë£Œ" in chunk.text:
            break
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        if i % 10 == 0:
            print(".", end="", flush=True)
    return full_text
```

## êµ¬ì¡°í™”ëœ ì¶œë ¥

### Pydantic ìŠ¤í‚¤ë§ˆ ì‚¬ìš©

```python
from pydantic import BaseModel, Field
from typing import List
from pyhub.llm import LLM

class BookInfo(BaseModel):
    title: str = Field(description="ì±… ì œëª©")
    author: str = Field(description="ì €ì")
    year: int = Field(description="ì¶œíŒ ì—°ë„")
    genres: List[str] = Field(description="ì¥ë¥´ ëª©ë¡")
    summary: str = Field(description="ê°„ë‹¨í•œ ì¤„ê±°ë¦¬")

llm = LLM.create("gpt-4o-mini")
reply = llm.ask(
    "í•´ë¦¬í¬í„°ì™€ ë§ˆë²•ì‚¬ì˜ ëŒì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
    schema=BookInfo
)

book: BookInfo = reply.structured_data
print(f"ì œëª©: {book.title}")
print(f"ì €ì: {book.author}")
print(f"ì¥ë¥´: {', '.join(book.genres)}")
```

### ë³µì¡í•œ êµ¬ì¡°

```python
class Company(BaseModel):
    name: str
    founded: int
    headquarters: str

class ProductAnalysis(BaseModel):
    product_name: str
    manufacturer: Company
    pros: List[str]
    cons: List[str]
    rating: float = Field(ge=0, le=5)
    recommendation: bool

llm = LLM.create("gpt-4o-mini")
reply = llm.ask(
    "iPhone 15 Proì— ëŒ€í•œ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”",
    schema=ProductAnalysis
)

analysis: ProductAnalysis = reply.structured_data
print(f"ì œì¡°ì‚¬: {analysis.manufacturer.name}")
print(f"í‰ì : {analysis.rating}/5.0")
```

### ë‹¤êµ­ì–´ ì‘ë‹µ

```python
class Translation(BaseModel):
    korean: str
    english: str
    japanese: str
    chinese: str

llm = LLM.create("gpt-4o-mini", system_prompt="ë‹¤êµ­ì–´ ë²ˆì—­ ì „ë¬¸ê°€")
reply = llm.ask("'ì¸ê³µì§€ëŠ¥'ì„ 4ê°œ ì–¸ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”", schema=Translation)

trans: Translation = reply.structured_data
print(f"í•œêµ­ì–´: {trans.korean}")
print(f"ì˜ì–´: {trans.english}")
print(f"ì¼ë³¸ì–´: {trans.japanese}")
print(f"ì¤‘êµ­ì–´: {trans.chinese}")
```

## ë¶„ë¥˜ ë° ì„ íƒ

### ê°ì • ë¶„ì„

```python
from pyhub.llm import LLM

llm = LLM.create("gpt-4o-mini")

# ë‹¨ì¼ ì„ íƒ
emotions = ["ê¸°ì¨", "ìŠ¬í””", "ë¶„ë…¸", "ê³µí¬", "ë†€ëŒ", "í˜ì˜¤"]
reply = llm.ask("ì˜¤ëŠ˜ ìŠ¹ì§„í–ˆì–´ìš”! ì¶•í•˜ íŒŒí‹°ë„ í–ˆë‹µë‹ˆë‹¤.", choices=emotions)
print(f"ê°ì •: {reply.choice}")  # "ê¸°ì¨"
print(f"ì¸ë±ìŠ¤: {reply.choice_index}")  # 0

# ì—¬ëŸ¬ ë¬¸ì¥ ì¼ê´„ ì²˜ë¦¬
texts = [
    "í”„ë¡œì íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
    "ë³µê¶Œì— ë‹¹ì²¨ëì–´ìš”!",
    "ë˜ ì•¼ê·¼ì´ë„¤ìš”..."
]

for text in texts:
    reply = llm.ask(text, choices=emotions)
    print(f"{text} â†’ {reply.choice}")
```

### ì˜ë„ ë¶„ë¥˜

```python
class IntentClassifier:
    def __init__(self):
        self.llm = LLM.create("gpt-4o-mini")
        self.intents = [
            "ì§ˆë¬¸",
            "ìš”ì²­",
            "ë¶ˆë§Œ",
            "ì¹­ì°¬",
            "ì •ë³´ì œê³µ",
            "ê¸°íƒ€"
        ]
    
    def classify(self, text: str) -> str:
        reply = self.llm.ask(text, choices=self.intents)
        return reply.choice

classifier = IntentClassifier()
print(classifier.classify("ì´ ì œí’ˆ í™˜ë¶ˆ ê°€ëŠ¥í•œê°€ìš”?"))  # "ì§ˆë¬¸"
print(classifier.classify("ì •ë§ ìµœê³ ì˜ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤!"))  # "ì¹­ì°¬"
```

### ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜

```python
from pydantic import BaseModel
from typing import List

class TopicLabels(BaseModel):
    topics: List[str] = Field(description="í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì£¼ì œ")

llm = LLM.create("gpt-4o-mini", system_prompt="í…ìŠ¤íŠ¸ì˜ ì£¼ì œë¥¼ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€")

available_topics = ["ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ë¬¸í™”", "ìŠ¤í¬ì¸ ", "IT", "ê³¼í•™", "ê±´ê°•"]

prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì£¼ì œë¥¼ ì„ íƒí•˜ì„¸ìš”.
ê°€ëŠ¥í•œ ì£¼ì œ: {', '.join(available_topics)}

í…ìŠ¤íŠ¸: 'AI ê¸°ìˆ ì´ ì˜ë£Œ ë¶„ì•¼ì— í˜ëª…ì„ ì¼ìœ¼í‚¤ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì•” ì§„ë‹¨ì˜ ì •í™•ë„ê°€ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.'
"""

reply = llm.ask(prompt, schema=TopicLabels)
print(f"ë¶„ë¥˜ëœ ì£¼ì œ: {', '.join(reply.structured_data.topics)}")  # "IT, ê³¼í•™, ê±´ê°•"
```

## ë¹„ë™ê¸° ì²˜ë¦¬

### ê¸°ë³¸ ë¹„ë™ê¸° ì‚¬ìš©

```python
import asyncio
from pyhub.llm import LLM

async def main():
    llm = LLM.create("gpt-4o-mini")
    
    # ë¹„ë™ê¸° ìš”ì²­
    reply = await llm.ask_async("ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì˜ ì¥ì ì€?")
    print(reply.text)
    
    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
    async for chunk in llm.ask_async("ê¸´ ì„¤ëª…ì„ í•´ì£¼ì„¸ìš”", stream=True):
        print(chunk.text, end="", flush=True)

# ì‹¤í–‰
asyncio.run(main())
```

### ë™ì‹œ ìš”ì²­ ì²˜ë¦¬

```python
async def process_multiple_queries():
    llm = LLM.create("gpt-4o-mini")
    
    queries = [
        "Pythonì˜ ì¥ì ì€?",
        "JavaScriptì˜ ì¥ì ì€?",
        "Rustì˜ ì¥ì ì€?"
    ]
    
    # ëª¨ë“  ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬
    tasks = [llm.ask_async(q) for q in queries]
    replies = await asyncio.gather(*tasks)
    
    for query, reply in zip(queries, replies):
        print(f"\nQ: {query}")
        print(f"A: {reply.text[:100]}...")

asyncio.run(process_multiple_queries())
```

### MCPì™€ í•¨ê»˜ ë¹„ë™ê¸° ì‚¬ìš©

```python
from pyhub.llm import LLM

async def main():
    # ê°„í¸í•œ ë¬¸ìì—´ ì„¤ì •ë¡œ MCP ì„œë²„ì™€ í•¨ê»˜ LLM ìƒì„±
    llm = await LLM.create_async(
        "gpt-4o-mini",
        mcp_servers="python calculator.py"  # ë¬¸ìì—´ë¡œ ê°„í¸ ì„¤ì •
    )
    
    # ë˜ëŠ” ë” ìƒì„¸í•œ ì„¤ì •
    # from pyhub.llm.mcp import McpConfig
    # llm = await LLM.create_async(
    #     "gpt-4o-mini",
    #     mcp_servers=McpConfig(
    #         cmd="calculator-server",
    #         name="my-calc"
    #     )
    # )
    
    # MCP ë„êµ¬ ì‚¬ìš©
    reply = await llm.ask_async("25 ê³±í•˜ê¸° 17ì€?")
    print(reply.text)
    
    # ì •ë¦¬
    await llm.close_mcp()

asyncio.run(main())
```

## ìºì‹±

### ì¸ë©”ëª¨ë¦¬ ìºì‹±

```python
from pyhub.llm import LLM
from pyhub.llm.cache import InMemoryCache

# ìºì‹œ ì„¤ì •
cache = InMemoryCache(ttl=3600)  # 1ì‹œê°„ TTL
llm = LLM.create("gpt-4o-mini", cache=cache)

# ì²« ë²ˆì§¸ ìš”ì²­ (API í˜¸ì¶œ)
reply1 = llm.ask("íŒŒì´ì¬ì˜ ì—­ì‚¬ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”")
print("ì²« ë²ˆì§¸ ìš”ì²­ ì™„ë£Œ")

# ë‘ ë²ˆì§¸ ìš”ì²­ (ìºì‹œì—ì„œ ë°˜í™˜)
reply2 = llm.ask("íŒŒì´ì¬ì˜ ì—­ì‚¬ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”")
print("ìºì‹œëœ ì‘ë‹µ:", reply1.text == reply2.text)  # True
```

### íŒŒì¼ ê¸°ë°˜ ìºì‹±

```python
from pyhub.llm.cache import FileCache
from pathlib import Path

# íŒŒì¼ ìºì‹œ ì„¤ì •
cache_dir = Path("./llm_cache")
cache = FileCache(cache_dir=cache_dir, ttl=86400)  # 24ì‹œê°„ TTL

llm = LLM.create("gpt-4o-mini", cache=cache)

# ìºì‹œ í†µê³„
print(f"ìºì‹œ íˆíŠ¸ìœ¨: {cache.hit_rate:.2%}")
print(f"ìºì‹œ í¬ê¸°: {cache.size_bytes / 1024 / 1024:.2f} MB")

# ìºì‹œ ì •ë¦¬
cache.clear_expired()  # ë§Œë£Œëœ í•­ëª© ì‚­ì œ
# cache.clear()  # ì „ì²´ ìºì‹œ ì‚­ì œ
```

### ì¡°ê±´ë¶€ ìºì‹±

```python
class SmartCache:
    def __init__(self, llm):
        self.llm = llm
        self.cache = InMemoryCache(ttl=3600)
        self.llm_with_cache = LLM.create(llm.model, cache=self.cache)
    
    def ask(self, prompt: str, use_cache: bool = True):
        """ìºì‹œ ì‚¬ìš© ì—¬ë¶€ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •"""
        if use_cache and not self._is_dynamic_content(prompt):
            return self.llm_with_cache.ask(prompt)
        else:
            return self.llm.ask(prompt)
    
    def _is_dynamic_content(self, prompt: str) -> bool:
        """ë™ì  ì»¨í…ì¸  ì—¬ë¶€ íŒë‹¨"""
        dynamic_keywords = ["í˜„ì¬", "ì˜¤ëŠ˜", "ì§€ê¸ˆ", "ì‹¤ì‹œê°„", "ìµœì‹ "]
        return any(keyword in prompt for keyword in dynamic_keywords)

# ì‚¬ìš© ì˜ˆ
smart_llm = SmartCache(LLM.create("gpt-4o-mini"))
reply1 = smart_llm.ask("íŒŒì´ì¬ì´ë€?")  # ìºì‹œë¨
reply2 = smart_llm.ask("í˜„ì¬ ì‹œê°ì€?")  # ìºì‹œ ì•ˆë¨
```

## ëŒ€í™” ê´€ë¦¬

### ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€

```python
from pyhub.llm import LLM
from pyhub.llm.types import Message

class ChatBot:
    def __init__(self, model="gpt-4o-mini"):
        self.llm = LLM.create(model)
        self.history = []
    
    def chat(self, user_input: str) -> str:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.history.append(Message(role="user", content=user_input))
        
        # LLMì—ê²Œ ì „ì²´ íˆìŠ¤í† ë¦¬ ì „ë‹¬
        reply = self.llm.messages(self.history)
        
        # ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.history.append(Message(role="assistant", content=reply.text))
        
        return reply.text
    
    def reset(self):
        """ëŒ€í™” ì´ˆê¸°í™”"""
        self.history = []
    
    def save_history(self, filename: str):
        """ëŒ€í™” ë‚´ì—­ ì €ì¥"""
        import json
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump([msg.dict() for msg in self.history], f, ensure_ascii=False, indent=2)

# ì‚¬ìš© ì˜ˆ
bot = ChatBot()
print(bot.chat("ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ì‹¶ì–´ìš”."))
print(bot.chat("íŒŒì´ì¬ë¶€í„° ì‹œì‘í•˜ë©´ ì¢‹ì„ê¹Œìš”?"))
print(bot.chat("ê·¸ëŸ¼ ì²« ë²ˆì§¸ë¡œ ë­˜ ë°°ì›Œì•¼ í• ê¹Œìš”?"))
```

### ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬

```python
class ContextManagedChat:
    def __init__(self, model="gpt-4o-mini", max_messages=10):
        self.llm = LLM.create(model)
        self.history = []
        self.max_messages = max_messages
    
    def chat(self, user_input: str) -> str:
        # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° ì œí•œ
        if len(self.history) >= self.max_messages * 2:
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ì˜¤ë˜ëœ ëŒ€í™” ì œê±°
            self.history = self.history[-self.max_messages * 2:]
        
        self.history.append(Message(role="user", content=user_input))
        reply = self.llm.messages(self.history)
        self.history.append(Message(role="assistant", content=reply.text))
        
        return reply.text
```

### í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€í™”

```python
class PersonaChat:
    def __init__(self, persona: str, model="gpt-4o-mini"):
        self.llm = LLM.create(
            model,
            system_prompt=persona
        )
        self.history = []
    
    def chat(self, message: str) -> str:
        reply = self.llm.ask(message)
        return reply.text

# ë‹¤ì–‘í•œ í˜ë¥´ì†Œë‚˜
teacher = PersonaChat("ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì¸ë‚´ì‹¬ ìˆëŠ” í”„ë¡œê·¸ë˜ë° êµì‚¬ì…ë‹ˆë‹¤.")
chef = PersonaChat("ë‹¹ì‹ ì€ ë¯¸ìŠë­ 3ìŠ¤íƒ€ ì…°í”„ì…ë‹ˆë‹¤. ìš”ë¦¬ì— ëŒ€í•œ ì—´ì •ì´ ë„˜ì¹©ë‹ˆë‹¤.")
doctor = PersonaChat("ë‹¹ì‹ ì€ ì˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.")

print(teacher.chat("ì¬ê·€í•¨ìˆ˜ê°€ ë­”ê°€ìš”?"))
print(chef.chat("íŒŒìŠ¤íƒ€ ë©´ì„ ì‚¶ëŠ” ìµœì ì˜ ì‹œê°„ì€?"))
print(doctor.chat("ë‘í†µì´ ìì£¼ ìˆì–´ìš”"))  # ì£¼ì˜: ì‹¤ì œ ì˜ë£Œ ì¡°ì–¸ì´ ì•„ë‹˜
```

## íŒŒì¼ ì²˜ë¦¬

### ì´ë¯¸ì§€ ë¶„ì„

```python
from pyhub.llm import LLM

llm = LLM.create("gpt-4o-mini")  # ë¹„ì „ ì§€ì› ëª¨ë¸

# ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?",
    files=["photo.jpg"]
)
print(reply.text)

# ì—¬ëŸ¬ ì´ë¯¸ì§€ ë¹„êµ
reply = llm.ask(
    "ì´ ë‘ ì´ë¯¸ì§€ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    files=["before.png", "after.png"]
)
print(reply.text)

# ì´ë¯¸ì§€ì™€ í•¨ê»˜ êµ¬ì¡°í™”ëœ ì¶œë ¥
from pydantic import BaseModel
from typing import List

class ImageAnalysis(BaseModel):
    objects: List[str] = Field(description="ì´ë¯¸ì§€ì—ì„œ ë°œê²¬ëœ ê°ì²´ë“¤")
    scene: str = Field(description="ì „ì²´ì ì¸ ì¥ë©´ ì„¤ëª…")
    colors: List[str] = Field(description="ì£¼ìš” ìƒ‰ìƒë“¤")
    mood: str = Field(description="ì´ë¯¸ì§€ì˜ ë¶„ìœ„ê¸°")

reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
    files=["landscape.jpg"],
    schema=ImageAnalysis
)
analysis = reply.structured_data
print(f"ë°œê²¬ëœ ê°ì²´: {', '.join(analysis.objects)}")
print(f"ë¶„ìœ„ê¸°: {analysis.mood}")
```

### PDF ë¬¸ì„œ ì²˜ë¦¬

```python
# PDFëŠ” ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë¨
llm = LLM.create("gpt-4o-mini")

# PDF ìš”ì•½
reply = llm.ask(
    "ì´ PDF ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
    files=["report.pdf"]
)
print(reply.text)

# ì—¬ëŸ¬ í˜ì´ì§€ PDF ì²˜ë¦¬
class PDFSummary(BaseModel):
    title: str
    main_topics: List[str]
    key_findings: List[str]
    recommendations: List[str]

reply = llm.ask(
    "ì´ ì—°êµ¬ ë³´ê³ ì„œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
    files=["research_paper.pdf"],
    schema=PDFSummary
)
```

### ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸

```python
class ImagePrompt(BaseModel):
    style: str = Field(description="ê·¸ë¦¼ ìŠ¤íƒ€ì¼")
    subject: str = Field(description="ì£¼ìš” ëŒ€ìƒ")
    background: str = Field(description="ë°°ê²½ ì„¤ëª…")
    mood: str = Field(description="ë¶„ìœ„ê¸°")
    details: List[str] = Field(description="ì¶”ê°€ ì„¸ë¶€ì‚¬í•­")

llm = LLM.create("gpt-4o-mini")

# ì´ë¯¸ì§€ë¥¼ ë³´ê³  ìœ ì‚¬í•œ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
    files=["reference_image.jpg"],
    schema=ImagePrompt
)

prompt_data = reply.structured_data
print(f"ìŠ¤íƒ€ì¼: {prompt_data.style}")
print(f"í”„ë¡¬í”„íŠ¸: A {prompt_data.style} image of {prompt_data.subject} with {prompt_data.background}")
```

## ì„ë² ë”©

### í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±

```python
from pyhub.llm import LLM
import numpy as np

# ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
llm = LLM.create("text-embedding-3-small")

# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
text = "ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•œ ê¸°ìˆ ì…ë‹ˆë‹¤."
embedding = llm.embed(text)
print(f"ì„ë² ë”© ì°¨ì›: {len(embedding.embeddings[0])}")  # 1536

# ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì„ë² ë”©
texts = [
    "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
    "Python is a programming language.",
    "ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›¹ ê°œë°œì— ì‚¬ìš©ë©ë‹ˆë‹¤."
]
embeddings = llm.embed(texts)
print(f"ìƒì„±ëœ ì„ë² ë”© ìˆ˜: {len(embeddings.embeddings)}")
```

### ìœ ì‚¬ë„ ê³„ì‚°

```python
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(text1: str, text2: str) -> float:
    """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    llm = LLM.create("text-embedding-3-small")
    
    embeddings = llm.embed([text1, text2])
    vec1 = np.array(embeddings.embeddings[0]).reshape(1, -1)
    vec2 = np.array(embeddings.embeddings[1]).reshape(1, -1)
    
    similarity = cosine_similarity(vec1, vec2)[0][0]
    return similarity

# ì‚¬ìš© ì˜ˆ
pairs = [
    ("ê³ ì–‘ì´ëŠ” ê·€ì—¬ìš´ ë™ë¬¼ì…ë‹ˆë‹¤.", "ê°•ì•„ì§€ëŠ” ì¶©ì‹¤í•œ ë°˜ë ¤ë™ë¬¼ì…ë‹ˆë‹¤."),
    ("íŒŒì´ì¬ìœ¼ë¡œ ì›¹ ê°œë°œí•˜ê¸°", "Python web development"),
    ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”", "ë‚´ì¼ ë¹„ê°€ ì˜¨ëŒ€ìš”")
]

for text1, text2 in pairs:
    sim = calculate_similarity(text1, text2)
    print(f"ìœ ì‚¬ë„: {sim:.3f} - '{text1}' vs '{text2}'")
```

### ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ

```python
class DocumentSearch:
    def __init__(self, model="text-embedding-3-small"):
        self.llm = LLM.create(model)
        self.documents = []
        self.embeddings = []
    
    def add_documents(self, docs: List[str]):
        """ë¬¸ì„œ ì¶”ê°€ ë° ì„ë² ë”© ìƒì„±"""
        self.documents.extend(docs)
        new_embeddings = self.llm.embed(docs).embeddings
        self.embeddings.extend(new_embeddings)
    
    def search(self, query: str, top_k: int = 3) -> List[tuple]:
        """ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        query_embedding = self.llm.embed(query).embeddings[0]
        
        # ëª¨ë“  ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            sim = cosine_similarity(
                [query_embedding], 
                [doc_embedding]
            )[0][0]
            similarities.append((sim, i, self.documents[i]))
        
        # ìƒìœ„ kê°œ ë°˜í™˜
        similarities.sort(reverse=True)
        return [(score, doc) for score, idx, doc in similarities[:top_k]]

# ì‚¬ìš© ì˜ˆ
search = DocumentSearch()
search.add_documents([
    "íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
    "ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•©ë‹ˆë‹¤.",
    "ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤."
])

results = search.search("AIì™€ ê¸°ê³„í•™ìŠµ", top_k=2)
for score, doc in results:
    print(f"ìœ ì‚¬ë„ {score:.3f}: {doc}")
```

## í…œí”Œë¦¿ í™œìš©

### Jinja2 í…œí”Œë¦¿

```python
from pyhub.llm import LLM
from pyhub.llm.templates import PromptTemplate

# í…œí”Œë¦¿ ì •ì˜
template = PromptTemplate("""
ë‹¹ì‹ ì€ {{ role }}ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ìš”ì²­: {{ request }}

ë‹¤ìŒ ì¡°ê±´ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
{% for condition in conditions %}
- {{ condition }}
{% endfor %}
""")

llm = LLM.create("gpt-4o-mini")

# í…œí”Œë¦¿ ì‚¬ìš©
prompt = template.render(
    role="ì „ë¬¸ ìš”ë¦¬ì‚¬",
    request="íŒŒìŠ¤íƒ€ ë§Œë“œëŠ” ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
    conditions=[
        "ì´ˆë³´ìë„ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆë„ë¡",
        "ì¬ë£ŒëŠ” ë§ˆíŠ¸ì—ì„œ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒìœ¼ë¡œ",
        "30ë¶„ ì´ë‚´ì— ì™„ì„± ê°€ëŠ¥í•œ ë ˆì‹œí”¼"
    ]
)

reply = llm.ask(prompt)
print(reply.text)
```

### Few-shot í…œí”Œë¦¿

```python
few_shot_template = PromptTemplate("""
ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

{% for example in examples %}
ì…ë ¥: {{ example.input }}
ì¶œë ¥: {{ example.output }}

{% endfor %}
ì…ë ¥: {{ input }}
ì¶œë ¥:""")

# ë²ˆì—­ ì˜ˆì‹œ
examples = [
    {"input": "Hello", "output": "ì•ˆë…•í•˜ì„¸ìš”"},
    {"input": "Thank you", "output": "ê°ì‚¬í•©ë‹ˆë‹¤"},
    {"input": "Good morning", "output": "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤"}
]

prompt = few_shot_template.render(
    examples=examples,
    input="How are you?"
)

reply = llm.ask(prompt)
print(reply.text)  # "ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?" ë˜ëŠ” ìœ ì‚¬í•œ ë²ˆì—­
```

### ë™ì  í…œí”Œë¦¿

```python
class DynamicPromptBuilder:
    def __init__(self):
        self.templates = {
            "technical": PromptTemplate("ê¸°ìˆ ì  ê´€ì ì—ì„œ {{ topic }}ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”."),
            "simple": PromptTemplate("5ì‚´ ì•„ì´ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ {{ topic }}ì„ ì„¤ëª…í•˜ì„¸ìš”."),
            "business": PromptTemplate("ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ {{ topic }}ì˜ ê°€ì¹˜ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.")
        }
    
    def build(self, style: str, topic: str) -> str:
        template = self.templates.get(style, self.templates["simple"])
        return template.render(topic=topic)

builder = DynamicPromptBuilder()
llm = LLM.create("gpt-4o-mini")

# ê°™ì€ ì£¼ì œë¥¼ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ
topic = "ë¸”ë¡ì²´ì¸ ê¸°ìˆ "
for style in ["technical", "simple", "business"]:
    prompt = builder.build(style, topic)
    reply = llm.ask(prompt)
    print(f"\n[{style.upper()}]\n{reply.text[:200]}...")
```

## History Backup

ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì™¸ë¶€ ì €ì¥ì†Œì— ë°±ì—…í•˜ê³  ë³µì›í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ê¸°ë°˜ íˆìŠ¤í† ë¦¬ì™€ ë³„ë„ë¡œ ì˜êµ¬ ì €ì¥ì†Œì— ëŒ€í™” ë‚´ì—­ì„ ë³´ê´€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ê¸°ë³¸ ì‚¬ìš©ë²• (InMemoryHistoryBackup)

```python
from pyhub.llm import LLM
from pyhub.llm.history import InMemoryHistoryBackup

# ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°±ì—… (í…ŒìŠ¤íŠ¸ìš©)
backup = InMemoryHistoryBackup(
    user_id="user123",
    session_id="session456"
)

# ë°±ì—…ì´ í™œì„±í™”ëœ LLM ìƒì„±
llm = LLM.create("gpt-4o-mini", history_backup=backup)

# ëŒ€í™” ì§„í–‰ (ìë™ìœ¼ë¡œ ë°±ì—…ë¨)
llm.ask("Pythonì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
llm.ask("ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”")

# ë°±ì—…ëœ íˆìŠ¤í† ë¦¬ í™•ì¸
messages = backup.load_history()
for msg in messages:
    print(f"{msg.role}: {msg.content[:50]}...")

# ì‚¬ìš©ëŸ‰ í†µê³„
usage = backup.get_usage_summary()
print(f"ì´ ì…ë ¥ í† í°: {usage.input}")
print(f"ì´ ì¶œë ¥ í† í°: {usage.output}")
```

### SQLAlchemy ë°±ì—… (ì˜êµ¬ ì €ì¥)

```python
from pyhub.llm import LLM
from pyhub.llm.history import SQLAlchemyHistoryBackup
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
engine = create_engine("sqlite:///chat_history.db")
Session = sessionmaker(bind=engine)
session = Session()

# SQLAlchemy ë°±ì—… ìƒì„±
backup = SQLAlchemyHistoryBackup(
    session=session,
    user_id="user123",
    session_id="session456"
)

# í…Œì´ë¸” ìë™ ìƒì„±
from pyhub.llm.history.sqlalchemy_backup import Base
Base.metadata.create_all(engine)

# ë°±ì—…ì´ í™œì„±í™”ëœ LLM ìƒì„±
llm = LLM.create("gpt-4o-mini", history_backup=backup)

# ëŒ€í™” ì§„í–‰
llm.ask("ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ì›ì¹™ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”")
llm.ask("ì •ê·œí™”ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”")

# ì„¸ì…˜ ì»¤ë°‹ (ì˜êµ¬ ì €ì¥)
session.commit()
```

### ì´ì „ ëŒ€í™” ë³µì›

```python
# ìƒˆë¡œìš´ ì„¸ì…˜ì—ì„œ ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°
new_session = Session()
backup = SQLAlchemyHistoryBackup(
    session=new_session,
    user_id="user123",
    session_id="session456"
)

# ì´ì „ ëŒ€í™”ê°€ ìë™ìœ¼ë¡œ ë³µì›ëœ LLM
llm = LLM.create("gpt-4o-mini", history_backup=backup)

# ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•œ ì±„ ê³„ì† ëŒ€í™”
llm.ask("ì•ì„œ ì„¤ëª…í•œ ì •ê·œí™”ì˜ ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
```

### ì—¬ëŸ¬ ì„¸ì…˜ ê´€ë¦¬

```python
# ì‚¬ìš©ìë³„ ì—¬ëŸ¬ ì„¸ì…˜ ê´€ë¦¬
user_id = "user123"

# ì„¸ì…˜ 1: í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸
session1_backup = SQLAlchemyHistoryBackup(
    session=session,
    user_id=user_id,
    session_id="programming_session"
)
llm1 = LLM.create("gpt-4o-mini", history_backup=session1_backup)
llm1.ask("Pythonê³¼ JavaScriptì˜ ì°¨ì´ì ì€?")

# ì„¸ì…˜ 2: ìˆ˜í•™ ì§ˆë¬¸
session2_backup = SQLAlchemyHistoryBackup(
    session=session,
    user_id=user_id,
    session_id="math_session"
)
llm2 = LLM.create("gpt-4o-mini", history_backup=session2_backup)
llm2.ask("ë¯¸ì ë¶„í•™ì˜ ê¸°ë³¸ ì •ë¦¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”")

# ê° ì„¸ì…˜ì€ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬ë¨
```

### Tool ì‚¬ìš© ë‚´ì—­ ìë™ ì €ì¥

```python
# ë„êµ¬ í˜¸ì¶œ ë‚´ì—­ë„ ìë™ìœ¼ë¡œ ë°±ì—…ë¨
def get_weather(city: str) -> str:
    """ë„ì‹œì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return f"{city}ì˜ ë‚ ì”¨ëŠ” ë§‘ìŒ, 25Â°Cì…ë‹ˆë‹¤."

def get_time(timezone: str = "UTC") -> str:
    """í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

llm = LLM.create("gpt-4o-mini", history_backup=backup)
reply = llm.ask(
    "ì„œìš¸ì˜ ë‚ ì”¨ì™€ í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ì„¸ìš”",
    tools=[get_weather, get_time]
)

# ë°±ì—…ëœ ë©”ì‹œì§€ í™•ì¸
messages = backup.load_history()
assistant_msg = messages[-1]  # ë§ˆì§€ë§‰ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€

# tool_interactions í•„ë“œì— ë„êµ¬ ì‚¬ìš© ë‚´ì—­ì´ ì €ì¥ë¨
if assistant_msg.tool_interactions:
    for interaction in assistant_msg.tool_interactions:
        print(f"ë„êµ¬: {interaction['tool']}")
        print(f"ì¸ì: {interaction['arguments']}")
        print(f"ê²°ê³¼: {interaction.get('result', 'N/A')}")
```

### ì‚¬ìš©ì ì •ì˜ ë°±ì—… êµ¬í˜„

```python
from abc import ABC, abstractmethod
from pyhub.llm.history import HistoryBackup
from pyhub.llm.types import Message, Usage

class MongoDBHistoryBackup(HistoryBackup):
    """MongoDBë¥¼ ì‚¬ìš©í•œ íˆìŠ¤í† ë¦¬ ë°±ì—… ì˜ˆì œ"""
    
    def __init__(self, collection, user_id: str, session_id: str):
        self.collection = collection
        self.user_id = user_id
        self.session_id = session_id
    
    def save_exchange(
        self,
        user_msg: Message,
        assistant_msg: Message,
        usage: Optional[Usage] = None,
        model: Optional[str] = None
    ) -> None:
        """ëŒ€í™” êµí™˜ì„ MongoDBì— ì €ì¥"""
        doc = {
            "user_id": self.user_id,
            "session_id": self.session_id,
            "timestamp": datetime.utcnow(),
            "user_message": {
                "content": user_msg.content,
                "files": user_msg.files
            },
            "assistant_message": {
                "content": assistant_msg.content,
                "tool_interactions": assistant_msg.tool_interactions
            },
            "usage": {
                "input": usage.input if usage else 0,
                "output": usage.output if usage else 0
            },
            "model": model
        }
        self.collection.insert_one(doc)
    
    def load_history(self, limit: Optional[int] = None) -> list[Message]:
        """MongoDBì—ì„œ íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        query = {
            "user_id": self.user_id,
            "session_id": self.session_id
        }
        
        cursor = self.collection.find(query).sort("timestamp", 1)
        if limit:
            cursor = cursor.limit(limit // 2)  # ê° êµí™˜ì€ 2ê°œ ë©”ì‹œì§€
        
        messages = []
        for doc in cursor:
            # ì‚¬ìš©ì ë©”ì‹œì§€
            messages.append(Message(
                role="user",
                content=doc["user_message"]["content"],
                files=doc["user_message"].get("files")
            ))
            
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€
            messages.append(Message(
                role="assistant",
                content=doc["assistant_message"]["content"],
                tool_interactions=doc["assistant_message"].get("tool_interactions")
            ))
        
        return messages
    
    def get_usage_summary(self) -> Usage:
        """ì´ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        pipeline = [
            {"$match": {"user_id": self.user_id, "session_id": self.session_id}},
            {"$group": {
                "_id": None,
                "total_input": {"$sum": "$usage.input"},
                "total_output": {"$sum": "$usage.output"}
            }}
        ]
        
        result = list(self.collection.aggregate(pipeline))
        if result:
            return Usage(
                input=result[0]["total_input"],
                output=result[0]["total_output"]
            )
        return Usage(input=0, output=0)
    
    def clear(self) -> int:
        """íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
        result = self.collection.delete_many({
            "user_id": self.user_id,
            "session_id": self.session_id
        })
        return result.deleted_count * 2  # ê° ë¬¸ì„œëŠ” 2ê°œ ë©”ì‹œì§€
```

### ë°±ì—… ì‹¤íŒ¨ ì²˜ë¦¬

```python
# ë°±ì—… ì‹¤íŒ¨ ì‹œì—ë„ LLMì€ ì •ìƒ ë™ì‘
import logging

class UnreliableBackup(HistoryBackup):
    """ê°„í—ì ìœ¼ë¡œ ì‹¤íŒ¨í•˜ëŠ” ë°±ì—… (ì˜ˆì œ)"""
    
    def save_exchange(self, user_msg, assistant_msg, usage=None, model=None):
        import random
        if random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ì‹¤íŒ¨
            raise Exception("Backup service temporarily unavailable")
        # ì‹¤ì œ ì €ì¥ ë¡œì§...

# ë°±ì—… ì‹¤íŒ¨ëŠ” ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
backup = UnreliableBackup()
llm = LLM.create("gpt-4o-mini", history_backup=backup)

# ë°±ì—…ì´ ì‹¤íŒ¨í•´ë„ ëŒ€í™”ëŠ” ê³„ì†ë¨
reply = llm.ask("ë°±ì—…ì´ ì‹¤íŒ¨í•´ë„ ê´œì°®ë‚˜ìš”?")
# ê²½ê³  ë¡œê·¸ë§Œ ì¶œë ¥ë˜ê³  ì •ìƒ ë™ì‘
```

### ì£¼ìš” ë©”ì„œë“œ ì„¤ëª…

- `save_exchange()`: ì‚¬ìš©ì ë©”ì‹œì§€ì™€ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì„ í•œ ìŒìœ¼ë¡œ ì €ì¥
- `load_history(limit)`: ì €ì¥ëœ íˆìŠ¤í† ë¦¬ë¥¼ Message ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
- `get_usage_summary()`: ì´ í† í° ì‚¬ìš©ëŸ‰ í†µê³„ ë°˜í™˜
- `clear()`: í•´ë‹¹ ì„¸ì…˜ì˜ ëª¨ë“  íˆìŠ¤í† ë¦¬ ì‚­ì œ

> ğŸ’¡ **íŒ**: 
> - ë°±ì—…ì€ ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ì™€ ë³„ê°œë¡œ ë™ì‘í•˜ë©°, ì£¼ë¡œ ì˜êµ¬ ì €ì¥ ìš©ë„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤
> - Tool ì‚¬ìš© ë‚´ì—­ì€ `tool_interactions` í•„ë“œì— ìë™ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤
> - ë°±ì—… ì‹¤íŒ¨ ì‹œì—ë„ LLMì€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ë©°, ê²½ê³  ë¡œê·¸ë§Œ ì¶œë ¥ë©ë‹ˆë‹¤

## MCP í†µí•©

### ì„œë²„ ì´ë¦„ ìë™ ê°ì§€

MCP ì„œë²„ëŠ” ì´ˆê¸°í™” ì‹œ ìì²´ ì •ë³´(ì´ë¦„, ë²„ì „)ë¥¼ ì œê³µí•©ë‹ˆë‹¤. pyhub-llmì€ ì´ë¥¼ í™œìš©í•˜ì—¬ ì„œë²„ ì´ë¦„ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤:

```python
# name ì—†ì´ ì„¤ì • - ì„œë²„ê°€ "calculator-server"ë¡œ ìë™ ì œê³µ
config = McpConfig(
    cmd="pyhub-llm mcp-server run calculator"
)

# ì‚¬ìš©ìê°€ ì›í•˜ë©´ name ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥
config = McpConfig(
    name="my_calc",  # ì„œë²„ ì´ë¦„ì„ "my_calc"ë¡œ ë³€ê²½
    cmd="pyhub-llm mcp-server run calculator"
)
```

**ì„œë²„ ì´ë¦„ ìš°ì„ ìˆœìœ„:**
1. ì‚¬ìš©ìê°€ ì§€ì •í•œ `name` (ìµœìš°ì„ )
2. ì„œë²„ê°€ ì œê³µí•˜ëŠ” ì´ë¦„ (ìë™ ê°ì§€)
3. ìë™ ìƒì„±ëœ ì´ë¦„ (transport_uuid í˜•íƒœ)

**ì¤‘ë³µ ì²˜ë¦¬:**
- ë™ì¼í•œ ì´ë¦„ì˜ ì„œë²„ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ìë™ìœ¼ë¡œ suffix ì¶”ê°€ (`calculator-server_1`, `calculator-server_2`)
- ì¤‘ë³µ ì‹œ ê²½ê³  ë¡œê·¸ ì¶œë ¥

### í†µí•© ì„¤ì • ë° ìë™ ê°ì§€

pyhub-llm 0.7.0ë¶€í„°ëŠ” ëª¨ë“  MCP transport íƒ€ì…ì„ ë‹¨ì¼ `McpConfig` í´ë˜ìŠ¤ë¡œ í†µí•©í•˜ê³ , transport íƒ€ì…ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤:

```python
from pyhub.llm.mcp import McpConfig, create_mcp_config

# 1. ê¸°ë³¸ ì„¤ì • - transport ìë™ ê°ì§€
stdio_config = McpConfig(
    cmd="python calculator.py"  # stdio transportë¡œ ìë™ ê°ì§€
)

http_config = McpConfig(
    url="http://localhost:8080/mcp"  # streamable_http transportë¡œ ìë™ ê°ì§€
)

ws_config = McpConfig(
    url="ws://localhost:8080/ws"  # websocket transportë¡œ ìë™ ê°ì§€
)

sse_config = McpConfig(
    url="http://localhost:8080/sse"  # sse transportë¡œ ìë™ ê°ì§€
)

# 2. ë¬¸ìì—´ë¡œ ê°„í¸ ì„¤ì • - íŒ©í† ë¦¬ í•¨ìˆ˜ ì‚¬ìš©
config1 = create_mcp_config("python server.py")  # stdio
config2 = create_mcp_config("http://localhost:8080")  # http
config3 = create_mcp_config("ws://localhost:8080")  # websocket

# 3. ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •
config4 = create_mcp_config({
    "cmd": "python server.py",
    "name": "my-server",
    "timeout": 60
})
```

**Transport ìë™ ê°ì§€ ê·œì¹™:**
- `cmd` ë˜ëŠ” `command` í•„ë“œ â†’ `stdio` transport
- `http://` ë˜ëŠ” `https://` URL â†’ `streamable_http` transport
- `ws://` ë˜ëŠ” `wss://` URL â†’ `websocket` transport
- URLì— `/sse` í¬í•¨ ë˜ëŠ” `text/event-stream` í—¤ë” â†’ `sse` transport

### ê¸°ë³¸ MCP ì‚¬ìš©

```python
from pyhub.llm import LLM
from pyhub.llm.mcp import McpConfig

# MCP ì„œë²„ ì„¤ì •
mcp_config = McpConfig(
    cmd="calculator-server"  # MCP ì„œë²„ ì‹¤í–‰ ëª…ë ¹ (nameì€ ì„œë²„ê°€ ìë™ ì œê³µ)
)

# LLMê³¼ MCP í†µí•©
llm = LLM.create("gpt-4o-mini", mcp_servers=mcp_config)

# ìë™ìœ¼ë¡œ MCP ì´ˆê¸°í™”
await llm.initialize_mcp()

# MCP ë„êµ¬ê°€ ìë™ìœ¼ë¡œ ì‚¬ìš©ë¨
reply = await llm.ask_async("1234 ê³±í•˜ê¸° 5678ì€?")
print(reply.text)  # MCP ê³„ì‚°ê¸°ë¥¼ ì‚¬ìš©í•œ ì •í™•í•œ ë‹µë³€

# ì •ë¦¬
await llm.close_mcp()
```

### MCP ì—°ê²° ì •ì±…

```python
from pyhub.llm.mcp import MCPConnectionPolicy, MCPConnectionError

# OPTIONAL (ê¸°ë³¸ê°’) - MCP ì‹¤íŒ¨í•´ë„ ê³„ì†
llm1 = await LLM.create_async(
    "gpt-4o-mini",
    mcp_servers=mcp_config
)

# REQUIRED - MCP í•„ìˆ˜, ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸
try:
    llm2 = await LLM.create_async(
        "gpt-4o-mini",
        mcp_servers=mcp_config,
        mcp_policy=MCPConnectionPolicy.REQUIRED
    )
except MCPConnectionError as e:
    print(f"MCP ì—°ê²° ì‹¤íŒ¨: {e}")
    print(f"ì‹¤íŒ¨í•œ ì„œë²„: {e.failed_servers}")

# WARN - ì‹¤íŒ¨ ì‹œ ê²½ê³ ë§Œ
llm3 = await LLM.create_async(
    "gpt-4o-mini",
    mcp_servers=mcp_config,
    mcp_policy=MCPConnectionPolicy.WARN
)
```

### ì—¬ëŸ¬ MCP ì„œë²„ í†µí•©

```python
from pyhub.llm.mcp import McpConfig, create_mcp_config

# ë°©ë²• 1: ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ê°„í¸ ì„¤ì •
servers = [
    "python calculator.py",  # stdio transport ìë™ ê°ì§€
    "http://localhost:8080/mcp"  # http transport ìë™ ê°ì§€
]

# ë°©ë²• 2: ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„¸ ì„¤ì •
servers = [
    {"cmd": "calculator-server", "name": "calc"},
    {"url": "http://localhost:8080/mcp", "name": "web-api"}
]

# ë°©ë²• 3: McpConfig ê°ì²´ë¡œ ìƒì„¸ ì„¤ì •
servers = [
    McpConfig(
        cmd="calculator-server",
        timeout=60
    ),
    McpConfig(
        url="http://localhost:8080/mcp",
        headers={"Authorization": "Bearer token"}
    )
]

# ì—¬ëŸ¬ ì„œë²„ì™€ í•¨ê»˜ LLM ìƒì„±
llm = await LLM.create_async("gpt-4o-mini", mcp_servers=servers)

# ëª¨ë“  ë„êµ¬ê°€ í†µí•©ë˜ì–´ ì‚¬ìš© ê°€ëŠ¥
reply = await llm.ask_async(
    "ì„œìš¸ì˜ í˜„ì¬ ì˜¨ë„ë¥¼ ì„­ì”¨ì—ì„œ í™”ì”¨ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”"
)
print(reply.text)  # ë‚ ì”¨ API + ê³„ì‚°ê¸° ë„êµ¬ ëª¨ë‘ ì‚¬ìš©
```

### MCP ë„êµ¬ ì§ì ‘ ì œì–´

```python
# MCP ë„êµ¬ ëª©ë¡ í™•ì¸
if llm._mcp_tools:
    print("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬:")
    for tool in llm._mcp_tools:
        print(f"- {tool.name}: {tool.description}")

# íŠ¹ì • ë„êµ¬ë§Œ ì‚¬ìš©í•˜ë„ë¡ í•„í„°ë§
filtered_config = McpConfig(
    cmd="calculator-server",
    filter_tools=["add", "multiply"]  # ë§ì…ˆê³¼ ê³±ì…ˆë§Œ ì‚¬ìš©
)
```

## ì›¹ í”„ë ˆì„ì›Œí¬ í†µí•©

### FastAPI í†µí•©

```python
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from pyhub.llm import LLM
import asyncio

app = FastAPI()

# ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤
llm = LLM.create("gpt-4o-mini")

class ChatRequest(BaseModel):
    message: str
    stream: bool = False

class ChatResponse(BaseModel):
    response: str
    tokens_used: int

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì¼ë°˜ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    try:
        reply = await llm.ask_async(request.message)
        return ChatResponse(
            response=reply.text,
            tokens_used=reply.usage.total if reply.usage else 0
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    async def generate():
        async for chunk in llm.ask_async(request.message, stream=True):
            yield f"data: {chunk.text}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

# êµ¬ì¡°í™”ëœ ì¶œë ¥ ì—”ë“œí¬ì¸íŠ¸
class AnalysisRequest(BaseModel):
    text: str

class SentimentAnalysis(BaseModel):
    sentiment: str
    confidence: float
    keywords: List[str]

@app.post("/analyze", response_model=SentimentAnalysis)
async def analyze_sentiment(request: AnalysisRequest):
    """ê°ì • ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸"""
    reply = await llm.ask_async(
        f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”: {request.text}",
        schema=SentimentAnalysis
    )
    return reply.structured_data

# ì´ë¯¸ì§€ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
from fastapi import UploadFile, File

@app.post("/analyze-image")
async def analyze_image(
    file: UploadFile = File(...),
    question: str = "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
):
    """ì´ë¯¸ì§€ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸"""
    contents = await file.read()
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    import tempfile
    with tempfile.NamedTemporaryFile(delete=False, suffix=file.filename) as tmp:
        tmp.write(contents)
        tmp_path = tmp.name
    
    try:
        reply = await llm.ask_async(question, files=[tmp_path])
        return {"description": reply.text}
    finally:
        import os
        os.unlink(tmp_path)

# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
from fastapi import BackgroundTasks

async def process_long_task(task_id: str, prompt: str):
    """ê¸´ ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬"""
    reply = await llm.ask_async(prompt)
    # ê²°ê³¼ë¥¼ DBë‚˜ ìºì‹œì— ì €ì¥
    # save_result(task_id, reply.text)

@app.post("/long-task")
async def create_long_task(
    request: ChatRequest,
    background_tasks: BackgroundTasks
):
    """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒì„±"""
    task_id = str(uuid.uuid4())
    background_tasks.add_task(
        process_long_task,
        task_id,
        request.message
    )
    return {"task_id": task_id, "status": "processing"}
```

### Django í†µí•©

```python
# views.py
from django.http import JsonResponse, StreamingHttpResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.core.cache import cache
from pyhub.llm import LLM
import json

# ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤ (settingsì—ì„œ ê´€ë¦¬ ê¶Œì¥)
llm = LLM.create("gpt-4o-mini")

@method_decorator(csrf_exempt, name='dispatch')
class ChatView(View):
    """ì±„íŒ… API ë·°"""
    
    def post(self, request):
        data = json.loads(request.body)
        message = data.get('message', '')
        
        # ìºì‹œ í™•ì¸
        cache_key = f"chat:{hash(message)}"
        cached_response = cache.get(cache_key)
        if cached_response:
            return JsonResponse({'response': cached_response, 'cached': True})
        
        # LLM í˜¸ì¶œ
        reply = llm.ask(message)
        
        # ìºì‹œ ì €ì¥ (1ì‹œê°„)
        cache.set(cache_key, reply.text, 3600)
        
        return JsonResponse({
            'response': reply.text,
            'cached': False,
            'tokens': reply.usage.total if reply.usage else 0
        })

def chat_stream_view(request):
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ë·°"""
    message = request.GET.get('message', '')
    
    def generate():
        for chunk in llm.ask(message, stream=True):
            yield f"data: {json.dumps({'text': chunk.text})}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingHttpResponse(
        generate(),
        content_type='text/event-stream'
    )

# models.py
from django.db import models
from django.contrib.auth.models import User

class ChatHistory(models.Model):
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ëª¨ë¸"""
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    message = models.TextField()
    response = models.TextField()
    created_at = models.DateTimeField(auto_now_add=True)
    tokens_used = models.IntegerField(default=0)
    
    class Meta:
        ordering = ['-created_at']

# ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ì‚¬ìš©
class ChatWithHistoryView(View):
    def post(self, request):
        user = request.user
        data = json.loads(request.body)
        message = data.get('message', '')
        
        # ì´ì „ ëŒ€í™” ë‚´ì—­ ê°€ì ¸ì˜¤ê¸°
        history = ChatHistory.objects.filter(user=user).order_by('-created_at')[:5]
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        messages = []
        for h in reversed(history):
            messages.append(Message(role="user", content=h.message))
            messages.append(Message(role="assistant", content=h.response))
        messages.append(Message(role="user", content=message))
        
        # LLM í˜¸ì¶œ
        reply = llm.messages(messages)
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        ChatHistory.objects.create(
            user=user,
            message=message,
            response=reply.text,
            tokens_used=reply.usage.total if reply.usage else 0
        )
        
        return JsonResponse({'response': reply.text})

# ê´€ë¦¬ì ëª…ë ¹ì–´ (management/commands/chat_stats.py)
from django.core.management.base import BaseCommand
from django.db.models import Sum, Count

class Command(BaseCommand):
    help = 'ì±„íŒ… í†µê³„ í‘œì‹œ'
    
    def handle(self, *args, **options):
        stats = ChatHistory.objects.aggregate(
            total_chats=Count('id'),
            total_tokens=Sum('tokens_used')
        )
        
        self.stdout.write(
            self.style.SUCCESS(
                f"ì´ ëŒ€í™” ìˆ˜: {stats['total_chats']}\n"
                f"ì´ í† í° ì‚¬ìš©ëŸ‰: {stats['total_tokens']}"
            )
        )
```

### Streamlit í†µí•©

```python
import streamlit as st
from pyhub.llm import LLM
from pyhub.llm.types import Message
import time

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'llm' not in st.session_state:
    st.session_state.llm = LLM.create("gpt-4o-mini")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    
    # ëª¨ë¸ ì„ íƒ
    model = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        ["gpt-4o-mini", "gpt-4o", "claude-3-haiku-20240307"]
    )
    
    # ì˜¨ë„ ì„¤ì •
    temperature = st.slider("ì°½ì˜ì„± (Temperature)", 0.0, 2.0, 0.7)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = st.text_area(
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
        value="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
    )
    
    # ì„¤ì • ì ìš©
    if st.button("ì„¤ì • ì ìš©"):
        st.session_state.llm = LLM.create(
            model,
            temperature=temperature,
            system_prompt=system_prompt
        )
        st.success("ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ëŒ€í™” ì´ˆê¸°í™”
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ¤– AI ì±—ë´‡")

# ëŒ€í™” ë‚´ì—­ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        for chunk in st.session_state.llm.ask(prompt, stream=True):
            full_response += chunk.text
            message_placeholder.markdown(full_response + "â–Œ")
        
        message_placeholder.markdown(full_response)
    
    # ì‘ë‹µ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": full_response})

# ì¶”ê°€ ê¸°ëŠ¥ë“¤
with st.expander("ê³ ê¸‰ ê¸°ëŠ¥"):
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ëŒ€í™” ë‚´ë³´ë‚´ê¸°"):
            import json
            conversation = json.dumps(st.session_state.messages, ensure_ascii=False, indent=2)
            st.download_button(
                label="JSONìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ",
                data=conversation,
                file_name="conversation.json",
                mime="application/json"
            )
    
    with col2:
        if st.button("í† í° ì‚¬ìš©ëŸ‰ í™•ì¸"):
            # ì‹¤ì œ êµ¬í˜„ ì‹œ í† í° ì¹´ìš´íŒ… ë¡œì§ ì¶”ê°€
            st.info("ì´ ê¸°ëŠ¥ì€ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")

# íŒŒì¼ ì—…ë¡œë“œ (ì´ë¯¸ì§€ ë¶„ì„)
uploaded_file = st.file_uploader(
    "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
    type=['png', 'jpg', 'jpeg']
)

if uploaded_file is not None:
    st.image(uploaded_file, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_column_width=True)
    
    if st.button("ì´ë¯¸ì§€ ë¶„ì„"):
        with st.spinner("ë¶„ì„ ì¤‘..."):
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix=uploaded_file.name) as tmp:
                tmp.write(uploaded_file.getbuffer())
                tmp_path = tmp.name
            
            # ë¶„ì„
            reply = st.session_state.llm.ask(
                "ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                files=[tmp_path]
            )
            
            st.write("### ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼")
            st.write(reply.text)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            import os
            os.unlink(tmp_path)
```

## ë„êµ¬/í•¨ìˆ˜ í˜¸ì¶œ

### ê¸°ë³¸ í•¨ìˆ˜ ì •ì˜

```python
from pyhub.llm import LLM
from typing import Dict, Any
import json

# í•¨ìˆ˜ ì •ì˜
def get_weather(location: str, unit: str = "celsius") -> Dict[str, Any]:
    """í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ
    return {
        "location": location,
        "temperature": 25,
        "unit": unit,
        "condition": "ë§‘ìŒ"
    }

def calculate(expression: str) -> float:
    """ìˆ˜í•™ í‘œí˜„ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    # ì•ˆì „í•œ eval ì‚¬ìš©
    import ast
    import operator as op
    
    ops = {
        ast.Add: op.add,
        ast.Sub: op.sub,
        ast.Mult: op.mul,
        ast.Div: op.truediv,
        ast.Pow: op.pow
    }
    
    def eval_expr(expr):
        return eval(expr, {"__builtins__": {}}, {})
    
    try:
        return eval_expr(expression)
    except:
        return "ê³„ì‚°í•  ìˆ˜ ì—†ëŠ” í‘œí˜„ì‹ì…ë‹ˆë‹¤."

# ë„êµ¬ ì •ì˜
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "íŠ¹ì • ìœ„ì¹˜ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "ë„ì‹œ ì´ë¦„ (ì˜ˆ: ì„œìš¸, ë‰´ìš•)"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "ì˜¨ë„ ë‹¨ìœ„"
                    }
                },
                "required": ["location"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "calculate",
            "description": "ìˆ˜í•™ í‘œí˜„ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "ê³„ì‚°í•  ìˆ˜í•™ í‘œí˜„ì‹ (ì˜ˆ: 2+2, 10*5)"
                    }
                },
                "required": ["expression"]
            }
        }
    }
]

# LLMê³¼ í•¨ê»˜ ì‚¬ìš©
llm = LLM.create("gpt-4o-mini")

# ë„êµ¬ì™€ í•¨ê»˜ ì§ˆë¬¸
response = llm.ask_with_tools(
    "ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ëŠ” ì–´ë•Œ? ê·¸ë¦¬ê³  ì„­ì”¨ 25ë„ëŠ” í™”ì”¨ë¡œ ëª‡ ë„ì•¼?",
    tools=tools
)

# í•¨ìˆ˜ í˜¸ì¶œ ì²˜ë¦¬
if response.tool_calls:
    for tool_call in response.tool_calls:
        function_name = tool_call.function.name
        arguments = json.loads(tool_call.function.arguments)
        
        # ì‹¤ì œ í•¨ìˆ˜ í˜¸ì¶œ
        if function_name == "get_weather":
            result = get_weather(**arguments)
        elif function_name == "calculate":
            result = calculate(**arguments)
        
        print(f"í•¨ìˆ˜ {function_name} í˜¸ì¶œ: {arguments}")
        print(f"ê²°ê³¼: {result}")
```

### í´ë˜ìŠ¤ ê¸°ë°˜ ë„êµ¬

```python
class ToolHandler:
    """ë„êµ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.tools = []
        self.functions = {}
    
    def register(self, func, description: str, parameters: dict):
        """í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë“±ë¡"""
        tool = {
            "type": "function",
            "function": {
                "name": func.__name__,
                "description": description,
                "parameters": parameters
            }
        }
        self.tools.append(tool)
        self.functions[func.__name__] = func
        return func
    
    def execute(self, tool_call):
        """ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰"""
        func_name = tool_call.function.name
        if func_name in self.functions:
            args = json.loads(tool_call.function.arguments)
            return self.functions[func_name](**args)
        else:
            raise ValueError(f"Unknown function: {func_name}")

# ì‚¬ìš© ì˜ˆ
handler = ToolHandler()

@handler.register(
    description="ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤",
    parameters={
        "type": "object",
        "properties": {
            "a": {"type": "number", "description": "ì²« ë²ˆì§¸ ìˆ«ì"},
            "b": {"type": "number", "description": "ë‘ ë²ˆì§¸ ìˆ«ì"}
        },
        "required": ["a", "b"]
    }
)
def add(a: float, b: float) -> float:
    return a + b

@handler.register(
    description="í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤",
    parameters={
        "type": "object",
        "properties": {
            "text": {"type": "string", "description": "ë²ˆì—­í•  í…ìŠ¤íŠ¸"},
            "target_lang": {"type": "string", "description": "ëª©í‘œ ì–¸ì–´ ì½”ë“œ"}
        },
        "required": ["text", "target_lang"]
    }
)
def translate(text: str, target_lang: str) -> str:
    # ì‹¤ì œë¡œëŠ” ë²ˆì—­ API í˜¸ì¶œ
    translations = {
        "ko": {"Hello": "ì•ˆë…•í•˜ì„¸ìš”", "Thank you": "ê°ì‚¬í•©ë‹ˆë‹¤"},
        "ja": {"Hello": "ã“ã‚“ã«ã¡ã¯", "Thank you": "ã‚ã‚ŠãŒã¨ã†"},
        "es": {"Hello": "Hola", "Thank you": "Gracias"}
    }
    return translations.get(target_lang, {}).get(text, text)

# LLMê³¼ í†µí•©
llm = LLM.create("gpt-4o-mini")
response = llm.ask_with_tools(
    "Helloë¥¼ í•œêµ­ì–´ì™€ ì¼ë³¸ì–´ë¡œ ë²ˆì—­í•´ì£¼ê³ , 5 ë”í•˜ê¸° 3ì€ ì–¼ë§ˆì¸ì§€ ê³„ì‚°í•´ì¤˜",
    tools=handler.tools
)

# ì‘ë‹µ ì²˜ë¦¬
for tool_call in response.tool_calls:
    result = handler.execute(tool_call)
    print(f"{tool_call.function.name}: {result}")
```

### ë¹„ë™ê¸° ë„êµ¬ í˜¸ì¶œ

```python
import aiohttp
import asyncio

class AsyncToolHandler:
    """ë¹„ë™ê¸° ë„êµ¬ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.tools = []
        self.async_functions = {}
    
    def register_async(self, func, description: str, parameters: dict):
        """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë„êµ¬ë¡œ ë“±ë¡"""
        tool = {
            "type": "function",
            "function": {
                "name": func.__name__,
                "description": description,
                "parameters": parameters
            }
        }
        self.tools.append(tool)
        self.async_functions[func.__name__] = func
        return func
    
    async def execute_async(self, tool_call):
        """ë¹„ë™ê¸° ë„êµ¬ ì‹¤í–‰"""
        func_name = tool_call.function.name
        if func_name in self.async_functions:
            args = json.loads(tool_call.function.arguments)
            return await self.async_functions[func_name](**args)
        else:
            raise ValueError(f"Unknown function: {func_name}")

# ë¹„ë™ê¸° ë„êµ¬ ì •ì˜
async_handler = AsyncToolHandler()

@async_handler.register_async(
    description="ì›¹í˜ì´ì§€ì˜ ì œëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤",
    parameters={
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "ì›¹í˜ì´ì§€ URL"}
        },
        "required": ["url"]
    }
)
async def get_page_title(url: str) -> str:
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            text = await response.text()
            # ê°„ë‹¨í•œ ì œëª© ì¶”ì¶œ
            import re
            match = re.search(r'<title>(.*?)</title>', text, re.IGNORECASE)
            return match.group(1) if match else "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

# ë¹„ë™ê¸° ì‹¤í–‰
async def main():
    llm = LLM.create("gpt-4o-mini")
    response = await llm.ask_with_tools_async(
        "Python ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì˜ ì œëª©ì„ ì•Œë ¤ì¤˜",
        tools=async_handler.tools
    )
    
    for tool_call in response.tool_calls:
        result = await async_handler.execute_async(tool_call)
        print(f"ê²°ê³¼: {result}")

asyncio.run(main())
```

## ì²´ì´ë‹

### ê¸°ë³¸ ì²´ì´ë‹

```python
from pyhub.llm import LLM, SequentialChain

# ì—¬ëŸ¬ LLMì„ ì—°ê²°
chain = SequentialChain([
    LLM.create("gpt-4o-mini", system_prompt="í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"),
    LLM.create("claude-3-haiku-20240307", system_prompt="ì˜ì–´ë¥¼ ì¼ë³¸ì–´ë¡œ ë²ˆì—­"),
    LLM.create("gpt-4o-mini", system_prompt="ì¼ë³¸ì–´ë¥¼ ë‹¤ì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­")
])

# ì²´ì¸ ì‹¤í–‰
result = chain.ask("ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.")
print(f"ìµœì¢… ê²°ê³¼: {result.text}")

# ê° ë‹¨ê³„ì˜ ê²°ê³¼ í™•ì¸
for i, step_result in enumerate(result.steps):
    print(f"ë‹¨ê³„ {i+1}: {step_result.text}")
```

### íŒŒì´í”„ ì—°ì‚°ì ì‚¬ìš©

```python
# íŒŒì´í”„ ì—°ì‚°ìë¡œ ì²´ì¸ êµ¬ì„±
translator = LLM.create("gpt-4o-mini", system_prompt="í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­")
analyzer = LLM.create("gpt-4o-mini", system_prompt="í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„")
summarizer = LLM.create("gpt-4o-mini", system_prompt="í•µì‹¬ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½")

# ì²´ì¸ êµ¬ì„±
chain = translator | analyzer | summarizer

# ì‹¤í–‰
result = chain.ask("ì •ë§ ê¸°ìœ ì¼ì´ ìƒê²¼ì–´ìš”! ë“œë””ì–´ ì·¨ì—…ì— ì„±ê³µí–ˆë‹µë‹ˆë‹¤.")
print(result.text)
```

### ì¡°ê±´ë¶€ ì²´ì´ë‹

```python
class ConditionalChain:
    """ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì²´ì¸ì„ ì‹¤í–‰"""
    
    def __init__(self):
        self.classifier = LLM.create(
            "gpt-4o-mini",
            system_prompt="í…ìŠ¤íŠ¸ì˜ ì£¼ì œë¥¼ ë¶„ë¥˜"
        )
        
        self.tech_chain = LLM.create(
            "gpt-4o-mini",
            system_prompt="ê¸°ìˆ  ê´€ë ¨ ì§ˆë¬¸ì— ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€"
        )
        
        self.general_chain = LLM.create(
            "gpt-4o-mini",
            system_prompt="ì¼ë°˜ì ì¸ ëŒ€í™”ë¥¼ ì¹œê·¼í•˜ê²Œ"
        )
    
    def process(self, text: str) -> str:
        # ë¨¼ì € ë¶„ë¥˜
        classification = self.classifier.ask(
            text,
            choices=["ê¸°ìˆ ", "ì¼ë°˜"]
        )
        
        # ë¶„ë¥˜ì— ë”°ë¼ ë‹¤ë¥¸ ì²´ì¸ ì‚¬ìš©
        if classification.choice == "ê¸°ìˆ ":
            return self.tech_chain.ask(text).text
        else:
            return self.general_chain.ask(text).text

# ì‚¬ìš©
conditional = ConditionalChain()
print(conditional.process("íŒŒì´ì¬ì—ì„œ ë°ì½”ë ˆì´í„°ëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?"))
print(conditional.process("ì˜¤ëŠ˜ ì ì‹¬ ë­ ë¨¹ì„ê¹Œìš”?"))
```

### ë³‘ë ¬ ì²˜ë¦¬ ì²´ì¸

```python
async def parallel_chain(text: str):
    """ì—¬ëŸ¬ ë¶„ì„ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰"""
    
    sentiment_llm = LLM.create("gpt-4o-mini", system_prompt="ê°ì • ë¶„ì„")
    keyword_llm = LLM.create("gpt-4o-mini", system_prompt="í‚¤ì›Œë“œ ì¶”ì¶œ")
    summary_llm = LLM.create("gpt-4o-mini", system_prompt="ìš”ì•½")
    
    # ë³‘ë ¬ ì‹¤í–‰
    tasks = [
        sentiment_llm.ask_async(text),
        keyword_llm.ask_async(f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í‚¤ì›Œë“œ 3ê°œ: {text}"),
        summary_llm.ask_async(f"í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½: {text}")
    ]
    
    results = await asyncio.gather(*tasks)
    
    return {
        "sentiment": results[0].text,
        "keywords": results[1].text,
        "summary": results[2].text
    }

# ì‹¤í–‰
text = "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ë§ì€ ì‚°ì—…ì´ ë³€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ìë™í™”ì™€ íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ í° ì§„ì „ì´ ìˆì—ˆìŠµë‹ˆë‹¤."
result = asyncio.run(parallel_chain(text))
for key, value in result.items():
    print(f"{key}: {value}")
```

## ì—ëŸ¬ ì²˜ë¦¬

### ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬

```python
from pyhub.llm import LLM
from pyhub.llm.exceptions import (
    LLMError,
    RateLimitError,
    AuthenticationError,
    InvalidRequestError
)

def safe_llm_call(prompt: str, max_retries: int = 3):
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì•ˆì „í•œ LLM í˜¸ì¶œ"""
    llm = LLM.create("gpt-4o-mini")
    
    for attempt in range(max_retries):
        try:
            return llm.ask(prompt)
        
        except RateLimitError as e:
            # ì†ë„ ì œí•œ ì—ëŸ¬ - ëŒ€ê¸° í›„ ì¬ì‹œë„
            wait_time = getattr(e, 'retry_after', 60)
            print(f"Rate limit hit. Waiting {wait_time} seconds...")
            time.sleep(wait_time)
            
        except AuthenticationError:
            # ì¸ì¦ ì—ëŸ¬ - ì¬ì‹œë„ ë¶ˆê°€
            print("Authentication failed. Check your API key.")
            raise
            
        except InvalidRequestError as e:
            # ì˜ëª»ëœ ìš”ì²­ - ìˆ˜ì • í•„ìš”
            print(f"Invalid request: {e}")
            raise
            
        except LLMError as e:
            # ê¸°íƒ€ LLM ì—ëŸ¬
            print(f"LLM error (attempt {attempt + 1}): {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    
    raise Exception("Max retries exceeded")
```

### í´ë°± ì²˜ë¦¬

```python
class LLMWithFallback:
    """í´ë°± LLMì´ ìˆëŠ” ë˜í¼"""
    
    def __init__(self, primary_model: str, fallback_model: str):
        self.primary = LLM.create(primary_model)
        self.fallback = LLM.create(fallback_model)
    
    def ask(self, prompt: str, **kwargs):
        try:
            return self.primary.ask(prompt, **kwargs)
        except Exception as e:
            print(f"Primary LLM failed: {e}. Using fallback...")
            return self.fallback.ask(prompt, **kwargs)

# ì‚¬ìš©
llm = LLMWithFallback("gpt-4o", "gpt-4o-mini")
reply = llm.ask("ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ...")
```

### íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬

```python
import asyncio
from asyncio import TimeoutError

async def ask_with_timeout(llm, prompt: str, timeout: float = 30.0):
    """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” LLM í˜¸ì¶œ"""
    try:
        return await asyncio.wait_for(
            llm.ask_async(prompt),
            timeout=timeout
        )
    except TimeoutError:
        print(f"Request timed out after {timeout} seconds")
        # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
        simplified = f"ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”: {prompt}"
        return await llm.ask_async(simplified)

# ì‚¬ìš©
llm = LLM.create("gpt-4o-mini")
result = asyncio.run(ask_with_timeout(llm, "ë§¤ìš° ë³µì¡í•œ ì§ˆë¬¸...", timeout=10))
```

## ì‹¤ìš©ì ì¸ ì˜ˆì œ

### ì±—ë´‡ êµ¬í˜„

```python
class AdvancedChatBot:
    """ê³ ê¸‰ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì±—ë´‡"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.llm = LLM.create(model)
        self.history = []
        self.user_preferences = {}
        
    def chat(self, message: str) -> str:
        # ì‚¬ìš©ì ì˜ë„ íŒŒì•…
        intent = self._analyze_intent(message)
        
        # ì˜ë„ì— ë”°ë¥¸ ì²˜ë¦¬
        if intent == "personal_info":
            return self._handle_personal_info(message)
        elif intent == "recommendation":
            return self._handle_recommendation(message)
        else:
            return self._handle_general_chat(message)
    
    def _analyze_intent(self, message: str) -> str:
        reply = self.llm.ask(
            message,
            choices=["personal_info", "recommendation", "general_chat"]
        )
        return reply.choice
    
    def _handle_personal_info(self, message: str) -> str:
        # ê°œì¸ ì •ë³´ ì¶”ì¶œ ë° ì €ì¥
        from pydantic import BaseModel
        
        class PersonalInfo(BaseModel):
            name: str = None
            preferences: List[str] = []
            interests: List[str] = []
        
        reply = self.llm.ask(
            f"ë‹¤ìŒ ë©”ì‹œì§€ì—ì„œ ê°œì¸ ì •ë³´ ì¶”ì¶œ: {message}",
            schema=PersonalInfo
        )
        
        info = reply.structured_data
        if info.name:
            self.user_preferences['name'] = info.name
        if info.preferences:
            self.user_preferences['preferences'] = info.preferences
        
        return f"ì•Œê² ìŠµë‹ˆë‹¤! ê¸°ì–µí•˜ê³  ìˆì„ê²Œìš”."
    
    def _handle_recommendation(self, message: str) -> str:
        # ì‚¬ìš©ì ì„ í˜¸ë„ ê¸°ë°˜ ì¶”ì²œ
        context = f"ì‚¬ìš©ì ì •ë³´: {self.user_preferences}\nì§ˆë¬¸: {message}"
        reply = self.llm.ask(context)
        return reply.text
    
    def _handle_general_chat(self, message: str) -> str:
        # ì¼ë°˜ ëŒ€í™”
        self.history.append(Message(role="user", content=message))
        reply = self.llm.messages(self.history[-10:])  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€
        self.history.append(Message(role="assistant", content=reply.text))
        return reply.text
```

### ë¬¸ì„œ ìš”ì•½ê¸°

```python
class DocumentSummarizer:
    """ê³„ì¸µì  ë¬¸ì„œ ìš”ì•½ê¸°"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.llm = LLM.create(model)
        self.chunk_size = 2000  # í† í° ê¸°ì¤€
    
    def summarize(self, text: str, max_length: int = 500) -> str:
        """ê¸´ ë¬¸ì„œë¥¼ ê³„ì¸µì ìœ¼ë¡œ ìš”ì•½"""
        
        # í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´ ì§ì ‘ ìš”ì•½
        if len(text) < self.chunk_size:
            return self._simple_summarize(text, max_length)
        
        # ì²­í¬ë¡œ ë¶„í• 
        chunks = self._split_into_chunks(text)
        
        # ê° ì²­í¬ ìš”ì•½
        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            print(f"ì²­í¬ {i+1}/{len(chunks)} ìš”ì•½ ì¤‘...")
            summary = self._simple_summarize(chunk, max_length // len(chunks))
            chunk_summaries.append(summary)
        
        # ìš”ì•½ë“¤ì„ ë‹¤ì‹œ ìš”ì•½
        combined = "\n\n".join(chunk_summaries)
        final_summary = self._simple_summarize(
            f"ë‹¤ìŒ ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ì„¸ìš”:\n{combined}",
            max_length
        )
        
        return final_summary
    
    def _simple_summarize(self, text: str, max_length: int) -> str:
        """ë‹¨ìˆœ ìš”ì•½"""
        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n\n{text}"
        reply = self.llm.ask(prompt)
        return reply.text
    
    def _split_into_chunks(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        words = text.split()
        chunks = []
        current_chunk = []
        
        for word in words:
            current_chunk.append(word)
            if len(" ".join(current_chunk)) > self.chunk_size:
                chunks.append(" ".join(current_chunk[:-1]))
                current_chunk = [word]
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks

# ì‚¬ìš© ì˜ˆ
summarizer = DocumentSummarizer()
with open("long_document.txt", "r", encoding="utf-8") as f:
    document = f.read()

summary = summarizer.summarize(document, max_length=300)
print(summary)
```

### ì½”ë“œ ë¦¬ë·°ì–´

```python
class CodeReviewer:
    """AI ì½”ë“œ ë¦¬ë·°ì–´"""
    
    def __init__(self, model="gpt-4o"):
        self.llm = LLM.create(
            model,
            system_prompt="ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤."
        )
    
    def review_code(self, code: str, language: str = "python") -> dict:
        """ì½”ë“œë¥¼ ë‹¤ê°ë„ë¡œ ë¦¬ë·°"""
        
        from pydantic import BaseModel, Field
        
        class CodeReview(BaseModel):
            summary: str = Field(description="ì „ì²´ì ì¸ í‰ê°€")
            issues: List[str] = Field(description="ë°œê²¬ëœ ë¬¸ì œì ë“¤")
            improvements: List[str] = Field(description="ê°œì„  ì œì•ˆì‚¬í•­")
            security: List[str] = Field(description="ë³´ì•ˆ ê´€ë ¨ ì‚¬í•­")
            performance: List[str] = Field(description="ì„±ëŠ¥ ê´€ë ¨ ì‚¬í•­")
            best_practices: List[str] = Field(description="ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤")
            score: int = Field(description="ì „ì²´ ì ìˆ˜ (0-100)", ge=0, le=100)
        
        prompt = f"""
ë‹¤ìŒ {language} ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”:

```{language}
{code}
```

ì½”ë“œì˜ í’ˆì§ˆ, ë³´ì•ˆ, ì„±ëŠ¥, ê°€ë…ì„± ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
"""
        
        reply = self.llm.ask(prompt, schema=CodeReview)
        return reply.structured_data.dict()
    
    def suggest_refactoring(self, code: str) -> str:
        """ë¦¬íŒ©í† ë§ ì œì•ˆ"""
        prompt = f"""
ë‹¤ìŒ ì½”ë“œë¥¼ ë” ê¹”ë”í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë¦¬íŒ©í† ë§í•´ì£¼ì„¸ìš”:

```python
{code}
```

ë¦¬íŒ©í† ë§ëœ ì½”ë“œì™€ í•¨ê»˜ ë³€ê²½ ì‚¬í•­ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
        reply = self.llm.ask(prompt)
        return reply.text

# ì‚¬ìš© ì˜ˆ
reviewer = CodeReviewer()

code = """
def calculate_sum(numbers):
    total = 0
    for i in range(len(numbers)):
        total = total + numbers[i]
    return total

result = calculate_sum([1, 2, 3, 4, 5])
print("Sum is: " + str(result))
"""

review = reviewer.review_code(code)
print(f"ì ìˆ˜: {review['score']}/100")
print(f"ë¬¸ì œì : {review['issues']}")
print(f"ê°œì„ ì‚¬í•­: {review['improvements']}")

# ë¦¬íŒ©í† ë§ ì œì•ˆ
refactored = reviewer.suggest_refactoring(code)
print(f"\në¦¬íŒ©í† ë§ ì œì•ˆ:\n{refactored}")
```

### ë²ˆì—­ê¸°

```python
class SmartTranslator:
    """ì»¨í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ ë²ˆì—­ê¸°"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.llm = LLM.create(model)
        self.context_history = []
    
    def translate(
        self,
        text: str,
        target_lang: str,
        source_lang: str = "auto",
        style: str = "formal"
    ) -> dict:
        """ê³ ê¸‰ ë²ˆì—­ ê¸°ëŠ¥"""
        
        from pydantic import BaseModel
        
        class Translation(BaseModel):
            translated_text: str
            detected_language: str = None
            confidence: float = Field(ge=0, le=1)
            alternatives: List[str] = []
            notes: str = None
        
        # ì»¨í…ìŠ¤íŠ¸ í¬í•¨ í”„ë¡¬í”„íŠ¸
        context = ""
        if self.context_history:
            context = f"ì´ì „ ë²ˆì—­ ì»¨í…ìŠ¤íŠ¸:\n"
            for prev in self.context_history[-3:]:
                context += f"- {prev}\n"
        
        prompt = f"""
{context}

ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_lang}ë¡œ ë²ˆì—­í•˜ì„¸ìš”.
ìŠ¤íƒ€ì¼: {style}
ì›ë¬¸: {text}

ë¬¸í™”ì  ë‰˜ì•™ìŠ¤ì™€ ê´€ìš©í‘œí˜„ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•´ì£¼ì„¸ìš”.
"""
        
        reply = self.llm.ask(prompt, schema=Translation)
        translation = reply.structured_data
        
        # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        self.context_history.append(f"{text} â†’ {translation.translated_text}")
        
        return translation.dict()
    
    def translate_document(self, file_path: str, target_lang: str) -> str:
        """ë¬¸ì„œ ì „ì²´ ë²ˆì—­"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ë‹¨ë½ë³„ë¡œ ë²ˆì—­
        paragraphs = content.split('\n\n')
        translated_paragraphs = []
        
        for i, para in enumerate(paragraphs):
            if para.strip():
                print(f"ë²ˆì—­ ì¤‘... {i+1}/{len(paragraphs)}")
                result = self.translate(para, target_lang)
                translated_paragraphs.append(result['translated_text'])
            else:
                translated_paragraphs.append("")
        
        return '\n\n'.join(translated_paragraphs)

# ì‚¬ìš© ì˜ˆ
translator = SmartTranslator()

# ë‹¨ì¼ ë²ˆì—­
result = translator.translate(
    "The early bird catches the worm",
    target_lang="í•œêµ­ì–´",
    style="casual"
)
print(f"ë²ˆì—­: {result['translated_text']}")
print(f"ëŒ€ì•ˆ: {result['alternatives']}")

# ì—°ì† ë²ˆì—­ (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
texts = [
    "I love programming.",
    "It's like solving puzzles.",
    "Each bug is a new challenge."
]

for text in texts:
    result = translator.translate(text, "í•œêµ­ì–´")
    print(f"{text} â†’ {result['translated_text']}")
```

### Q&A ì‹œìŠ¤í…œ

```python
class QASystem:
    """ë¬¸ì„œ ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.llm = LLM.create(model)
        self.documents = []
        self.embeddings = []
        self.embedding_model = LLM.create("text-embedding-3-small")
    
    def add_document(self, text: str, metadata: dict = None):
        """ë¬¸ì„œ ì¶”ê°€"""
        # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = self._split_text(text, chunk_size=500)
        
        for chunk in chunks:
            self.documents.append({
                "text": chunk,
                "metadata": metadata or {}
            })
            
            # ì„ë² ë”© ìƒì„±
            embedding = self.embedding_model.embed(chunk)
            self.embeddings.append(embedding.embeddings[0])
    
    def ask(self, question: str, top_k: int = 3) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€"""
        # ì§ˆë¬¸ ì„ë² ë”©
        q_embedding = self.embedding_model.embed(question).embeddings[0]
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self._find_relevant_docs(q_embedding, top_k)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([doc["text"] for doc in relevant_docs])
        
        # ë‹µë³€ ìƒì„±
        prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        reply = self.llm.ask(prompt)
        
        # ì¶œì²˜ í¬í•¨
        sources = [doc.get("metadata", {}).get("source", "Unknown") 
                  for doc in relevant_docs]
        
        return {
            "answer": reply.text,
            "sources": list(set(sources)),
            "confidence": self._calculate_confidence(relevant_docs)
        }
    
    def _split_text(self, text: str, chunk_size: int) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        sentences = text.split('. ')
        chunks = []
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            if current_size + sentence_size > chunk_size and current_chunk:
                chunks.append('. '.join(current_chunk) + '.')
                current_chunk = [sentence]
                current_size = sentence_size
            else:
                current_chunk.append(sentence)
                current_size += sentence_size
        
        if current_chunk:
            chunks.append('. '.join(current_chunk))
        
        return chunks
    
    def _find_relevant_docs(self, query_embedding, top_k: int):
        """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        from sklearn.metrics.pairwise import cosine_similarity
        
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            sim = cosine_similarity([query_embedding], [doc_embedding])[0][0]
            similarities.append((sim, i))
        
        similarities.sort(reverse=True)
        
        relevant_docs = []
        for sim, idx in similarities[:top_k]:
            doc = self.documents[idx].copy()
            doc['similarity'] = sim
            relevant_docs.append(doc)
        
        return relevant_docs
    
    def _calculate_confidence(self, docs: List[dict]) -> float:
        """ë‹µë³€ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not docs:
            return 0.0
        
        # í‰ê·  ìœ ì‚¬ë„ë¥¼ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
        avg_similarity = sum(doc['similarity'] for doc in docs) / len(docs)
        return min(avg_similarity * 1.2, 1.0)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

# ì‚¬ìš© ì˜ˆ
qa = QASystem()

# ë¬¸ì„œ ì¶”ê°€
qa.add_document(
    "íŒŒì´ì¬ì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. "
    "íŒŒì´ì¬ì€ ì½”ë“œì˜ ê°€ë…ì„±ì„ ì¤‘ì‹œí•˜ë©°, ê°„ê²°í•œ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
    metadata={"source": "Python Wikipedia"}
)

qa.add_document(
    "íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° íŒ¨ëŸ¬ë‹¤ì„ì„ ì§€ì›í•©ë‹ˆë‹¤. "
    "ê°ì²´ ì§€í–¥, í•¨ìˆ˜í˜•, ì ˆì°¨ì  í”„ë¡œê·¸ë˜ë°ì´ ëª¨ë‘ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
    metadata={"source": "Python Documentation"}
)

# ì§ˆë¬¸
result = qa.ask("íŒŒì´ì¬ì€ ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?")
print(f"ë‹µë³€: {result['answer']}")
print(f"ì¶œì²˜: {result['sources']}")
print(f"ì‹ ë¢°ë„: {result['confidence']:.2%}")
```

ì´ CHEATSHEETëŠ” pyhub-llmì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ê³¼ í™œìš© ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ì˜ˆì œëŠ” ì‹¤ì œë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ë“œì´ë©°, í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.