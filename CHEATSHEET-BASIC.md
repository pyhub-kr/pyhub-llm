# pyhub-llm ì´ˆê¸‰ ê°€ì´ë“œ

pyhub-llmì„ ì²˜ìŒ ì‚¬ìš©í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ ê¸°ë³¸ ê°€ì´ë“œì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” ì„¤ì¹˜ë¶€í„° ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²•, ëŒ€í™” ê´€ë¦¬, íŒŒì¼ ì²˜ë¦¬ ë“± í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.

> ğŸ’¡ ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ me@pyhub.krë¡œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.

## ëª©ì°¨

1. [ì„¤ì¹˜](#ì„¤ì¹˜)
2. [ê¸°ë³¸ ì‚¬ìš©ë²•](#ê¸°ë³¸-ì‚¬ìš©ë²•)
   - [í™˜ê²½ë³€ìˆ˜ ì„¤ì •](#í™˜ê²½ë³€ìˆ˜-ì„¤ì •)
   - [OpenAI](#openai)
   - [Anthropic](#anthropic)
   - [Google](#google)
   - [Ollama (ë¡œì»¬)](#ollama-ë¡œì»¬)
   - [Upstage](#upstage)
3. [ìŠ¤íŠ¸ë¦¬ë°](#ìŠ¤íŠ¸ë¦¬ë°)
4. [ì¶œë ¥ í¬ë§·íŒ…](#ì¶œë ¥-í¬ë§·íŒ…)
5. [ëŒ€í™” ê´€ë¦¬](#ëŒ€í™”-ê´€ë¦¬)
   - [ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€](#ëŒ€í™”-íˆìŠ¤í† ë¦¬-ìœ ì§€)
   - [ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬](#ì»¨í…ìŠ¤íŠ¸-ìœˆë„ìš°-ê´€ë¦¬)
   - [í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€í™”](#í˜ë¥´ì†Œë‚˜-ê¸°ë°˜-ëŒ€í™”)
6. [íŒŒì¼ ì²˜ë¦¬](#íŒŒì¼-ì²˜ë¦¬)
   - [ì´ë¯¸ì§€ ë¶„ì„](#ì´ë¯¸ì§€-ë¶„ì„)
   - [PDF ë¬¸ì„œ ì²˜ë¦¬](#pdf-ë¬¸ì„œ-ì²˜ë¦¬)
   - [ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸](#ì´ë¯¸ì§€-ìƒì„±-í”„ë¡¬í”„íŠ¸)
7. [ì—ëŸ¬ ì²˜ë¦¬](#ì—ëŸ¬-ì²˜ë¦¬)
   - [ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬](#ê¸°ë³¸-ì—ëŸ¬-ì²˜ë¦¬)
   - [í´ë°± ì²˜ë¦¬](#í´ë°±-ì²˜ë¦¬)
   - [íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬](#íƒ€ì„ì•„ì›ƒ-ì²˜ë¦¬)
8. [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

## ì„¤ì¹˜

```bash
# ì „ì²´ ì„¤ì¹˜ (ëª¨ë“  í”„ë¡œë°”ì´ë”)
pip install "pyhub-llm[all]"

# íŠ¹ì • í”„ë¡œë°”ì´ë”ë§Œ ì„¤ì¹˜
pip install "pyhub-llm[openai]"      # OpenAIë§Œ
pip install "pyhub-llm[anthropic]"   # Anthropicë§Œ
pip install "pyhub-llm[google]"      # Googleë§Œ
pip install "pyhub-llm[ollama]"      # Ollamaë§Œ

# MCP ì§€ì› í¬í•¨
pip install "pyhub-llm[all,mcp]"
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/basic/01_hello_world.py)

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# Linux/macOS
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GOOGLE_API_KEY="..."
export UPSTAGE_API_KEY="..."

# Windows PowerShell
$env:OPENAI_API_KEY="sk-..."
$env:ANTHROPIC_API_KEY="sk-ant-..."
```

### OpenAI

```python
from pyhub.llm import LLM, OpenAILLM

# íŒ©í† ë¦¬ íŒ¨í„´ ì‚¬ìš© (ê¶Œì¥)
llm = LLM.create("gpt-4o-mini")
reply = llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")
print(reply.text)

# ì§ì ‘ ìƒì„±
llm = OpenAILLM(model="gpt-4o-mini", temperature=0.7)
reply = llm.ask("íŒŒì´ì¬ì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”")
print(reply.text)
```

### Anthropic

```python
from pyhub.llm import LLM, AnthropicLLM

# Claude ì‚¬ìš©
llm = LLM.create("claude-3-haiku-20240307")
reply = llm.ask("ì–‘ì ì»´í“¨í„°ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”")
print(reply.text)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
llm = AnthropicLLM(
    model="claude-3-5-sonnet-20241022",
    system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
)
```

### Google

```python
from pyhub.llm import LLM, GoogleLLM

# Gemini ì‚¬ìš©
llm = LLM.create("gemini-1.5-flash")
reply = llm.ask("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”. ì‚°ì±…í•˜ê¸° ì¢‹ì€ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.")
print(reply.text)

# ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
llm = GoogleLLM(model="gemini-1.5-pro", max_tokens=8192)
```

### Ollama (ë¡œì»¬)

```python
from pyhub.llm import LLM, OllamaLLM

# OllamaëŠ” API í‚¤ ë¶ˆí•„ìš”
llm = LLM.create("mistral")
reply = llm.ask("ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” LLMì˜ ì¥ì ì€?")
print(reply.text)

# ì»¤ìŠ¤í…€ ì„œë²„ ì£¼ì†Œ
llm = OllamaLLM(
    model="llama2",
    base_url="http://192.168.1.100:11434"
)
```

### Upstage

```python
from pyhub.llm import UpstageLLM

llm = UpstageLLM(model="solar-mini")
reply = llm.ask("í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ì˜ ì–´ë ¤ìš´ ì ì€?")
print(reply.text)
```

## ìŠ¤íŠ¸ë¦¬ë°

ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ ë°›ì•„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/basic/02_streaming.py)

```python
from pyhub.llm import LLM

llm = LLM.create("gpt-4o-mini")

# ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë°
for chunk in llm.ask("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)
print()

# ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì²˜ë¦¬
def process_stream(llm, prompt):
    full_text = ""
    for i, chunk in enumerate(llm.ask(prompt, stream=True)):
        full_text += chunk.text
        # íŠ¹ì • ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
        if "ì¢…ë£Œ" in chunk.text:
            break
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        if i % 10 == 0:
            print(".", end="", flush=True)
    return full_text
```

## ì¶œë ¥ í¬ë§·íŒ…

í¸ë¦¬í•œ ì¶œë ¥ ê¸°ëŠ¥ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ì„ ì§€ì›í•©ë‹ˆë‹¤.

### display() í•¨ìˆ˜ ì‚¬ìš©

```python
from pyhub.llm import LLM, display

llm = LLM.create("gpt-4o-mini")

# ìŠ¤íŠ¸ë¦¬ë°ê³¼ í•¨ê»˜ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
response = llm.ask("íŒŒì´ì¬ í•¨ìˆ˜ ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”", stream=True)
display(response)  # ìë™ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§!

# ì¼ë°˜ ì‘ë‹µë„ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
reply = llm.ask("# ì œëª©\n\n**êµµì€ ê¸€ì”¨**ë¡œ ì‘ì„±")
display(reply)

# ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥
display(reply, markdown=False)
```

### Response.print() ë©”ì„œë“œ

```python
# ëª¨ë“  Response ê°ì²´ì— print() ë©”ì„œë“œ ì œê³µ
reply = llm.ask("ë§ˆí¬ë‹¤ìš´ í‘œ ì˜ˆì œ")

# ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
reply.print()  # ê¸°ë³¸ê°’: markdown=True

# ì¼ë°˜ í…ìŠ¤íŠ¸
reply.print(markdown=False)

# ìŠ¤íŠ¸ë§Œë„ ë™ì¼í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥
response = llm.ask("ì½”ë“œ ì˜ˆì œ", stream=True)
response.print()  # ìŠ¤íŠ¸ë¦¬ë°í•˜ë©´ì„œ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
```

### Rich ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ì„ ìœ„í•´ì„œëŠ” Rich ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤:

```bash
pip install "pyhub-llm[rich]"
# ë˜ëŠ”
pip install rich
```

Richê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°, ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.

## ëŒ€í™” ê´€ë¦¬

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/basic/03_conversation.py)

### ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€

pyhub-llmì€ ë‚´ë¶€ì ìœ¼ë¡œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìë™ ê´€ë¦¬í•©ë‹ˆë‹¤. ë³„ë„ì˜ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ì—†ì´ë„ ì—°ì†ì ì¸ ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
from pyhub.llm import LLM

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = LLM.create("gpt-4o-mini")

# ìë™ìœ¼ë¡œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ìœ ì§€ë¨
print(llm.ask("ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ì‹¶ì–´ìš”.").text)
print(llm.ask("íŒŒì´ì¬ë¶€í„° ì‹œì‘í•˜ë©´ ì¢‹ì„ê¹Œìš”?").text)
print(llm.ask("ê·¸ëŸ¼ ì²« ë²ˆì§¸ë¡œ ë­˜ ë°°ì›Œì•¼ í• ê¹Œìš”?").text)

# ëŒ€í™” ì´ˆê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš°
llm.clear()  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
```


### Stateless ëª¨ë“œ (íˆìŠ¤í† ë¦¬ ì—†ëŠ” ë…ë¦½ ì²˜ë¦¬)

ë°˜ë³µì ì¸ ë…ë¦½ ì‘ì—…ì—ì„œëŠ” ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤. Stateless ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ë©´ ê° ìš”ì²­ì´ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.

```python
from pyhub.llm import LLM

# Stateless ëª¨ë“œë¡œ ìƒì„± (íˆìŠ¤í† ë¦¬ ì €ì¥ ì•ˆ í•¨)
classifier = LLM.create("gpt-4o-mini", stateless=True)

# ëŒ€ëŸ‰ì˜ ë…ë¦½ì ì¸ ë¶„ë¥˜ ì‘ì—…
texts = ["í™˜ë¶ˆí•´ì£¼ì„¸ìš”", "ì–¸ì œ ë°°ì†¡ë˜ë‚˜ìš”?", "ì œí’ˆì´ ê³ ì¥ë‚¬ì–´ìš”"]
for text in texts:
    reply = classifier.ask(
        f"ê³ ê° ë¬¸ì˜ ë¶„ë¥˜: {text}",
        choices=["í™˜ë¶ˆ", "ë°°ì†¡", "AS", "ê¸°íƒ€"]
    )
    print(f"{text} -> {reply.choice}")
    # ê° ìš”ì²­ì´ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ API ë¹„ìš© ì ˆê°
```

### ì¼ë°˜ ëª¨ë“œ vs Stateless ëª¨ë“œ ë¹„êµ

| íŠ¹ì§• | ì¼ë°˜ ëª¨ë“œ | Stateless ëª¨ë“œ |
|------|-----------|----------------|
| ëŒ€í™” íˆìŠ¤í† ë¦¬ | ìë™ ì €ì¥ | ì €ì¥ ì•ˆ í•¨ |
| ì—°ì† ëŒ€í™” | ê°€ëŠ¥ | ë¶ˆê°€ëŠ¥ |
| ë©”ëª¨ë¦¬ ì‚¬ìš© | ëˆ„ì ë¨ | í•­ìƒ ìµœì†Œ |
| API í† í° ì‚¬ìš© | ëˆ„ì ë¨ | í•­ìƒ ìµœì†Œ |
| ì‚¬ìš© ì‚¬ë¡€ | ì±—ë´‡, ëŒ€í™”í˜• AI | ë¶„ë¥˜, ì¶”ì¶œ, ë²ˆì—­ |
| `use_history` | ë™ì‘í•¨ | ë¬´ì‹œë¨ |
| `clear()` | íˆìŠ¤í† ë¦¬ ì‚­ì œ | ì•„ë¬´ ë™ì‘ ì•ˆ í•¨ |

### ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬

pyhub-llmì€ ë‚´ë¶€ì ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¥¼ ê´€ë¦¬í•˜ì§€ë§Œ, í•„ìš”ì— ë”°ë¼ ìˆ˜ë™ìœ¼ë¡œ ì œì–´í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```python
from pyhub.llm import LLM

# ìë™ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ (ê¸°ë³¸ê°’)
llm = LLM.create("gpt-4o-mini")

# ê¸´ ëŒ€í™” ì§„í–‰ - LLMì´ ìë™ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
for i in range(20):
    reply = llm.ask(f"ì§ˆë¬¸ {i}: ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ë‚˜ìš”?")
    print(f"ë‹µë³€ {i}: {reply.text[:50]}...")

# ìˆ˜ë™ìœ¼ë¡œ ë©”ì‹œì§€ ìˆ˜ ì œí•œí•˜ê¸°
if len(llm.history) > 10:
    # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
    llm.history = llm.history[-10:]
```

### í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€í™”

ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ ë‹¤ì–‘í•œ í˜ë¥´ì†Œë‚˜ë¡œ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from pyhub.llm import LLM

# ë‹¤ì–‘í•œ í˜ë¥´ì†Œë‚˜ ì„¤ì •
teacher = LLM.create(
    "gpt-4o-mini",
    system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì¸ë‚´ì‹¬ ìˆëŠ” í”„ë¡œê·¸ë˜ë° êµì‚¬ì…ë‹ˆë‹¤."
)

chef = LLM.create(
    "gpt-4o-mini",
    system_prompt="ë‹¹ì‹ ì€ ë¯¸ìŠë­ 3ìŠ¤íƒ€ ì…°í”„ì…ë‹ˆë‹¤. ìš”ë¦¬ì— ëŒ€í•œ ì—´ì •ì´ ë„˜ì¹©ë‹ˆë‹¤."
)

doctor = LLM.create(
    "gpt-4o-mini",
    system_prompt="ë‹¹ì‹ ì€ ì˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤."
)

# ê° í˜ë¥´ì†Œë‚˜ì™€ ëŒ€í™” (ì»¨í…ìŠ¤íŠ¸ ìë™ ìœ ì§€)
print(teacher.ask("ì¬ê·€í•¨ìˆ˜ê°€ ë­”ê°€ìš”?").text)
print(teacher.ask("ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.").text)  # ì´ì „ ëŒ€í™” ê¸°ì–µ

print(chef.ask("íŒŒìŠ¤íƒ€ ë©´ì„ ì‚¶ëŠ” ìµœì ì˜ ì‹œê°„ì€?").text)
print(chef.ask("ì†Œê¸ˆì€ ì–¸ì œ ë„£ë‚˜ìš”?").text)  # íŒŒìŠ¤íƒ€ ê´€ë ¨ ëŒ€í™” ê³„ì†

print(doctor.ask("ë‘í†µì´ ìì£¼ ìˆì–´ìš”").text)  # ì£¼ì˜: ì‹¤ì œ ì˜ë£Œ ì¡°ì–¸ì´ ì•„ë‹˜
```

## íŒŒì¼ ì²˜ë¦¬

ğŸ’» [ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ](examples/basic/04_file_processing.py)

### ì´ë¯¸ì§€ ë¶„ì„

```python
from pyhub.llm import LLM

llm = LLM.create("gpt-4o-mini")  # ë¹„ì „ ì§€ì› ëª¨ë¸

# ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?",
    files=["photo.jpg"]
)
print(reply.text)

# ì—¬ëŸ¬ ì´ë¯¸ì§€ ë¹„êµ
reply = llm.ask(
    "ì´ ë‘ ì´ë¯¸ì§€ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    files=["before.png", "after.png"]
)
print(reply.text)

# ì´ë¯¸ì§€ì™€ í•¨ê»˜ êµ¬ì¡°í™”ëœ ì¶œë ¥
from pydantic import BaseModel
from typing import List

class ImageAnalysis(BaseModel):
    objects: List[str] = Field(description="ì´ë¯¸ì§€ì—ì„œ ë°œê²¬ëœ ê°ì²´ë“¤")
    scene: str = Field(description="ì „ì²´ì ì¸ ì¥ë©´ ì„¤ëª…")
    colors: List[str] = Field(description="ì£¼ìš” ìƒ‰ìƒë“¤")
    mood: str = Field(description="ì´ë¯¸ì§€ì˜ ë¶„ìœ„ê¸°")

reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
    files=["landscape.jpg"],
    schema=ImageAnalysis
)
analysis = reply.structured_data
print(f"ë°œê²¬ëœ ê°ì²´: {', '.join(analysis.objects)}")
print(f"ë¶„ìœ„ê¸°: {analysis.mood}")
```

### PDF ë¬¸ì„œ ì²˜ë¦¬

```python
# PDFëŠ” ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë¨
llm = LLM.create("gpt-4o-mini")

# PDF ìš”ì•½
reply = llm.ask(
    "ì´ PDF ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
    files=["report.pdf"]
)
print(reply.text)

# ì—¬ëŸ¬ í˜ì´ì§€ PDF ì²˜ë¦¬
class PDFSummary(BaseModel):
    title: str
    main_topics: List[str]
    key_findings: List[str]
    recommendations: List[str]

reply = llm.ask(
    "ì´ ì—°êµ¬ ë³´ê³ ì„œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
    files=["research_paper.pdf"],
    schema=PDFSummary
)
```

### ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸

```python
class ImagePrompt(BaseModel):
    style: str = Field(description="ê·¸ë¦¼ ìŠ¤íƒ€ì¼")
    subject: str = Field(description="ì£¼ìš” ëŒ€ìƒ")
    background: str = Field(description="ë°°ê²½ ì„¤ëª…")
    mood: str = Field(description="ë¶„ìœ„ê¸°")
    details: List[str] = Field(description="ì¶”ê°€ ì„¸ë¶€ì‚¬í•­")

llm = LLM.create("gpt-4o-mini")

# ì´ë¯¸ì§€ë¥¼ ë³´ê³  ìœ ì‚¬í•œ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
    files=["reference_image.jpg"],
    schema=ImagePrompt
)

prompt_data = reply.structured_data
print(f"ìŠ¤íƒ€ì¼: {prompt_data.style}")
print(f"í”„ë¡¬í”„íŠ¸: A {prompt_data.style} image of {prompt_data.subject} with {prompt_data.background}")
```

## ì—ëŸ¬ ì²˜ë¦¬

### ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬

```python
from pyhub.llm import LLM
from pyhub.llm.exceptions import (
    LLMError,
    RateLimitError,
    AuthenticationError,
    InvalidRequestError
)

def safe_llm_call(prompt: str, max_retries: int = 3):
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì•ˆì „í•œ LLM í˜¸ì¶œ"""
    llm = LLM.create("gpt-4o-mini")
    
    for attempt in range(max_retries):
        try:
            return llm.ask(prompt)
        
        except RateLimitError as e:
            # ì†ë„ ì œí•œ ì—ëŸ¬ - ëŒ€ê¸° í›„ ì¬ì‹œë„
            wait_time = getattr(e, 'retry_after', 60)
            print(f"Rate limit hit. Waiting {wait_time} seconds...")
            time.sleep(wait_time)
            
        except AuthenticationError:
            # ì¸ì¦ ì—ëŸ¬ - ì¬ì‹œë„ ë¶ˆê°€
            print("Authentication failed. Check your API key.")
            raise
            
        except InvalidRequestError as e:
            # ì˜ëª»ëœ ìš”ì²­ - ìˆ˜ì • í•„ìš”
            print(f"Invalid request: {e}")
            raise
            
        except LLMError as e:
            # ê¸°íƒ€ LLM ì—ëŸ¬
            print(f"LLM error (attempt {attempt + 1}): {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    
    raise Exception("Max retries exceeded")
```

### í´ë°± ì²˜ë¦¬

```python
class LLMWithFallback:
    """í´ë°± LLMì´ ìˆëŠ” ë˜í¼"""
    
    def __init__(self, primary_model: str, fallback_model: str):
        self.primary = LLM.create(primary_model)
        self.fallback = LLM.create(fallback_model)
    
    def ask(self, prompt: str, **kwargs):
        try:
            return self.primary.ask(prompt, **kwargs)
        except Exception as e:
            print(f"Primary LLM failed: {e}. Using fallback...")
            return self.fallback.ask(prompt, **kwargs)

# ì‚¬ìš©
llm = LLMWithFallback("gpt-4o", "gpt-4o-mini")
reply = llm.ask("ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ...")
```

### íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬

```python
import asyncio
from asyncio import TimeoutError

async def ask_with_timeout(llm, prompt: str, timeout: float = 30.0):
    """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” LLM í˜¸ì¶œ"""
    try:
        return await asyncio.wait_for(
            llm.ask_async(prompt),
            timeout=timeout
        )
    except TimeoutError:
        print(f"Request timed out after {timeout} seconds")
        # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
        simplified = f"ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”: {prompt}"
        return await llm.ask_async(simplified)

# ì‚¬ìš©
llm = LLM.create("gpt-4o-mini")
result = asyncio.run(ask_with_timeout(llm, "ë§¤ìš° ë³µì¡í•œ ì§ˆë¬¸...", timeout=10))
```

## ì‹¤ì „ ì˜ˆì œ: Stateless ëª¨ë“œ í™œìš©

### ê³ ê° ë¬¸ì˜ ë¶„ë¥˜ ì‹œìŠ¤í…œ

```python
from pyhub.llm import LLM
from typing import List, Dict

def classify_customer_inquiries(inquiries: List[str]) -> List[Dict[str, str]]:
    """ëŒ€ëŸ‰ì˜ ê³ ê° ë¬¸ì˜ë¥¼ ë¶„ë¥˜"""
    # Stateless ëª¨ë“œë¡œ ë¶„ë¥˜ê¸° ìƒì„±
    classifier = LLM.create("gpt-4o-mini", stateless=True)
    
    categories = ["í™˜ë¶ˆ/ë°˜í’ˆ", "ë°°ì†¡ë¬¸ì˜", "ì œí’ˆë¬¸ì˜", "ASìš”ì²­", "ê¸°íƒ€"]
    results = []
    
    for inquiry in inquiries:
        reply = classifier.ask(
            f"ë‹¤ìŒ ê³ ê° ë¬¸ì˜ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”: {inquiry}",
            choices=categories
        )
        results.append({
            "inquiry": inquiry,
            "category": reply.choice,
            "confidence": reply.confidence
        })
    
    return results

# ì‚¬ìš© ì˜ˆ
inquiries = [
    "ì œí’ˆì´ íŒŒì†ë˜ì–´ ë„ì°©í–ˆì–´ìš”",
    "ì£¼ë¬¸í•œ ì§€ ì¼ì£¼ì¼ì´ ëëŠ”ë° ì•„ì§ ì•ˆ ì™”ì–´ìš”",
    "ì´ ì œí’ˆ ì‚¬ìš©ë²•ì„ ëª¨ë¥´ê² ì–´ìš”",
    "í™˜ë¶ˆ ì²˜ë¦¬ëŠ” ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?"
]

results = classify_customer_inquiries(inquiries)
for r in results:
    print(f"{r['inquiry'][:20]}... -> {r['category']} ({r['confidence']:.2f})")
```

### ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„

```python
from pyhub.llm import LLM
from pydantic import BaseModel
from concurrent.futures import ThreadPoolExecutor
import time

class SentimentResult(BaseModel):
    sentiment: str  # positive, negative, neutral
    confidence: float
    keywords: List[str]

def analyze_sentiment_batch(texts: List[str], batch_size: int = 10):
    """ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)"""
    # Stateless ëª¨ë“œë¡œ ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    analyzers = [
        LLM.create("gpt-4o-mini", stateless=True) 
        for _ in range(batch_size)
    ]
    
    def analyze_single(analyzer_text_pair):
        analyzer, text = analyzer_text_pair
        reply = analyzer.ask(
            f"Analyze sentiment of: {text}",
            schema=SentimentResult
        )
        return {
            "text": text,
            "result": reply.structured_data
        }
    
    # ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=batch_size) as executor:
        # ê° í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ê¸°ì— í• ë‹¹
        pairs = [(analyzers[i % batch_size], text) for i, text in enumerate(texts)]
        results = list(executor.map(analyze_single, pairs))
    
    return results

# ì‚¬ìš© ì˜ˆ
reviews = [
    "This product is amazing! Best purchase ever.",
    "Terrible experience, would not recommend.",
    "It's okay, nothing special.",
    # ... ìˆ˜ë°± ê°œì˜ ë¦¬ë·°
]

start = time.time()
results = analyze_sentiment_batch(reviews[:50], batch_size=5)
print(f"Analyzed {len(results)} reviews in {time.time() - start:.2f}s")
```

### ë¬¸ì„œ ìš”ì•½ ë°°ì¹˜ ì²˜ë¦¬

```python
from pyhub.llm import LLM
from pathlib import Path
import json

def summarize_documents(doc_folder: str, output_file: str):
    """í´ë” ë‚´ ëª¨ë“  ë¬¸ì„œë¥¼ ìš”ì•½"""
    # Stateless ëª¨ë“œ - ê° ë¬¸ì„œê°€ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë¨
    summarizer = LLM.create("gpt-4o-mini", stateless=True)
    
    summaries = {}
    doc_path = Path(doc_folder)
    
    for file_path in doc_path.glob("*.txt"):
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ê° ë¬¸ì„œë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìš”ì•½
        reply = summarizer.ask(
            f"ë‹¤ìŒ ë¬¸ì„œë¥¼ 3ì¤„ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n\n{content[:2000]}"
        )
        
        summaries[file_path.name] = {
            "summary": reply.text,
            "file_size": len(content),
            "processed_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        print(f"âœ“ {file_path.name} ì²˜ë¦¬ ì™„ë£Œ")
    
    # ê²°ê³¼ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summaries, f, ensure_ascii=False, indent=2)
    
    return summaries

# ì‚¬ìš© ì˜ˆ
summaries = summarize_documents("./documents", "./summaries.json")
print(f"ì´ {len(summaries)}ê°œ ë¬¸ì„œ ìš”ì•½ ì™„ë£Œ")
```

## ë‹¤ìŒ ë‹¨ê³„

ì¶•í•˜í•©ë‹ˆë‹¤! pyhub-llmì˜ ê¸°ë³¸ ê¸°ëŠ¥ë“¤ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ì œ ë” ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ë°°ì›Œë³¼ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤:

### ì¤‘ê¸‰ ê°€ì´ë“œì—ì„œ ë‹¤ë£° ë‚´ìš©
- **êµ¬ì¡°í™”ëœ ì¶œë ¥**: Pydantic ìŠ¤í‚¤ë§ˆë¥¼ í™œìš©í•œ ë³µì¡í•œ ì‘ë‹µ ì²˜ë¦¬
- **ë¹„ë™ê¸° ì²˜ë¦¬**: ë™ì‹œì— ì—¬ëŸ¬ ìš”ì²­ ì²˜ë¦¬í•˜ê¸°
- **ê³ ê¸‰ ìºì‹±**: ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹± ì „ëµ
- **ì„ë² ë”©ê³¼ ë²¡í„° ê²€ìƒ‰**: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„
- **ì²´ì¸ê³¼ íŒŒì´í”„ë¼ì¸**: ë³µì¡í•œ ì‘ì—… íë¦„ êµ¬ì„±
- **MCP (Model Context Protocol)**: ì™¸ë¶€ ë„êµ¬ì™€ í†µí•©

### ì¶”ê°€ ë¦¬ì†ŒìŠ¤
- [ì „ì²´ ì¹˜íŠ¸ì‹œíŠ¸ (CHEATSHEET.md)](./CHEATSHEET.md) - ëª¨ë“  ê¸°ëŠ¥ì˜ ìƒì„¸í•œ ì˜ˆì œ
- [ê³µì‹ ë¬¸ì„œ](https://github.com/pyhub-kr/pyhub-llm) - ìµœì‹  ì—…ë°ì´íŠ¸ ë° API ë¬¸ì„œ
- [ì˜ˆì œ ì½”ë“œ](https://github.com/pyhub-kr/pyhub-llm/tree/main/examples) - ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€

### ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?
- GitHub Issuesì—ì„œ ì§ˆë¬¸í•˜ê¸°
- ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼ ì°¸ì—¬í•˜ê¸°
- ê¸°ì—¬í•˜ê¸°: ë²„ê·¸ ë¦¬í¬íŠ¸, ê¸°ëŠ¥ ì œì•ˆ, ë¬¸ì„œ ê°œì„ 

ì¦ê±°ìš´ ì½”ë”© ë˜ì„¸ìš”! ğŸš€
