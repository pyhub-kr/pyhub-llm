# pyhub-llm ì´ˆê¸‰ ê°€ì´ë“œ

pyhub-llmì„ ì²˜ìŒ ì‚¬ìš©í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ ê¸°ë³¸ ê°€ì´ë“œì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” ì„¤ì¹˜ë¶€í„° ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²•, ëŒ€í™” ê´€ë¦¬, íŒŒì¼ ì²˜ë¦¬ ë“± í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ëª©ì°¨

1. [ì„¤ì¹˜](#ì„¤ì¹˜)
2. [ê¸°ë³¸ ì‚¬ìš©ë²•](#ê¸°ë³¸-ì‚¬ìš©ë²•)
   - [í™˜ê²½ë³€ìˆ˜ ì„¤ì •](#í™˜ê²½ë³€ìˆ˜-ì„¤ì •)
   - [OpenAI](#openai)
   - [Anthropic](#anthropic)
   - [Google](#google)
   - [Ollama (ë¡œì»¬)](#ollama-ë¡œì»¬)
   - [Upstage](#upstage)
3. [ìŠ¤íŠ¸ë¦¬ë°](#ìŠ¤íŠ¸ë¦¬ë°)
4. [ëŒ€í™” ê´€ë¦¬](#ëŒ€í™”-ê´€ë¦¬)
   - [ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€](#ëŒ€í™”-íˆìŠ¤í† ë¦¬-ìœ ì§€)
   - [ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬](#ì»¨í…ìŠ¤íŠ¸-ìœˆë„ìš°-ê´€ë¦¬)
   - [í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€í™”](#í˜ë¥´ì†Œë‚˜-ê¸°ë°˜-ëŒ€í™”)
5. [íŒŒì¼ ì²˜ë¦¬](#íŒŒì¼-ì²˜ë¦¬)
   - [ì´ë¯¸ì§€ ë¶„ì„](#ì´ë¯¸ì§€-ë¶„ì„)
   - [PDF ë¬¸ì„œ ì²˜ë¦¬](#pdf-ë¬¸ì„œ-ì²˜ë¦¬)
   - [ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸](#ì´ë¯¸ì§€-ìƒì„±-í”„ë¡¬í”„íŠ¸)
6. [ì—ëŸ¬ ì²˜ë¦¬](#ì—ëŸ¬-ì²˜ë¦¬)
   - [ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬](#ê¸°ë³¸-ì—ëŸ¬-ì²˜ë¦¬)
   - [í´ë°± ì²˜ë¦¬](#í´ë°±-ì²˜ë¦¬)
   - [íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬](#íƒ€ì„ì•„ì›ƒ-ì²˜ë¦¬)
7. [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

## ì„¤ì¹˜

```bash
# ì „ì²´ ì„¤ì¹˜ (ëª¨ë“  í”„ë¡œë°”ì´ë”)
pip install "pyhub-llm[all]"

# íŠ¹ì • í”„ë¡œë°”ì´ë”ë§Œ ì„¤ì¹˜
pip install "pyhub-llm[openai]"      # OpenAIë§Œ
pip install "pyhub-llm[anthropic]"   # Anthropicë§Œ
pip install "pyhub-llm[google]"      # Googleë§Œ
pip install "pyhub-llm[ollama]"      # Ollamaë§Œ

# MCP ì§€ì› í¬í•¨
pip install "pyhub-llm[all,mcp]"
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# Linux/macOS
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GOOGLE_API_KEY="..."
export UPSTAGE_API_KEY="..."

# Windows PowerShell
$env:OPENAI_API_KEY="sk-..."
$env:ANTHROPIC_API_KEY="sk-ant-..."
```

### OpenAI

```python
from pyhub.llm import LLM, OpenAILLM

# íŒ©í† ë¦¬ íŒ¨í„´ ì‚¬ìš© (ê¶Œì¥)
llm = LLM.create("gpt-4o-mini")
reply = llm.ask("ì•ˆë…•í•˜ì„¸ìš”!")
print(reply.text)

# ì§ì ‘ ìƒì„±
llm = OpenAILLM(model="gpt-4o-mini", temperature=0.7)
reply = llm.ask("íŒŒì´ì¬ì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”")
print(reply.text)
```

### Anthropic

```python
from pyhub.llm import LLM, AnthropicLLM

# Claude ì‚¬ìš©
llm = LLM.create("claude-3-haiku-20240307")
reply = llm.ask("ì–‘ì ì»´í“¨í„°ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”")
print(reply.text)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
llm = AnthropicLLM(
    model="claude-3-5-sonnet-20241022",
    system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
)
```

### Google

```python
from pyhub.llm import LLM, GoogleLLM

# Gemini ì‚¬ìš©
llm = LLM.create("gemini-1.5-flash")
reply = llm.ask("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”. ì‚°ì±…í•˜ê¸° ì¢‹ì€ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.")
print(reply.text)

# ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
llm = GoogleLLM(model="gemini-1.5-pro", max_tokens=8192)
```

### Ollama (ë¡œì»¬)

```python
from pyhub.llm import LLM, OllamaLLM

# OllamaëŠ” API í‚¤ ë¶ˆí•„ìš”
llm = LLM.create("mistral")
reply = llm.ask("ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” LLMì˜ ì¥ì ì€?")
print(reply.text)

# ì»¤ìŠ¤í…€ ì„œë²„ ì£¼ì†Œ
llm = OllamaLLM(
    model="llama2",
    base_url="http://192.168.1.100:11434"
)
```

### Upstage

```python
from pyhub.llm import UpstageLLM

llm = UpstageLLM(model="solar-mini")
reply = llm.ask("í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ì˜ ì–´ë ¤ìš´ ì ì€?")
print(reply.text)
```

## ìŠ¤íŠ¸ë¦¬ë°

ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ ë°›ì•„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
from pyhub.llm import LLM

llm = LLM.create("gpt-4o-mini")

# ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë°
for chunk in llm.ask("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”", stream=True):
    print(chunk.text, end="", flush=True)
print()

# ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì²˜ë¦¬
def process_stream(llm, prompt):
    full_text = ""
    for i, chunk in enumerate(llm.ask(prompt, stream=True)):
        full_text += chunk.text
        # íŠ¹ì • ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
        if "ì¢…ë£Œ" in chunk.text:
            break
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        if i % 10 == 0:
            print(".", end="", flush=True)
    return full_text
```

## ëŒ€í™” ê´€ë¦¬

### ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€

```python
from pyhub.llm import LLM
from pyhub.llm.types import Message

class ChatBot:
    def __init__(self, model="gpt-4o-mini"):
        self.llm = LLM.create(model)
        self.history = []
    
    def chat(self, user_input: str) -> str:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.history.append(Message(role="user", content=user_input))
        
        # LLMì—ê²Œ ì „ì²´ íˆìŠ¤í† ë¦¬ ì „ë‹¬
        reply = self.llm.messages(self.history)
        
        # ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.history.append(Message(role="assistant", content=reply.text))
        
        return reply.text
    
    def reset(self):
        """ëŒ€í™” ì´ˆê¸°í™”"""
        self.history = []
    
    def save_history(self, filename: str):
        """ëŒ€í™” ë‚´ì—­ ì €ì¥"""
        import json
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump([msg.dict() for msg in self.history], f, ensure_ascii=False, indent=2)

# ì‚¬ìš© ì˜ˆ
bot = ChatBot()
print(bot.chat("ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ì‹¶ì–´ìš”."))
print(bot.chat("íŒŒì´ì¬ë¶€í„° ì‹œì‘í•˜ë©´ ì¢‹ì„ê¹Œìš”?"))
print(bot.chat("ê·¸ëŸ¼ ì²« ë²ˆì§¸ë¡œ ë­˜ ë°°ì›Œì•¼ í• ê¹Œìš”?"))
```

### ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬

```python
class ContextManagedChat:
    def __init__(self, model="gpt-4o-mini", max_messages=10):
        self.llm = LLM.create(model)
        self.history = []
        self.max_messages = max_messages
    
    def chat(self, user_input: str) -> str:
        # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° ì œí•œ
        if len(self.history) >= self.max_messages * 2:
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ì˜¤ë˜ëœ ëŒ€í™” ì œê±°
            self.history = self.history[-self.max_messages * 2:]
        
        self.history.append(Message(role="user", content=user_input))
        reply = self.llm.messages(self.history)
        self.history.append(Message(role="assistant", content=reply.text))
        
        return reply.text
```

### í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€í™”

```python
class PersonaChat:
    def __init__(self, persona: str, model="gpt-4o-mini"):
        self.llm = LLM.create(
            model,
            system_prompt=persona
        )
        self.history = []
    
    def chat(self, message: str) -> str:
        reply = self.llm.ask(message)
        return reply.text

# ë‹¤ì–‘í•œ í˜ë¥´ì†Œë‚˜
teacher = PersonaChat("ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì¸ë‚´ì‹¬ ìˆëŠ” í”„ë¡œê·¸ë˜ë° êµì‚¬ì…ë‹ˆë‹¤.")
chef = PersonaChat("ë‹¹ì‹ ì€ ë¯¸ìŠë­ 3ìŠ¤íƒ€ ì…°í”„ì…ë‹ˆë‹¤. ìš”ë¦¬ì— ëŒ€í•œ ì—´ì •ì´ ë„˜ì¹©ë‹ˆë‹¤.")
doctor = PersonaChat("ë‹¹ì‹ ì€ ì˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.")

print(teacher.chat("ì¬ê·€í•¨ìˆ˜ê°€ ë­”ê°€ìš”?"))
print(chef.chat("íŒŒìŠ¤íƒ€ ë©´ì„ ì‚¶ëŠ” ìµœì ì˜ ì‹œê°„ì€?"))
print(doctor.chat("ë‘í†µì´ ìì£¼ ìˆì–´ìš”"))  # ì£¼ì˜: ì‹¤ì œ ì˜ë£Œ ì¡°ì–¸ì´ ì•„ë‹˜
```

## íŒŒì¼ ì²˜ë¦¬

### ì´ë¯¸ì§€ ë¶„ì„

```python
from pyhub.llm import LLM

llm = LLM.create("gpt-4o-mini")  # ë¹„ì „ ì§€ì› ëª¨ë¸

# ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?",
    files=["photo.jpg"]
)
print(reply.text)

# ì—¬ëŸ¬ ì´ë¯¸ì§€ ë¹„êµ
reply = llm.ask(
    "ì´ ë‘ ì´ë¯¸ì§€ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    files=["before.png", "after.png"]
)
print(reply.text)

# ì´ë¯¸ì§€ì™€ í•¨ê»˜ êµ¬ì¡°í™”ëœ ì¶œë ¥
from pydantic import BaseModel
from typing import List

class ImageAnalysis(BaseModel):
    objects: List[str] = Field(description="ì´ë¯¸ì§€ì—ì„œ ë°œê²¬ëœ ê°ì²´ë“¤")
    scene: str = Field(description="ì „ì²´ì ì¸ ì¥ë©´ ì„¤ëª…")
    colors: List[str] = Field(description="ì£¼ìš” ìƒ‰ìƒë“¤")
    mood: str = Field(description="ì´ë¯¸ì§€ì˜ ë¶„ìœ„ê¸°")

reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
    files=["landscape.jpg"],
    schema=ImageAnalysis
)
analysis = reply.structured_data
print(f"ë°œê²¬ëœ ê°ì²´: {', '.join(analysis.objects)}")
print(f"ë¶„ìœ„ê¸°: {analysis.mood}")
```

### PDF ë¬¸ì„œ ì²˜ë¦¬

```python
# PDFëŠ” ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë¨
llm = LLM.create("gpt-4o-mini")

# PDF ìš”ì•½
reply = llm.ask(
    "ì´ PDF ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
    files=["report.pdf"]
)
print(reply.text)

# ì—¬ëŸ¬ í˜ì´ì§€ PDF ì²˜ë¦¬
class PDFSummary(BaseModel):
    title: str
    main_topics: List[str]
    key_findings: List[str]
    recommendations: List[str]

reply = llm.ask(
    "ì´ ì—°êµ¬ ë³´ê³ ì„œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
    files=["research_paper.pdf"],
    schema=PDFSummary
)
```

### ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸

```python
class ImagePrompt(BaseModel):
    style: str = Field(description="ê·¸ë¦¼ ìŠ¤íƒ€ì¼")
    subject: str = Field(description="ì£¼ìš” ëŒ€ìƒ")
    background: str = Field(description="ë°°ê²½ ì„¤ëª…")
    mood: str = Field(description="ë¶„ìœ„ê¸°")
    details: List[str] = Field(description="ì¶”ê°€ ì„¸ë¶€ì‚¬í•­")

llm = LLM.create("gpt-4o-mini")

# ì´ë¯¸ì§€ë¥¼ ë³´ê³  ìœ ì‚¬í•œ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
reply = llm.ask(
    "ì´ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
    files=["reference_image.jpg"],
    schema=ImagePrompt
)

prompt_data = reply.structured_data
print(f"ìŠ¤íƒ€ì¼: {prompt_data.style}")
print(f"í”„ë¡¬í”„íŠ¸: A {prompt_data.style} image of {prompt_data.subject} with {prompt_data.background}")
```

## ì—ëŸ¬ ì²˜ë¦¬

### ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬

```python
from pyhub.llm import LLM
from pyhub.llm.exceptions import (
    LLMError,
    RateLimitError,
    AuthenticationError,
    InvalidRequestError
)

def safe_llm_call(prompt: str, max_retries: int = 3):
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì•ˆì „í•œ LLM í˜¸ì¶œ"""
    llm = LLM.create("gpt-4o-mini")
    
    for attempt in range(max_retries):
        try:
            return llm.ask(prompt)
        
        except RateLimitError as e:
            # ì†ë„ ì œí•œ ì—ëŸ¬ - ëŒ€ê¸° í›„ ì¬ì‹œë„
            wait_time = getattr(e, 'retry_after', 60)
            print(f"Rate limit hit. Waiting {wait_time} seconds...")
            time.sleep(wait_time)
            
        except AuthenticationError:
            # ì¸ì¦ ì—ëŸ¬ - ì¬ì‹œë„ ë¶ˆê°€
            print("Authentication failed. Check your API key.")
            raise
            
        except InvalidRequestError as e:
            # ì˜ëª»ëœ ìš”ì²­ - ìˆ˜ì • í•„ìš”
            print(f"Invalid request: {e}")
            raise
            
        except LLMError as e:
            # ê¸°íƒ€ LLM ì—ëŸ¬
            print(f"LLM error (attempt {attempt + 1}): {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    
    raise Exception("Max retries exceeded")
```

### í´ë°± ì²˜ë¦¬

```python
class LLMWithFallback:
    """í´ë°± LLMì´ ìˆëŠ” ë˜í¼"""
    
    def __init__(self, primary_model: str, fallback_model: str):
        self.primary = LLM.create(primary_model)
        self.fallback = LLM.create(fallback_model)
    
    def ask(self, prompt: str, **kwargs):
        try:
            return self.primary.ask(prompt, **kwargs)
        except Exception as e:
            print(f"Primary LLM failed: {e}. Using fallback...")
            return self.fallback.ask(prompt, **kwargs)

# ì‚¬ìš©
llm = LLMWithFallback("gpt-4o", "gpt-4o-mini")
reply = llm.ask("ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ...")
```

### íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬

```python
import asyncio
from asyncio import TimeoutError

async def ask_with_timeout(llm, prompt: str, timeout: float = 30.0):
    """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” LLM í˜¸ì¶œ"""
    try:
        return await asyncio.wait_for(
            llm.ask_async(prompt),
            timeout=timeout
        )
    except TimeoutError:
        print(f"Request timed out after {timeout} seconds")
        # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
        simplified = f"ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”: {prompt}"
        return await llm.ask_async(simplified)

# ì‚¬ìš©
llm = LLM.create("gpt-4o-mini")
result = asyncio.run(ask_with_timeout(llm, "ë§¤ìš° ë³µì¡í•œ ì§ˆë¬¸...", timeout=10))
```

## ë‹¤ìŒ ë‹¨ê³„

ì¶•í•˜í•©ë‹ˆë‹¤! pyhub-llmì˜ ê¸°ë³¸ ê¸°ëŠ¥ë“¤ì„ ëª¨ë‘ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ì œ ë” ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ë°°ì›Œë³¼ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤:

### ì¤‘ê¸‰ ê°€ì´ë“œì—ì„œ ë‹¤ë£° ë‚´ìš©
- **êµ¬ì¡°í™”ëœ ì¶œë ¥**: Pydantic ìŠ¤í‚¤ë§ˆë¥¼ í™œìš©í•œ ë³µì¡í•œ ì‘ë‹µ ì²˜ë¦¬
- **ë¹„ë™ê¸° ì²˜ë¦¬**: ë™ì‹œì— ì—¬ëŸ¬ ìš”ì²­ ì²˜ë¦¬í•˜ê¸°
- **ê³ ê¸‰ ìºì‹±**: ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹± ì „ëµ
- **ì„ë² ë”©ê³¼ ë²¡í„° ê²€ìƒ‰**: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„
- **ì²´ì¸ê³¼ íŒŒì´í”„ë¼ì¸**: ë³µì¡í•œ ì‘ì—… íë¦„ êµ¬ì„±
- **MCP (Model Context Protocol)**: ì™¸ë¶€ ë„êµ¬ì™€ í†µí•©

### ì¶”ê°€ ë¦¬ì†ŒìŠ¤
- [ì „ì²´ ì¹˜íŠ¸ì‹œíŠ¸ (CHEATSHEET.md)](./CHEATSHEET.md) - ëª¨ë“  ê¸°ëŠ¥ì˜ ìƒì„¸í•œ ì˜ˆì œ
- [ê³µì‹ ë¬¸ì„œ](https://github.com/pyhub-kr/pyhub-llm) - ìµœì‹  ì—…ë°ì´íŠ¸ ë° API ë¬¸ì„œ
- [ì˜ˆì œ ì½”ë“œ](https://github.com/pyhub-kr/pyhub-llm/tree/main/examples) - ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€

### ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?
- GitHub Issuesì—ì„œ ì§ˆë¬¸í•˜ê¸°
- ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼ ì°¸ì—¬í•˜ê¸°
- ê¸°ì—¬í•˜ê¸°: ë²„ê·¸ ë¦¬í¬íŠ¸, ê¸°ëŠ¥ ì œì•ˆ, ë¬¸ì„œ ê°œì„ 

ì¦ê±°ìš´ ì½”ë”© ë˜ì„¸ìš”! ğŸš€
